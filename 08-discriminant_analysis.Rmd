# Analiza dyskryminacyjna {#LDA}

Analiza dyskryminacyjna (ang. *discriminant analysis*) jest grupą technik dyskryminacji obserwacji względem przynależności do klas. Część z nich należy do klasyfikatorów liniowych (choć nie zawsze w ścisłym sensie). Za autorów tej metody uważa się  Fisher'a -@fisher1936 i Welch'a -@welch1939. Każdy z nich prezentował nieco inne podejście do tematu klasyfikacji. Welch poszukiwał klasyfikacji minimalizującej prawdopodobieństwo błędnej klasyfikacji, znane jako [klasyfikatory bayesowskie](#bayes). Podejście Fisher'a skupiało się raczej na porównaniu zmienności międzygrupowej do zmienności wewnątrzgrupowej. Wychodząc z założenia, że iloraz tych wariancji powinien być stosunkowo duży przy różnych klasach, jeśli do ich opisu użyjemy odpowiednich zmiennych niezależnych. W istocie chodzi o znalezienie takiego wektora, w kierunku którego wspomniany iloraz wariancji jest największy.

## Liniowa analiza dyskryminacyjna Fisher'a

### Dwie kategorie zmiennej grupującej 

Niech $\boldsymbol D$ będzie zbiorem zawierającym $n$ punktów $\{\boldsymbol x, y\}$, gdzie $\boldsymbol x\in \mathbb{R}^d$, a $y\in \{c_1,\ldots,c_k\}$. Niech $\boldsymbol D_i$ oznacza podzbiór punktów zbioru $\boldsymbol D$, które należą do klasy $c_i$, czyli $\boldsymbol D_i=\{\boldsymbol x|y=c_i\}$ i niech $|\boldsymbol D_i|=n_i$. Na początek załóżmy, że $\boldsymbol D$ składa się tylko z $\boldsymbol D_1$ i $\boldsymbol D_2$. 

Niech $\boldsymbol w$ będzie wektorem jednostkowym ($\boldsymbol w'\boldsymbol w=1$), wówczas rzut ortogonalny punku $\boldsymbol x_i$ na wektor $\boldsymbol w$ można zapisać następująco
\begin{equation}
    \tilde{\boldsymbol x}=\left(\frac{\boldsymbol w'\boldsymbol x}{\boldsymbol w'\boldsymbol w}\right)\boldsymbol w=(\boldsymbol w'\boldsymbol x)\boldsymbol w = a\boldsymbol w,
\end{equation}
gdzie $a$ jest współrzędną punktu $\tilde{\boldsymbol x}$ w kierunku wektora $\boldsymbol w$, czyli 
\begin{equation}
    a=\boldsymbol w'\boldsymbol x.
\end{equation}
Zatem $(a_1,\ldots,a_n)$ reprezentują odwzorowanie $\mathbb{R}^d$ w $\mathbb{R}$, czyli z $d$-wymiarowej przestrzeni w przestrzeń generowaną przez $\boldsymbol w$. 

```{r rzut, echo=FALSE, fig.cap="Rzut ortogonalny punktów w kierunku wektora $\\boldsymbol w$", fig.align='center'}
knitr::include_graphics("images/rzut.png", dpi = 100)
```

Każdy punkt należy do pewnej klasy, dlatego możemy wyliczyć
\begin{align}
    m_1=&\frac{1}{n_1}\sum_{ \boldsymbol x\in \boldsymbol D_1}a=\\
    =&\frac{1}{n_1}\sum_{ \boldsymbol x\in \boldsymbol D_1} \boldsymbol w' \boldsymbol x=\\
    =& \boldsymbol w'\left(\frac{1}{n_1}\sum_{ \boldsymbol x\in \boldsymbol D_1} \boldsymbol x \right)=\\
    =& \boldsymbol w' \boldsymbol{\mu}_1,
    (\#eq:m)
\end{align}
gdzie $\boldsymbol \mu_1$ jest wektorem średnich punktów z $\boldsymbol D_1$. W podobny sposób można policzyć $m_2 = \boldsymbol w' \boldsymbol \mu_2$. Oznacza to, że średnia projekcji jest projekcją średnich.

Rozsądnym wydaje się teraz poszukać takiego wektora, aby $|m_1-m_2|$ była maksymalnie duża przy zachowaniu niezbyt dużej zmienności wewnątrz grup. Dlatego kryterium Fisher'a przyjmuje postać
\begin{equation}
    \max_{ \boldsymbol w}J(\boldsymbol w)=\frac{(m_1-m_2)^2}{ss_1^2+ss_2^2},
    (\#eq:condFisher)
\end{equation}
gdzie $ss_j^2=\sum_{ \boldsymbol x\in \boldsymbol D_j}(a-m_j)^2=n_j\sigma_j^2.$

Zauważmy, że licznik w \@ref(eq:condFisher) da się zapisać jako
\begin{align}
    (m_1-m_2)^2=& ( \boldsymbol w'( \boldsymbol \mu_1- \boldsymbol \mu_2))^2=\\
    =& \boldsymbol w'(\boldsymbol \mu_1- \boldsymbol \mu_2)(\boldsymbol \mu_1- \boldsymbol \mu_2)'\boldsymbol w=\\
    =& \boldsymbol w' \boldsymbol B \boldsymbol w
\end{align}
gdzie $\boldsymbol B=(\boldsymbol \mu_1- \boldsymbol \mu_2)(\boldsymbol \mu_1- \boldsymbol \mu_2)'$ jest macierzą $d\times d$.

Ponadto 
\begin{align}
    ss_j^2=&\sum_{ \boldsymbol x\in \boldsymbol D_j}(a-m_j)^2=\\
    =&\sum_{ \boldsymbol x\in \boldsymbol D_j}( \boldsymbol w' \boldsymbol x- \boldsymbol w' \boldsymbol\mu_j)^2=\\
    =& \sum_{ \boldsymbol x\in \boldsymbol D_j}( \boldsymbol{w}'( \boldsymbol{x}- \boldsymbol{\mu}_j))^2=\\
    =& \boldsymbol{w}'\left(\sum_{ \boldsymbol x\in \boldsymbol D_j}(\boldsymbol{x}-\boldsymbol \mu_j)(\boldsymbol x- \boldsymbol \mu_j)'\right) \boldsymbol{w}=\\
    =& \boldsymbol{w}' \boldsymbol{S}_j \boldsymbol{w},
    (\#eq:Sj)
\end{align}
gdzie $\boldsymbol{S}_j=n_j \boldsymbol{\Sigma}_j$.
Zatem mianownik \@ref(eq:condFisher) możemy zapisać jako
\begin{equation}
    ss_1^2+ss_2^2= \boldsymbol{w}'(\boldsymbol{S}_1+ \boldsymbol{S}_2) \boldsymbol{w}= \boldsymbol{w}' \boldsymbol{S} \boldsymbol{w},
\end{equation}
gdzie $\boldsymbol{S}=\boldsymbol{S}_1+\boldsymbol{S}_2$.
Ostatecznie warunek Fisher'a przyjmuje postać
\begin{equation}
    \max_{ \boldsymbol{w}}J( \boldsymbol{w})=\frac{ \boldsymbol{w}' \boldsymbol{B} \boldsymbol{w}}{ \boldsymbol{w}' \boldsymbol{S} \boldsymbol{w}}.
    (\#eq:condFisher2)
\end{equation}

Różniczkując \@ref(eq:condFisher2) po $\boldsymbol{w}$ i przyrównując go do 0, otrzymamy warunek
\begin{equation}
    \boldsymbol{B} \boldsymbol{w} = \lambda \boldsymbol{S} \boldsymbol{w},
    (\#eq:condFisher3)
\end{equation}
gdzie $\lambda=J(\boldsymbol{w})$. Maksimum \@ref(eq:condFisher2) jest osiągane dla wektora $\boldsymbol{w}$ równego wektorowi własnemu odpowiadającemu największej wartości własnej równania charakterystycznego $|\boldsymbol{B}-\lambda\boldsymbol{S}|=0$. Jeśli $\boldsymbol{S}$ nie jest osobliwa, to rozwiązanie \@ref(eq:condFisher3) otrzymujemy przez znalezienie największej wartości własnej macierzy $\boldsymbol{B}\boldsymbol{S}^{-1}$ lub bez wykorzystania wartości i wektorów własnych.

Ponieważ $\boldsymbol{B}\boldsymbol w=\left((\boldsymbol{\mu}_1-\boldsymbol{\mu}_2)(\boldsymbol{\mu}_1-\boldsymbol{\mu}_2)'\right)\boldsymbol{w}$ jest macierzą $d \times 1$ rzędu 1, to $\boldsymbol{B}\boldsymbol{w}$ jest punktem na kierunku wyznaczonym przez wektor $\boldsymbol{\mu}_1-\boldsymbol{\mu}_2$, bo
\begin{align}
    \boldsymbol{B}\boldsymbol{w}=& \left((\boldsymbol{\mu}_1-\boldsymbol{\mu}_2)(\boldsymbol{\mu}_1-\boldsymbol{\mu}_2)'\right)\boldsymbol{w}=\\
    =&(\boldsymbol{\mu}_1-\boldsymbol{\mu}_2)((\boldsymbol{\mu}_1-\boldsymbol{\mu}_2)'\boldsymbol{w})=\\
    =& b(\boldsymbol{\mu}_1-\boldsymbol{\mu}_2),
\end{align}
gdzie $b = (\boldsymbol{\mu}_1-\boldsymbol{\mu}_2)'\boldsymbol{w}$ jest skalarem.

Wówczas \@ref(eq:condFisher3) zapiszemy jako
\begin{gather}
    b(\boldsymbol{\mu}_1-\boldsymbol{\mu}_2) = \lambda\boldsymbol{S}\boldsymbol{w}\\
    \boldsymbol{w}= \frac{b}{\lambda}\boldsymbol{S}^{-1}(\boldsymbol{\mu}_1-\boldsymbol{\mu}_2)
\end{gather}

A ponieważ $b/\lambda$ jest liczbą, to kierunek najlepszej dyskryminacji grup wyznacza wektor
\begin{equation}
    \boldsymbol{w}=\boldsymbol{S}^{-1}(\boldsymbol{\mu}_1-\boldsymbol{\mu}_2).
\end{equation}

```{r rzut2, echo=FALSE, fig.align='center', fig.cap="Rzut ortogonalny w kierunku wektora $\\boldsymbol{w}$, będącego najlepiej dyskryminującym obie grupy obserwacji"}
knitr::include_graphics("images/rzut2.JPG", dpi = 100)
```

### $k$-kategorii zmiennej grupującej

Uogólnieniem tej teorii na przypadek $k$ klas otrzymujemy przez uwzględnienie $k-1$ funkcji dyskryminacyjnych. Zmienność wewnątrzgrupowa przyjmuje wówczas postać
\begin{equation}
    \boldsymbol{S}_W=\sum_{i=1}^k\boldsymbol{S}_i,
\end{equation}
gdzie $\boldsymbol{S}_i$ jest zdefiniowane jak w \@ref(eq:Sj).
Niech średnia i zmienność całkowita będą dane wzorami
\begin{equation}
    \boldsymbol{m}=\frac{1}{n}\sum_{i=1}^kn_i\boldsymbol{m}_i,
\end{equation}
\begin{equation}
    \boldsymbol{S}_T=\sum_{j=1}^k\sum_{\boldsymbol{x}\in D_j}(\boldsymbol{x}-\boldsymbol{m})(\boldsymbol{x}-\boldsymbol{m})'
\end{equation}
gdzie $\boldsymbol{m}_i$ jest określone jak w \@ref(eq:m). Wtedy zmienność międzygrupową możemy wyrazić jako
\begin{equation}
    \boldsymbol{S}_B=\sum_{i=1}^kn_i(\boldsymbol{m}_i-\boldsymbol{m})(\boldsymbol{m}_i-\boldsymbol{m})',
\end{equation}
bo $\boldsymbol{S}_T=\boldsymbol{S}_W+\boldsymbol{S}_B.$
Określamy projekcję $d$-wymiarowej przestrzeni na $k-1$-wymiarową przestrzeń za pomocą $k-1$ funkcji dyskryminacyjnych postaci
\begin{equation}
    a_j=\boldsymbol{w}_j'\boldsymbol{x}, \quad j=1,\ldots,k-1.
\end{equation}
Połączone wszystkie $k-1$ rzutów możemy zapisać jako
\begin{equation}
    \boldsymbol{a}=\boldsymbol{W}'\boldsymbol{x}.
\end{equation}

W nowej przestrzeni $k-1$-wymiarowej możemy zdefiniować
\begin{equation}
    \tilde{\boldsymbol{m}}=\frac{1}{n}\sum_{i=1}^kn_i\tilde{\boldsymbol{m}}_i,
\end{equation}
gdzie $\tilde{\boldsymbol{m}}_i= \frac{1}{n_i}\sum_{\boldsymbol{a}\in A_i}\boldsymbol{a}$, a $A_i$ jest projekcją obiektów z $i$-tej klasy na hiperpowierzchnię generowaną przez $\boldsymbol{W}$.
Dalej możemy zdefiniować zmienności miedzy- i wewnątrzgrupowe dla obiektów przekształconych przez $\boldsymbol{W}$
\begin{align}
    \tilde{\boldsymbol{S}}_W=&\sum_{i=1}^k\sum_{\boldsymbol{a}\in A_i}(\boldsymbol{a}-\tilde{\boldsymbol{m}})(\boldsymbol{a}-\tilde{\boldsymbol{m}})'\\
    \tilde{\boldsymbol{S}}_B=&\sum_{i=1}^kn_i(\tilde{\boldsymbol{m}}_i-\tilde{\boldsymbol{m}})(\tilde{\boldsymbol{m}}_i-\tilde{\boldsymbol{m}})'.
\end{align}
Łatwo można zatem pokazać, że
\begin{align}
    \tilde{\boldsymbol{S}}_W = & \boldsymbol{W}'\boldsymbol{S}_W\boldsymbol{W}\\
    \tilde{\boldsymbol{S}}_B = & \boldsymbol{W}'\boldsymbol{S}_B\boldsymbol{W}.
\end{align}
Ostatecznie warunek \@ref(eq:condFisher) w $k$-wymiarowym ujęciu można przedstawić jako
\begin{equation}
    \max_{\boldsymbol{W}}J(\boldsymbol{W})=\frac{\tilde{\boldsymbol{S}}_B}{\tilde{\boldsymbol{S}}_W}=\frac{\boldsymbol{W}'\boldsymbol{S}_B\boldsymbol{W}}{\boldsymbol{W}'\boldsymbol{S}_W\boldsymbol{W}}.
\end{equation}
Maksimum można znaleźć poprzez rozwiązanie równania charakterystycznego \begin{equation}
    |\boldsymbol{S}_B-\lambda_i\boldsymbol{S}_W|=0
\end{equation}
dla każdego $i$.

```{example}
Dla danych ze zbioru `iris` przeprowadzimy analizę dyskryminacji. Implementację metody LDA znajdziemy w pakiecie `MASS` w postaci funkcji `lda`.
```

Zaczynamy od standaryzacji zmiennych i podziału próby na uczącą i testową.

```{r message=FALSE, warning=FALSE}
library(MASS)
library(tidyverse)
iris.std <- iris %>% 
    mutate_if(is.numeric, scale)
set.seed(44)
ind <- sample(nrow(iris.std), size = 100)
dt.ucz <- iris.std[ind,]
dt.test <- iris.std[-ind,]
```

Budowa modelu

```{r}
mod.lda <- lda(Species~., data = dt.ucz)
mod.lda$prior
```

Prawdopodobieństwa *a priori* przynależności do klas przyjęto na podstawie próby uczącej.

```{r}
mod.lda$means
```


W części `means` wyświetlone są średnie poszczególnych zmiennych niezależnych w podziale na grupy. Dzięki temu można określić położenia środków ciężkości poszczególnych klas w oryginalnej przestrzeni.

```{r}
mod.lda$scaling
```

Powyższa tabela zawiera współrzędne wektorów wyznaczających funkcje dyskryminacyjne. Na ich podstawie możemy określić, która z nich wpływa najmocniej na tworzenie się nowej przestrzeni.

Obiekt `svd` przechowuje pierwiastki z $\lambda_i$, dlatego podnosząc je do kwadratu i dzieląc przez ich sumę otrzymamy udział poszczególnych zmiennych w dyskryminacji przypadków. Jak widać pierwsza funkcja dyskryminacyjna w zupełności by wystarczyła.

```{r}
mod.lda$svd^2/sum(mod.lda$svd^2)
```

Klasyfikacja na podstawie modelu

```{r}
pred.lda <- predict(mod.lda, dt.test)
```
Wynik predykcji przechowuje trzy rodzaje obiektów:

- klasy, które przypisał obiektom model (`class`);
- prawdopodobieństwa *a posteriori* przynależności do klas na podstawie modelu (`posterior`);
- współrzędne w nowej przestrzeni LD1, LD2 (`x`).

Sprawdzenie jakości klasyfikacji

```{r}
tab <- table(pred = pred.lda$class, obs = dt.test$Species)
tab
sum(diag(prop.table(tab)))
```

Jak widać z powyższej tabeli model dobrze sobie radzi z klasyfikacją obiektów. Odsetek poprawnych klasyfikacji wynosi 96%.

```{r, fig.align='center', fig.cap="Klasyfikacja w przestrzeni LD1, LD2 na podstawie modelu mod.lda"}
cbind.data.frame(obs = dt.test$Species,
                 pred.lda$x, 
                 pred = pred.lda$class) %>% 
    ggplot(aes(x = LD1, y = LD2))+
    geom_point(aes(color = pred, shape = obs), size = 2)
```

## Liniowa analiza dyskryminacyjna - podejście probabilistyczne

Jak wspomniano na wstępie (patrz rozdział \@ref(LDA)), podejście prezentowane przez Welcha polegało na minimalizacji prawdopodobieństwa popełnienia błędu przy klasyfikacji. Cała rodzina klasyfikatorów Bayesa (patrz rozdział \@ref(bayes)) polega na wyznaczeniu prawdopodobieństw *a posteriori*, na podstawie których dokonuje się decyzji o klasyfikacji obiektów. Tym razem dodajemy również założenie, że zmienne niezależne $\boldsymbol{x}=(\boldsymbol{x}_1,\ldots,\boldsymbol{x}_d)$ charakteryzują się wielowymiarowym rozkładem normalnym
\begin{equation}
    f(\boldsymbol{x}) = \frac{1}{(2\pi)^{d/2}|\boldsymbol{\Sigma}|^{1/2}}\exp\left[-\frac{1}{2}(\boldsymbol{x}-\boldsymbol{\mu})'\boldsymbol{\Sigma}(\boldsymbol{x}-\boldsymbol{\mu})\right],
    (\#eq:mnv)
\end{equation}
gdzie $\boldsymbol{\mu}$ jest wektorem średnich $\boldsymbol{x}$, a $\boldsymbol{\Sigma}$ jest macierzą kowariancji $\boldsymbol{x}$. 

```{remark, whitening}
Liniowa kombinacja zmiennych losowych o normalnym rozkładzie łącznym ma również rozkład łączny normalny. W szczególności, jeśli $A$ jest macierzą wymiaru $d\times k$ i $\boldsymbol{y} = A'\boldsymbol{x}$, to $f(\boldsymbol{y})\sim N(A'\boldsymbol{\mu}, A'\boldsymbol{\Sigma}A)$. Odpowiednia forma macierzy przekształcenia $A_w$, sprawia, że zmienne po transformacji charakteryzują się rozkładem normalnym łącznym o wariancji określonej przez $I$. Jeśli $\boldsymbol{\Phi}$ jest macierzą, której kolumny są ortonormalnymi wektorami własnymi macierzy $\boldsymbol{\Sigma}$, a $\boldsymbol{\Lambda}$ macierzą diagonalną wartości własnych, to transformacja $A_w=\boldsymbol{\Phi}\boldsymbol{\Lambda}^{-1}$ przekształca $\boldsymbol{x}$ w $\boldsymbol{y}\sim N(A_w'\boldsymbol{\mu}, I)$.
```

```{r trans, echo=FALSE, fig.align='center', fig.cap="Transformacje rozkładu normalnego łącznego. Źródło: @duda2001"}
knitr::include_graphics("images/transform.JPG", dpi = 100)
```

```{definition}
Niech $g_i(\boldsymbol{x}),\ i=1,\ldots,k$ będą pewnymi funkcjami dyskryminacyjnymi. Wówczas obiekt $\boldsymbol{x}$ klasyfikujemy do grupy $c_i$ jeśli spełniony jest warunek
\begin{equation}
    g_i(\boldsymbol{x})>g_j(\boldsymbol{x}), \quad j\neq i.
\end{equation}
``` 

W podejściu polegającym na minimalizacji prawdopodobieństwa błędnej klasyfikacji, przyjmuje się najczęściej, że
\begin{equation}
    g_i(\boldsymbol{x})=\P(c_i|\boldsymbol{x}),
\end{equation}
czyli jako prawdopodobieństwo a posteriori.
Wszystkie trzy poniższe postaci funkcji dyskryminacyjnych są dopuszczalne i równoważne ze względu na rezultat grupowania
\begin{align}
    g_i(\boldsymbol{x})=&\P(c_i|\boldsymbol{x})=\frac{\P(\boldsymbol{x}|c_i)\P(c_i)}{\sum_{i=1}^k\P(\boldsymbol{x}|c_i)\P(c_i)},\\
    g_i(\boldsymbol{x})=&\P(\boldsymbol{x}|c_i)\P(c_i),\\
    g_i(\boldsymbol{x})=&\log\P(\boldsymbol{x}|c_i)+\log\P(c_i)
    (\#eq:gi3)
\end{align}
W przypadku gdy $\boldsymbol{x}|c_i\sim N(\boldsymbol{\mu}_i, \boldsymbol{\Sigma}_i)$, to na podstawie \@ref(eq:mnv) $g_i$ danej jako \@ref(eq:gi3) przyjmuje postać
\begin{equation}
    g_i(\boldsymbol{x})=-\frac{1}{2}(\boldsymbol{x}-\boldsymbol{\mu}_i)'\boldsymbol{\Sigma}_i^{-1}(\boldsymbol{x}-\boldsymbol{\mu}_i)-\frac{d}{2}\log(2\pi)-\frac{1}{2}\log|\boldsymbol{\Sigma}_i|+\log\P(c_i).
\end{equation}

W kolejnych podrozdziałach przeanalizujemy trzy możliwe formy macierzy kowariancji.

### Przypadek gdy $\boldsymbol{\Sigma}_i=\sigma^2I$ {#przypI}

To najprostszy przypadek, zakładający niezależność zmiennych wchodzących w skład $\boldsymbol x$, których wariancje są stałe $\sigma^2$.
Wówczas $g_i$ przyjmuje postać
\begin{equation}
    g_i(\boldsymbol x)=-\frac{||\boldsymbol x-\boldsymbol \mu_i||^2}{2\sigma^2}+\log\P(c_i),
    (\#eq:row88)
\end{equation}
gdzie $||\cdot ||$ jest normą euklidesową.

Rozpisując licznik równania \@ref(eq:row88) mamy
\begin{equation}
    ||\boldsymbol x-\boldsymbol \mu_i||^2=(\boldsymbol x-\boldsymbol \mu_i)'(\boldsymbol x-\boldsymbol \mu_i).
\end{equation}
Zatem
\begin{equation}
    g_i(\boldsymbol x)=-\frac{1}{2\sigma^2}[\boldsymbol x'\boldsymbol x-2\boldsymbol \mu_i'\boldsymbol x+\boldsymbol \mu_i'\boldsymbol \mu_i]+\log\P(c_i).
\end{equation}
A ponieważ $\boldsymbol x'\boldsymbol x$ nie zależy do $i$, to funkcję dyskryminacyjną możemy zapisać jako
\begin{equation}
    g_i(\boldsymbol x)=\boldsymbol w_i'\boldsymbol x+w_{i0},
\end{equation}
gdzie $\boldsymbol w_i=\frac{1}{\sigma^2}\boldsymbol \mu_i$, a $w_{i0}=\frac{-1}{2\sigma^2}\boldsymbol \mu_i'\boldsymbol \mu_i+\log\P(c_i).$

Na podstawie funkcji dyskryminacyjnych wyznaczamy hiperpłaszczyzny decyzyjne jako zbiory punktów dla których $g_i(\boldsymbol x)=g_j(\boldsymbol x)$, gdzie $g_i, g_j$ są największe. Możemy to zapisać w następujący sposób
\begin{equation}
    \boldsymbol w'(\boldsymbol x-\boldsymbol x_0)=0,
    (\#eq:row89)
\end{equation}
gdzie 
\begin{equation}
    \boldsymbol w = \boldsymbol \mu_i-\boldsymbol \mu_j,
\end{equation}
oraz 
\begin{equation}
    \boldsymbol x_0 = \frac12(\boldsymbol \mu_i+\boldsymbol\mu_j)-\frac{\sigma^2}{||\boldsymbol \mu_i-\boldsymbol \mu_j||^2}\log\frac{\P(c_i)}{\P(c_j)}(\boldsymbol \mu_i-\boldsymbol \mu_j).
\end{equation}

Równanie \@ref(eq:row89) określa hiperpłaszczyznę przechodzącą przez $\boldsymbol x_0$ i prostopadłą do $\boldsymbol w$.

```{r hiper, echo=FALSE, fig.align='center', fig.cap="Dyskrymiancja hiperpłaszczyznami w sygucaji dwóch klas. Wykres po lewej, to ujęcie jednowymiarowe, wykresy po środu - ujęcie 2-wymiarowe i wykresy po prawej, to ujęcie 3-wymiarowe. Źródło: @duda2001"}
knitr::include_graphics("images/dyskrym1.JPG", dpi = 100)
```

### Przypadek gdy $\boldsymbol \Sigma_i=\boldsymbol \Sigma$ {#przypSig}

Przypadek ten opisuje sytuację, gdy rozkłady $\boldsymbol x$ są identyczne we wszystkich grupach ale zmienne w ich skład wchodzące nie są niezależne.
W tym przypadku funkcje dyskryminacyjne przyjmują postać
\begin{equation}
    g_i(\boldsymbol x)=\frac12(\boldsymbol x-\boldsymbol \mu_i)'\boldsymbol\Sigma^{-1}(\boldsymbol x-\boldsymbol \mu_i)+\log\P(c_i).
    (\#eq:row810)
\end{equation}
Jeśli $\P(c_i)$ są identyczne dla wszystkich klas, to można je pominąć we wzorze \@ref(eq:row810). Metryka euklidesowa ze wzoru \@ref(eq:row88) została zastąpiona przez odległość Mahalonobis'a. Podobnie ja w przypadku gdy $\boldsymbol \Sigma_i=\sigma^2I$, tak i teraz można uprościć \@ref(eq:row810) przez rozpisanie formy kwadratowej, aby otrzymać, że
\begin{equation}
    g_i(\boldsymbol x)=\boldsymbol w_i'\boldsymbol x+w_{i0},
\end{equation}
gdzie $\boldsymbol w_i=\boldsymbol\Sigma^{-1}\boldsymbol \mu_i$, a $w_{i0}=-\frac{1}{2}\boldsymbol \mu_i'\boldsymbol\Sigma^{-1}\boldsymbol \mu_i+\log\P(c_i).$

Ponieważ funkcje dyskryminacyjne są liniowe, to hiperpłaszczyzny są ograniczeniami obszarów obserwacji każdej z klas
\begin{equation}
    \boldsymbol w'(\boldsymbol x-\boldsymbol x_0)=0,
    (\#eq:row812)
\end{equation}
gdzie 
\begin{equation}
    \boldsymbol w = \boldsymbol\Sigma^{-1} (\boldsymbol \mu_i-\boldsymbol \mu_j),
\end{equation}
oraz 
\begin{equation}
    \boldsymbol x_0 = \frac12(\boldsymbol \mu_i+\boldsymbol\mu_j)-\frac{\log[ \P(c_i)/\P(c_j)]}{(\boldsymbol x-\boldsymbol \mu_i)'\boldsymbol\Sigma^{-1}(\boldsymbol x-\boldsymbol \mu_i)}(\boldsymbol \mu_i-\boldsymbol \mu_j).
\end{equation}
Tym razem $\boldsymbol{w}=\Sigma^{-1}(\boldsymbol \mu_i-\boldsymbol \mu_j)$ nie jest wektorem w kierunku $\boldsymbol \mu_i-\boldsymbol \mu_j$, więc hiperpłaszczyzna rozdzielająca obiekty różnych klas nie jest prostopadła do wektora $\boldsymbol \mu_i-\boldsymbol \mu_j$ ale przecina go w połowie (w punkcie $\boldsymbol x_0$).

```{r hiper2, echo=F, fig.align='center', fig.cap="Hiperpłaszczyzna rozdzielająca obszary innych klas może być przesunięta w kierunku bardziej prawdopodobnej klasy, jeśli prawdopodobieństwa a priori są różne. Źródło: @duda2001"}
knitr::include_graphics("images/dyskrym2.JPG", dpi = 100)
```

### Przypadek gdy $\boldsymbol \Sigma_i$ jest dowolnej postaci

Jest to najbardziej ogólny przypadek, kiedy nie nakłada się żadnych ograniczeń na macierze kowariancji grupowych. Postać funkcji dyskryminacyjnych jest następująca
\begin{equation}
    g_i(\boldsymbol x)=\boldsymbol x'\boldsymbol W_i\boldsymbol x+\boldsymbol w_i'\boldsymbol x+w_{i0}
    (\#eq:row813)
\end{equation}
gdzie 
\begin{align}
    \boldsymbol W_i = &-\frac12 \boldsymbol\Sigma_i^{-1},\\
    \boldsymbol w_i=& \boldsymbol\Sigma_i^{-1}\boldsymbol\mu_i,\\
    w_{i0} = &-\frac12\boldsymbol\mu_i'\boldsymbol\Sigma_i^{-1}\boldsymbol\mu_i-\frac12\log|\boldsymbol\Sigma_i|+\log\P(c_i).
\end{align}

Ograniczenia w ten sposób budowane są hiperpowierzchniami (nie koniecznie hiperpłaszczyznami). W literaturze ta metoda znana jest pod nazwą kwadratowej analizy dyskryminacyjnej (ang. *Quadratic Discriminant Analysis*).

```{r hiper3, echo=F, fig.align='center', fig.cap="Przykład zastosowania kwadratowej analizy dyskryminacyjnej. Pokazane są dopuszczalne postaci zbiorów ograniczających. Źródło: @duda2001"}
knitr::include_graphics("images/dyskrym3.JPG", dpi = 100)
```

```{example, caravan}
Przeprowadzimy klasyfikację na podstawie zbioru `Smarket` pakietu `ILSR`. Dane zawierają kursy indeksu giełdowego S&P500 w latach 2001-2005. Na podstawie wartości waloru z poprzednich 2 dni będziemy chcieli przewidzieć czy ruch w kolejnym okresie czasu będzie w górę czy w dół.
```

```{r}
library(ISLR)
head(Smarket)
set.seed(44)
dt.ucz <- Smarket %>% 
    mutate_if(is.numeric, scale) %>% 
    sample_frac(size = 2/3) 
dt.test <- Smarket[-as.numeric(rownames(dt.ucz)),]
mod.qda <- qda(Direction~Lag1+Lag2, data = dt.ucz)
mod.qda
```

Ponieważ funkcje dyskryminacyjne mogą być nieliniowe, to podsumowanie modelu nie zawiera współczynników funkcji. Podsumowanie zawiera tylko prawdopodobieństwa a priori i średnie poszczególnych zmiennych niezależnych w klasach.

```{r}
pred.qda <- predict(mod.qda, dt.test)
tab <- table(pred = pred.qda$class, dt.test$Direction)
tab
sum(diag(prop.table(tab)))
```

```{r qda, fig.align='center', fig.cap="Wykres klasyfikacji na podstawie QDA. Obserwacje zaznczone kolorem niebieskim są prawidłowo zaklasyfikowane, a czerwonym źle"}
library(klaR)
partimat(Direction ~ Lag1+Lag2, 
         data = dt.ucz,
         method = "qda",
         col.correct='blue',
         col.wrong='red')
```

## Analiza dyskryminacyjna metodą częściowych najmniejszych kwadratów

Analiza dyskryminacyjna metodą częściowych najmniejszych kwadratów (ang. *Partial Least Squares Discriminant Analysis*) jest wykorzystywana szczególnie w sytuacjach gdy zestaw predyktorów zwiera zmienne silnie ze sobą skorelowane. Jak wiadomo z wcześniejszych rozważań, metody dyskryminacji obserwacji są mało odporne na nadmiarowość zmiennych niezależnych. Stąd powstał pomysł zastosowania połączenia LDA z PLS (Partial Least Squares), której celem jest redukcja wymiaru przestrzeni jednocześnie maksymalizując korelację zmiennych niezależnych ze zmienną wynikową. 

Parametrem, który jest kontrolowany podczas budowy modelu jest liczba ukrytych zmiennych. Metoda PLSDA ma kilka implementacji w R, ale najbardziej znana jest funkcja `plsda` z pakietu `caret` [@kuhn]. 

```{example, plsda}
Kontynując poprzedni przykład przeprowadzimy klasyfikacje ruchu waloru korzystając z metody PLSDA. W przeciwieństwie do poprzednich funkcji `plsda` potrzebuje przekazania zbioru predyktorów i wektora zmiennej wynikowej oddzielnie, a nie za pomocą formuły. Doboru liczby zmiennych latentnych dokonamy arbitralnie.
```

```{r message=FALSE, warning=FALSE}
library(caret)
mod.plsda <- plsda(dt.ucz[,-c(1,7:9)],
                   as.factor(dt.ucz$Direction), 
                   ncomp = 2)
mod.plsda$loadings
```

Dwie ukryte zmienne użyte do redukcji wymiaru przestrzeni wyjaśniają około 40% zmienności pierwotnych zmiennych. Ładunki (`Loadings`) pokazują kontrybucje poszczególnych zmiennych w tworzenie się zmiennych ukrytych.

```{r}
pred.plsda <- predict(mod.plsda, dt.test[,-c(1,7:9)])
tab <- table(pred.plsda, dt.test$Direction)
tab
sum(diag(prop.table(tab)))
```

Ponieważ korelacje pomiędzy predyktorami w naszym przypadku nie były duże, to zastosowanie PLSDA nie poprawiło klasyfikacji w stosunku do metody QDA.

```{r}
cor(dt.ucz[,2:6])
```

## Regularyzowana analiza dyskryminacyjna

Regularyzowana analiza dyskryminacyjna (ang. *Regularized Discriminant Analysis*) powstała jako technika równoważąca zalety i wady LDA i QDA. Ze względu na zdolności generalizacyjne model LDA jest lepszy od QDA (mniejsza wariancja modelu), ale jednocześnie QDA ma bardziej elastyczną postać hiperpowierzchni brzegowych rozdzielających obiekty różnych klas. Dlatego Friedman -@friedman1989 wprowadził technikę będącą kompromisem pomiędzy LDA i QDA poprzez odpowiednie określenie macierzy kowariancji
\begin{equation}
    \tilde{\boldsymbol \Sigma}_i(\lambda) = \lambda\boldsymbol\Sigma_i + (1-\lambda)\boldsymbol\Sigma,
\end{equation}
gdzie $\boldsymbol \Sigma_i$ jest macierzą kowariancji dla $i$-tej klasy, a $\boldsymbol \Sigma$ jest uśrednioną macierzą kowariancji wszystkich klas. Zatem odpowiedni dobór parametru $\lambda$ decyduje czy poszukujemy modelu prostszego ($\lambda = 0$ odpowiada LDA), czy bardziej elastycznego ($\lambda=1$ oznacza QDA).

Dodatkowo metoda RDA pozwala na elastyczny wybór pomiędzy postaciami macierzy kowariancji wspólnej dla wszystkich klas $\boldsymbol\Sigma$. Może ona być macierzą jednostkową, jak w przypadku \@ref(przypI), co oznacza niezależność predyktorów modelu, może też być jak w przypadku \@ref(przypSig), gdzie dopuszcza się korelacje między predyktorami. Dokonuje się tego przez odpowiedni dobór parametru $\gamma$
\begin{equation}
    \boldsymbol \Sigma(\gamma) = \gamma\boldsymbol \Sigma+(1-\gamma)\sigma^2I.
\end{equation}

```{example rda}
Funkcja `rda` pakietu `klaR` jest implementacją powyższej metody. Ilustrają jej działania będzie klasyfikacja stanów z poprzedniego przykładu.
```

```{r}
library(klaR)
mod.rda <- rda(Direction~Lag1+Lag2+Lag3+Lag4+Lag5, dt.ucz)
mod.rda
```

Model został oszacowany z parametrami wyznaczonymi na podstawie sprawdzianu krzyżowego zastosowanego w funkcji `rda`.

```{r}
pred.rda <- predict(mod.rda, dt.test)
(tab <- table(pred = pred.rda$class, dt.test$Direction))
sum(diag(prop.table(tab)))
```

Jakość klasyfikacji jest na zbliżonym poziomie jak przy poprzednich metodach.

## Analiza dyskryminacyjna mieszana

Liniowa analiza dyskryminacyjna zakładała, że średnie (centroidy) w klasach są różne ale macierz kowariancji wszystkich klas jest jednakowa. Analiza dyskryminacyjna mieszana (ang. *Mixture Discriminant Analysis*) prezentuje jeszcze inne podejście ponieważ zakłada, że każda klasa może być charakteryzowana przez wiele wielowymiarowych rozkładów normalnych, których centroidy mogą się różnic, ale macierze kowariancji nie.

Wówczas rozkład dla danej klasy jest mieszaniną rozkładów składowych, a funkcja dyskryminacyjna dla $i$-tej klasy przyjmuje postać
\begin{equation}
    g_i(\boldsymbol x)\propto \sum_{k=1}^{L_i}\phi_{ik}g_{ik}(\boldsymbol x),
\end{equation} 
gdzie $L_i$ jest liczbą rozkładów składających się na $i$-tą klasę, a $\phi_{ik}$ jest współczynnikiem proporcji estymowanych w czasie uczenia modelu.

```{example, mda}
Funkcja `mda` pakietu `mda` [@R-mda] jest implementacją tej techniki w R. Jej zastosowanie pokażemy na przykładzie danych giełdowych z poprzedniego przykładu. Użyjemy domyślnych ustawień funkcji (trzy rozkłady dla każdej klasy).
```

```{r}
library(mda)
mod.mda <- mda(Direction~Lag1+Lag2+Lag3+Lag4+Lag5, dt.ucz)
mod.mda
```

```{r message=FALSE, warning=FALSE}
pred.mda <- predict(mod.mda, dt.test)
(tab <- table(pred = pred.mda, dt.test$Direction))
sum(diag(prop.table(tab)))
```

Kolejny raz model dyskryminacyjny charakteryzuje się podobną jakością klasyfikacji.

## Elastyczna analiza dyskryminacyjna

Zupełnie inne podejście w stosunku do wcześniejszych rozwiązań, prezentuje elastyczna analiza dyskryminacyjna (ang. *Flexible Discriminant Analysis*) . Kodując klasy wynikowe jako zmienne dychotomiczne (dla każdej klasy jest odrębna zmienna wynikowa) dla każdej z nich budowanych jest $k$ modeli regresji. Mogą to być modele regresji penalizowanej, jak regresja grzbietowa lub LASSO, modele regresji wielomianowej albo modele regresji sklejanej (MARS), o których będzie mowa w dalszej części tego opracowania.

Przykładowo, jeśli modelem bazowym jest MARS, to funkcja dyskryminacyjna $i$-tej klasy może być postaci
\begin{equation}
    g_i(\boldsymbol x)=\beta_0+\beta_1h(1-x_1)+\beta_2h(x_2-1)+\beta_3h(1-x_3)+\beta_4h(x_1-1),
\end{equation}
gdzie $h$ są tzw. funkcjami bazowymi postaci
\begin{equation}
    h(x)= \begin{cases}
        x, & x> 0\\
        0, & x\leq 0.
    \end{cases}
\end{equation}
Klasyfikacji dokonujemy sprawdzając znak funkcji dyskryminacyjnej $g_i$, jeśli jest dodatni, to funkcja przypisuje obiekt do klasy $i$-tej. W przeciwnym przypadku nie należy do tej klasy.  

```{r fda, echo=F, fig.align='center', fig.cap="Przykład klasyfikacji dwustanowej za pomocą metody FDA"}
knitr::include_graphics("images/fda.JPG", dpi = 100)
```

```{example, przykFDA}
Funkcja `fda` pakietu `mda` jest implementacją techniki FDA w R. Na postawie danych z poprzedniego przykładu zostanie przedstawiona zasada dziełania. Przyjmiemy domyślne ustawienia funkcji, z wyjątkiem metody estymacji modelu, jako którą przyjmiemy MARS.
```

```{r}
mod.fda <- fda(Direction ~ Lag1+Lag2, dt.ucz, method = mars)
mod.fda
```

Ponieważ, zmienna wynikowa jest dwustanowa, to powstała tylko jedna funkcja dyskryminacyjna.
Parametry modelu są następujące

```{r}
mod.fda$fit$coefficients
```

```{r}
pred.fda <- predict(mod.fda, dt.test)
(tab <- table(pred = pred.fda, dt.test$Direction))
sum(diag(prop.table(tab)))
```

Jakość klasyfikacji jest tylko nieco lepsza niż w przypadku poprzednich metod.

