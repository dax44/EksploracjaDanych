% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{book}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Eksploracja danych},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs}
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
%\usepackage[cp1250]{inputenc}
%\usepackage{amsmath}
\usepackage[MeX]{polski}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{url}
\usepackage{graphicx}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{hhline}
\usepackage{array}
\usepackage{ragged2e}
\usepackage{caption}
\usepackage{tikzsymbols}
\usepackage{textcomp}
\usepackage{parskip}
\usepackage{wrapfig}
\usepackage{booktabs}
\usepackage{lscape}
\usepackage{tabu}
\usepackage{bm}
\usepackage{booktabs}
\usepackage{fontspec}
\usepackage{bm}

\newcommand{\zb}[1]{\buildrel #1 \over \longrightarrow}
\newcommand{\PP}{\mathrm{P}}
\newcommand{\E}{\mathrm{E}}
\newcommand{\Var}{\operatorname{Var}}
\newcommand{\Cor}{\operatorname{Cor}}
\newcommand{\Cov}{\operatorname{Cov}}
\newcommand{\Tr}{\operatorname{Tr}}
\newcommand{\logit}{\operatorname{logit}}
\newcommand{\probit}{\operatorname{probit}}
\newcommand{\row}[1]{\buildrel \text{#1} \over =}
\newcommand{\pp}{\text{p.p.}}
\newcommand{\wtt}{wtedy i tylko wtedy, gdy }
\renewcommand{\arraystretch}{1.4}

\newcommand{\btwocol}{\begin{multicols}{2}}
\newcommand{\etwocol}{\end{multicols}}

%definicje twierdze
\theoremstyle{plain}
\newtheorem{tw}{Twierdzenie}[section]
\newtheorem{lm}[tw]{Lemat}
\newtheorem{uw}[tw]{Uwaga}
\newtheorem{wn}[tw]{Wniosek}

\theoremstyle{definition}
\newtheorem{df}[tw]{Definicja}
\newtheorem{prz}[tw]{Przykad}
\let\proof\uuundefined
\let\endproof\uuundefined
\newenvironment{proof}[1][Dow贸d]{\textbf{#1} }{\ \rule{0.5em}{0.5em}}

\usepackage{makeidx}
\makeindex

\title{Eksploracja danych}
\author{true}
\date{2020-03-11}

\usepackage{amsthm}
\newtheorem{theorem}{Twierdzenie}[chapter]
\newtheorem{lemma}{Lemat}[chapter]
\newtheorem{corollary}{Wniosek}[chapter]
\newtheorem{proposition}{Propozycja}[chapter]
\newtheorem{conjecture}{Przypuszczenie}[chapter]
\theoremstyle{definition}
\newtheorem{definition}{Definicja}[chapter]
\theoremstyle{definition}
\newtheorem{example}{Przykad}[chapter]
\theoremstyle{definition}
\newtheorem{exercise}{wiczenie}[chapter]
\theoremstyle{remark}
\newtheorem*{remark}{Uwaga}
\newtheorem*{solution}{Rozwizanie}
\let\BeginKnitrBlock\begin \let\EndKnitrBlock\end
\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\hypertarget{wstux119p}{%
\chapter*{Wstp}\label{wstux119p}}
\addcontentsline{toc}{chapter}{Wstp}

\hypertarget{o-ksiux105ux17cce}{%
\section*{O ksi偶ce}\label{o-ksiux105ux17cce}}
\addcontentsline{toc}{section}{O ksi偶ce}

Niniejsza ksi偶ka powstaa na bazie dowiadcze autora, a g贸wnym jej celem jest przybli偶enie czytelnikowi podstaw z dziedziny \emph{Data mining} studentom kierunku \emph{Matematyka} Politechniki Lubelskiej. Bdzie czy w sobie zar贸wno treci teoretyczne zwizane z przedstawianymi etapami eksploracji danych i budow modeli, jak i praktyczne wskaz贸wki dotczce budowy modeli w rodowisku \textbf{R} (R Core Team \protect\hyperlink{ref-R-base}{2018}). Podane zostan r贸wnie偶 wskaz贸wki, jak raportowa wyniki analiz i jak dokona waciwych ilustracji wynik贸w. Bardzo u偶yteczny w napisaniu ksi偶ki byy pakiety programu R: \textbf{bookdown} (Xie \protect\hyperlink{ref-R-bookdown}{2018}\protect\hyperlink{ref-R-bookdown}{a}), \textbf{knitr} (Xie \protect\hyperlink{ref-R-knitr}{2018}\protect\hyperlink{ref-R-knitr}{b}) oraz pakiet \textbf{rmarkdown} (Allaire et al. \protect\hyperlink{ref-R-rmarkdown}{2018}).

\hypertarget{zakres-przedmiotu}{%
\section*{Zakres przedmiotu}\label{zakres-przedmiotu}}
\addcontentsline{toc}{section}{Zakres przedmiotu}

Przedmiot \emph{Eksploracja danych} bdzie obejmowa swoim zakresem eksploracj i wizualizacj danych oraz uczenie maszynowe. Eksploracja danych ma na celu pozyskiwanie i systematyzacj wiedzy pochodzcej z danych. Odbywa si ona g贸wnie przy u偶yciu technik statystycznych, rachunku prawdopodobiestwa i metod z zakresu baz danych. Natomiast uczenie maszynowe, to ga藕 nauki (obejmuje nie tylko statystyk, cho to na niej si g贸wnie opiera) dotyczcej budowy modeli zdolnych do rozpoznawania wzorc贸w, przewidywania wartoci i klasyfikacji obiekt贸w. Data mining to szybko rosnaca grupa metod analizy danych rozwijana nie tylko przez statystyk贸w ale r贸wnie偶 przez biolog贸w, genetyk贸w, cybernetyk贸w, informatyk贸w, ekonomist贸w, osoby pracujace nad rozpoznawaniem obraz贸w i wiele innych grup zawodowych. W dzisiejszych czasch trudno sobie wyobrazi 偶ycie bez sztucznej inteligencji. Towarzyszy ona nam w codziennym, 偶yciu kiedy korzystamy z telefon贸w kom贸rkowych, wyszukiwarek internetowych, robot贸w sprztajcych, automatycznych samochod贸w, nawigacji czy gier komputerowych. Lista ta jest niepena i stale si wydu偶a.

href=``\url{https://twitter.com/i/status/1091069356367200256}''\textgreater January 31, 2019

\hypertarget{zakres-technik-stosowanych-w-data-mining}{%
\section*{Zakres technik stosowanych w data mining}\label{zakres-technik-stosowanych-w-data-mining}}
\addcontentsline{toc}{section}{Zakres technik stosowanych w data mining}

\begin{itemize}
\tightlist
\item
  statystyka opisowa
\item
  wielowymiarowa analiza danych
\item
  analiza szereg贸w czasowych
\item
  analiza danych przestrzennych
\item
  reguy asocjacji
\item
  uczenie maszynowe\footnote{ang. \emph{machine learning}}, w tym:

  \begin{itemize}
  \tightlist
  \item
    klasyfikacja
  \item
    predykcja
  \item
    analiza skupie
  \item
    \emph{text mining}
  \end{itemize}
\item
  i wiele innych
\end{itemize}

\begin{figure}
\includegraphics[width=3.33in]{images/cluster1} \caption{Przykad nienadzorowanego uczenia maszynowego.\  *殴r贸do:*https://analyticstraining.com/cluster-analysis-for-business/}\label{fig:cluster1}
\end{figure}

href=``\url{https://twitter.com/i/status/1097199751072690176}''\textgreater Ferbruary 17, 2019

\hypertarget{etapy-eksploracji-danych}{%
\section*{Etapy eksploracji danych}\label{etapy-eksploracji-danych}}
\addcontentsline{toc}{section}{Etapy eksploracji danych}

\begin{figure}
\includegraphics[width=7.35in]{images/dm_stages} \caption{Etapy eksploracji danych [@kavakiotis2017]}\label{fig:unnamed-chunk-2}
\end{figure}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Czyszczenie danych - polega na usuwaniu brak贸w danych, usuwaniu staych zmiennych, imputacji brak贸w danych oraz przygotowaniu danych do dalszych analiz.
\item
  Integracja danych - czenie danych pochodzcych z r贸偶nych 藕r贸de.
\item
  Selekcja danych - wyb贸r z bazy tych danych, kt贸re s potrzebne do dalszych analiz.
\item
  Transformacja danych - przeksztacenie i konsolidacja danych do postaci przydatnej do eksploracji.
\item
  Eksploracja danych - zastosowanie technik wymienionych wczeniej w celu odnalezienia wzorc贸w\footnote{ang. \emph{patterns}} i zale偶noci.
\item
  Ewaluacja modeli - ocena poprawnoci modeli oraz wzorc贸w z nich uzyskanych.
\item
  Wizualizacja wynik贸w - graficzne przedstawienie odkrytych wzorc贸w.
\item
  Wdra偶anie modeli - zastosowanie wyznaczonych wzorc贸w.
\end{enumerate}

\hypertarget{roz1}{%
\chapter{Import danych}\label{roz1}}

rodowisko \textbf{R} pozwala na import i export plik贸w o r贸偶nych rozszerzeniach (\texttt{txt,\ csv,\ xls,\ xlsx,\ sav,\ xpt,\ dta}, itd.)\footnote{p贸ki co nie jest mi znana funkcja pozwalajca na import plik贸w programu Statistica}. W tym celu czasami trzeba zainstalowa pakiety rozszerzajce podstawowe mo偶liwoci R-a. Najnowsza\footnote{na dzie 19.02.2019} wersja programu \href{https://www.rstudio.com}{RStudio} (v. 1.1.463)\footnote{istniej rownie偶 nowsze wersje deweloperskie} pozwala na wczytanie danych z popularnych 藕r贸de za pomoc GUI.

\begin{figure}
\includegraphics[width=4.77in]{images/import} \caption{Narzdzie do importu plik贸w programu RStudio}\label{fig:import1}
\end{figure}

Jeli dane s zapisane w trybie tekstowym (np. \texttt{txt}, \texttt{csv}), to wczytujemy je w nastpujcy spos贸b

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dane1 <-}\StringTok{ }\KeywordTok{read.table}\NormalTok{(}\StringTok{"data/dane1.txt"}\NormalTok{, }\DataTypeTok{header =}\NormalTok{ T)}
\KeywordTok{head}\NormalTok{(dane1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species
## 1          5.1         3.5          1.4         0.2  setosa
## 2          4.9         3.0          1.4         0.2  setosa
## 3          4.7         3.2          1.3         0.2  setosa
## 4          4.6         3.1          1.5         0.2  setosa
## 5          5.0         3.6          1.4         0.2  setosa
## 6          5.4         3.9          1.7         0.4  setosa
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dane2 <-}\StringTok{ }\KeywordTok{read.csv2}\NormalTok{(}\StringTok{"data/dane1.csv"}\NormalTok{, }\DataTypeTok{header =}\NormalTok{ T)}
\KeywordTok{head}\NormalTok{(dane2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species
## 1          5.1         3.5          1.4         0.2  setosa
## 2          4.9         3.0          1.4         0.2  setosa
## 3          4.7         3.2          1.3         0.2  setosa
## 4          4.6         3.1          1.5         0.2  setosa
## 5          5.0         3.6          1.4         0.2  setosa
## 6          5.4         3.9          1.7         0.4  setosa
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# funkcja pakietu readr wczytuje plik jako ramk danych w formacie tibble}
\CommentTok{# pakiet readr jest czsi wikszego pakietu tidyverse, }
\CommentTok{# kt贸ry zosta wczytany wczsniej}
\NormalTok{dane3 <-}\StringTok{ }\KeywordTok{read_csv2}\NormalTok{(}\StringTok{"data/dane1.csv"}\NormalTok{)}
\NormalTok{dane3}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 150 x 5
##    Sepal.Length Sepal.Width Petal.Length Petal.Width Species
##           <dbl>       <dbl>        <dbl>       <dbl> <chr>  
##  1          5.1         3.5          1.4         0.2 setosa 
##  2          4.9         3            1.4         0.2 setosa 
##  3          4.7         3.2          1.3         0.2 setosa 
##  4          4.6         3.1          1.5         0.2 setosa 
##  5          5           3.6          1.4         0.2 setosa 
##  6          5.4         3.9          1.7         0.4 setosa 
##  7          4.6         3.4          1.4         0.3 setosa 
##  8          5           3.4          1.5         0.2 setosa 
##  9          4.4         2.9          1.4         0.2 setosa 
## 10          4.9         3.1          1.5         0.1 setosa 
## # ... with 140 more rows
\end{verbatim}

Jeli dane s przechowywane w pliku Excel (np. \texttt{xlsx}), to importujemy je za pomoc funkcji \texttt{read\_excel} pakietu \texttt{readxl}. Domylnie jest wczytywany arkusz pierwszy ale jeli zachodzi taka potrzeba, to mo偶na ustali, kt贸ry arkusz pliku Excel ma by wczytany za pomoc paramteru \texttt{sheet}, np. \texttt{sheet=3}, co oznacza, 偶e zostanie wczytany trzeci arkusz pliku.

\begin{figure}
\includegraphics[width=5.68in]{images/excel} \caption{Fragment pliku Excel}\label{fig:excel}
\end{figure}

Poniewa偶 w pliku \texttt{dane1.xlsx} braki danych zostay zakodowane znakami \texttt{BD} oraz \texttt{-}, to nale偶y ten fakt przekaza funkcji, aby poprawnie wczyta braki danych. W przeciwnym przypadku zmienne zawierajce braki tak kodowane, bd wczytane jako zmienne znakowe.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(readxl)}
\NormalTok{dane4 <-}\StringTok{ }\KeywordTok{read_excel}\NormalTok{(}\StringTok{"data/dane1.xlsx"}\NormalTok{, }\DataTypeTok{na =} \KeywordTok{c}\NormalTok{(}\StringTok{"BD"}\NormalTok{, }\StringTok{"-"}\NormalTok{))}
\NormalTok{dane4}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 150 x 5
##    `Dugo kielich~ `Szeroko kieli~ `Dugo patka` `Szeroko pat~ Gatunki
##                <dbl>             <dbl>            <dbl>            <dbl> <chr>  
##  1               5.1               3.5              1.4              0.2 setosa 
##  2               4.9               3                1.4              0.2 setosa 
##  3               4.7               3.2              1.3              0.2 setosa 
##  4               4.6               3.1              1.5              0.2 setosa 
##  5               5                 3.6              1.4              0.2 setosa 
##  6               5.4               3.9              1.7              0.4 setosa 
##  7              NA                NA                1.4              0.3 setosa 
##  8               5                 3.4              1.5              0.2 <NA>   
##  9               4.4               2.9              1.4              0.2 setosa 
## 10               4.9               3.1              1.5              0.1 setosa 
## # ... with 140 more rows
\end{verbatim}

Istniej oczywicie jeszcze wiele innych fomat贸w danych, charakterystycznych dla program贸w, w kt贸rych s traktowane jako domylne.\footnote{do ich wczytywania stosujemy funkcje pakietu \texttt{foreign}} W szczeg贸lny spos贸b nale偶y zwr贸ci uwag na pliki o rozszerzeniu \texttt{RData} lub \texttt{rda}\footnote{oznaczaj to samo} oraz pliki \texttt{rds}. Pliki \texttt{rda} su偶 do przechowywania obiekt贸w programu \texttt{R}. Mog to by pliki danych ale r贸wnie偶 obiekty graficzne (typu wyniki funkcji \texttt{ggplot}), modele (np. wynik funkcji \texttt{lm()}), zdefiniowane funkcje i wszystkie inne obiekty, kt贸re da si zapisa w rodowisku \texttt{R}. Ponadto pliki \texttt{rda} pozawalaj na zapisanie wielu obiekt贸w w jednym pliku. Pliki o rozszerzeniu \texttt{rds} maj podobn funkcj z tym, 偶e pozwalaj na przechowywanie tylko jednego obiektu.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# wszystkie wczytane wczeniej pliki zapisuje w jednym pliku}
\KeywordTok{save}\NormalTok{(dane1, dane2, dane3, dane4, }\DataTypeTok{file =} \StringTok{"data/dane.rda"}\NormalTok{)}
\CommentTok{# plik rda zosta zapisany}
\KeywordTok{list.files}\NormalTok{(}\DataTypeTok{path =} \StringTok{"data/"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "algae.csv"    "Analysis.txt" "dane.rda"     "dane1.csv"    "dane1.txt"   
## [6] "dane1.xlsx"   "dane4.rds"    "dane4.sav"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# usuwam dane ze rodowiska R}
\KeywordTok{rm}\NormalTok{(dane1, dane2, dane3, dane4)}
\CommentTok{# sprawdzam co jest wczytane do R}
\KeywordTok{ls}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## character(0)
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# wczytuj plik rda}
\KeywordTok{load}\NormalTok{(}\StringTok{"data/dane.rda"}\NormalTok{)}
\CommentTok{# jeszcze raz sprawdzam co jest wczytane do R}
\KeywordTok{ls}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "dane1" "dane2" "dane3" "dane4"
\end{verbatim}

Zapisujc obiekty jako oddzielne pliki, mo偶na przy wczytywaniu nadawa im nazwy.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{rm}\NormalTok{(dane1, dane2, dane3)}
\KeywordTok{ls}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "dane4"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{saveRDS}\NormalTok{(dane4, }\DataTypeTok{file =} \StringTok{"data/dane4.rds"}\NormalTok{)}
\NormalTok{nowe_dane <-}\StringTok{ }\KeywordTok{readRDS}\NormalTok{(}\StringTok{"data/dane4.rds"}\NormalTok{)}
\NormalTok{nowe_dane}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 150 x 5
##    `Dugo kielich~ `Szeroko kieli~ `Dugo patka` `Szeroko pat~ Gatunki
##                <dbl>             <dbl>            <dbl>            <dbl> <chr>  
##  1               5.1               3.5              1.4              0.2 setosa 
##  2               4.9               3                1.4              0.2 setosa 
##  3               4.7               3.2              1.3              0.2 setosa 
##  4               4.6               3.1              1.5              0.2 setosa 
##  5               5                 3.6              1.4              0.2 setosa 
##  6               5.4               3.9              1.7              0.4 setosa 
##  7              NA                NA                1.4              0.3 setosa 
##  8               5                 3.4              1.5              0.2 <NA>   
##  9               4.4               2.9              1.4              0.2 setosa 
## 10               4.9               3.1              1.5              0.1 setosa 
## # ... with 140 more rows
\end{verbatim}

Opr贸cz wielu zalet takiego sposobu importu i eksportu danych jest jedna powa偶na wada, pliki te mo偶na odczyta jedynie za pomoc \texttt{R}. Osobicie polecam stosowa do importu i eksportu danych plik贸w w takich formatach, kt贸re mog przeczyta wszyscy. Jak dotd wida do importu r贸偶nych format贸w danych potrzebujemy r贸偶nych funkcji, czasami nawet z r贸偶nych pakiet贸w. Istnieje rozwizanie tej niedogodnoci 

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(rio)}
\NormalTok{dane1 <-}\StringTok{ }\KeywordTok{import}\NormalTok{(}\StringTok{"data/dane1.txt"}\NormalTok{)}
\KeywordTok{head}\NormalTok{(dane1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species
## 1          5.1         3.5          1.4         0.2  setosa
## 2          4.9         3.0          1.4         0.2  setosa
## 3          4.7         3.2          1.3         0.2  setosa
## 4          4.6         3.1          1.5         0.2  setosa
## 5          5.0         3.6          1.4         0.2  setosa
## 6          5.4         3.9          1.7         0.4  setosa
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dane2 <-}\StringTok{ }\KeywordTok{import}\NormalTok{(}\StringTok{"data/dane1.csv"}\NormalTok{, }\DataTypeTok{dec =} \StringTok{","}\NormalTok{)}
\CommentTok{# dane1.csv miay , jako znak rozdzielajcy cech i mantys liczb}
\CommentTok{# dlatego wczamy parametr dec}
\KeywordTok{head}\NormalTok{(dane2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species
## 1          5.1         3.5          1.4         0.2  setosa
## 2          4.9         3.0          1.4         0.2  setosa
## 3          4.7         3.2          1.3         0.2  setosa
## 4          4.6         3.1          1.5         0.2  setosa
## 5          5.0         3.6          1.4         0.2  setosa
## 6          5.4         3.9          1.7         0.4  setosa
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dane3 <-}\StringTok{ }\KeywordTok{import}\NormalTok{(}\StringTok{"data/dane1.xlsx"}\NormalTok{, }\DataTypeTok{na=}\KeywordTok{c}\NormalTok{(}\StringTok{"BD"}\NormalTok{,}\StringTok{"-"}\NormalTok{))}
\KeywordTok{head}\NormalTok{(dane3)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   Dugo kielicha Szeroko kielicha Dugo patka Szeroko patka Gatunki
## 1              5.1                3.5            1.4              0.2  setosa
## 2              4.9                3.0            1.4              0.2  setosa
## 3              4.7                3.2            1.3              0.2  setosa
## 4              4.6                3.1            1.5              0.2  setosa
## 5              5.0                3.6            1.4              0.2  setosa
## 6              5.4                3.9            1.7              0.4  setosa
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dane4 <-}\StringTok{ }\KeywordTok{import}\NormalTok{(}\StringTok{"data/dane4.rds"}\NormalTok{)}
\NormalTok{dane4}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 150 x 5
##    `Dugo kielich~ `Szeroko kieli~ `Dugo patka` `Szeroko pat~ Gatunki
##                <dbl>             <dbl>            <dbl>            <dbl> <chr>  
##  1               5.1               3.5              1.4              0.2 setosa 
##  2               4.9               3                1.4              0.2 setosa 
##  3               4.7               3.2              1.3              0.2 setosa 
##  4               4.6               3.1              1.5              0.2 setosa 
##  5               5                 3.6              1.4              0.2 setosa 
##  6               5.4               3.9              1.7              0.4 setosa 
##  7              NA                NA                1.4              0.3 setosa 
##  8               5                 3.4              1.5              0.2 <NA>   
##  9               4.4               2.9              1.4              0.2 setosa 
## 10               4.9               3.1              1.5              0.1 setosa 
## # ... with 140 more rows
\end{verbatim}

Lista mo偶liwoci jak daje nam pakiet \texttt{rio} (Chan and Leeper \protect\hyperlink{ref-R-rio}{2018}) jest niemal nieograniczona:\footnote{fragment pliku \texttt{help} funkcji \texttt{import}}

\begin{itemize}
\tightlist
\item
  Comma-separated data (.csv), using fread or, if fread = FALSE, read.table with row.names = FALSE and stringsAsFactors = FALSE
\item
  Pipe-separated data (.psv), using fread or, if fread = FALSE, read.table with sep = `\textbar{}', row.names = FALSE and stringsAsFactors = FALSE
\item
  Tab-separated data (.tsv), using fread or, if fread = FALSE, read.table with row.names = FALSE and stringsAsFactors = FALSE
\item
  SAS (.sas7bdat), using read\_sas.
\item
  SAS XPORT (.xpt), using read\_xpt or, if haven = FALSE, read.xport.
\item
  SPSS (.sav), using read\_sav. If haven = FALSE, read.spss can be used.
\item
  Stata (.dta), using read\_dta. If haven = FALSE, read.dta can be used.
\item
  SAS XPORT (.xpt), using read.xport.
\item
  SPSS Portable Files (.por), using read\_por.
\item
  Excel (.xls and .xlsx), using read\_excel. Use which to specify a sheet number. For .xlsx files, it is possible to set readxl = FALSE, so that read.xlsx can be used instead of readxl (the default).
\item
  R syntax object (.R), using dget
\item
  Saved R objects (.RData,.rda), using load for single-object .Rdata files. Use which to specify an object name for multi-object .Rdata files. This can be any R object (not just a data frame).
\item
  Serialized R objects (.rds), using readRDS. This can be any R object (not just a data frame).
\item
  Epiinfo (.rec), using read.epiinfo
\item
  Minitab (.mtp), using read.mtp
\item
  Systat (.syd), using read.systat
\item
  ``XBASE'' database files (.dbf), using read.dbf
\item
  Weka Attribute-Relation File Format (.arff), using read.arff
\item
  Data Interchange Format (.dif), using read.DIF
\item
  Fortran data (no recognized extension), using read.fortran
\item
  Fixed-width format data (.fwf), using a faster version of read.fwf that requires a widths argument and by default in rio has stringsAsFactors = FALSE. If readr = TRUE, import will be performed using read\_fwf, where widths should be: NULL, a vector of column widths, or the output of fwf\_empty, fwf\_widths, or fwf\_positions.
\item
  gzip comma-separated data (.csv.gz), using read.table with row.names = FALSE and stringsAsFactors = FALSE
\item
  CSVY (CSV with a YAML metadata header) using read\_csvy.
\item
  Feather R/Python interchange format (.feather), using read\_feather
\item
  Fast storage (.fst), using read.fst
\item
  JSON (.json), using fromJSON
\item
  Matlab (.mat), using read.mat
\item
  EViews (.wf1), using readEViews
\item
  OpenDocument Spreadsheet (.ods), using read\_ods. Use which to specify a sheet number.
\item
  Single-table HTML documents (.html), using read\_html. The data structure will only be read correctly if the HTML file can be converted to a list via as\_list.
\item
  Shallow XML documents (.xml), using read\_xml. The data structure will only be read correctly if the XML file can be converted to a list via as\_list.
\item
  YAML (.yml), using yaml.load
\item
  Clipboard import (on Windows and Mac OS), using read.table with row.names = FALSE
\item
  Google Sheets, as Comma-separated data (.csv)
\end{itemize}

\BeginKnitrBlock{example}
\protect\hypertarget{exm:przyk1}{}{\label{exm:przyk1} }Poni偶sza ilustracja przedstawia fragment pliku danych \texttt{Analysis.txt} zawierajcego pewne bdy, kt贸re nale偶y naprawi na etapie importu danych. Po pierwsze brakuje w nim nazw zmiennych (cho nie wida tego na rysunku). Poszczeg贸lne kolumny nazywaj si nastpujco: \texttt{season}, \texttt{size}, \texttt{speed}, \texttt{mxPH}, \texttt{mnO2}, \texttt{Cl}, \texttt{NO3}, \texttt{NH4}, \texttt{oPO4}, \texttt{PO4}, \texttt{Chla}, \texttt{a1}, \texttt{a2}, \texttt{a3}, \texttt{a4}, \texttt{a5}, \texttt{a6}, \texttt{a7}. Naszym zadaniem jest import tego pliku z jednoczesn obsug brak贸w (braki danych s zakodowane przez \texttt{XXXXXXX}) oraz nadaniem nag贸wk贸w kolumn. Plik \texttt{Analisis.txt} jest umieszczony w kagalogu \texttt{data/}. Z racji, 偶e plik dotyczy glon贸w, to dane zapiszemy pod nazw \texttt{algae}.
\EndKnitrBlock{example}

\begin{figure}
\includegraphics[width=10.67in]{images/analalysi_foto} \caption{Fragment pliku danych Analisis.txt}\label{fig:foto}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{algae <-}\StringTok{ }\KeywordTok{import}\NormalTok{(}\StringTok{'data/Analysis.txt'}\NormalTok{, }\DataTypeTok{header=}\NormalTok{F, }
                \DataTypeTok{dec=}\StringTok{'.'}\NormalTok{, }
                \DataTypeTok{col.names=}\KeywordTok{c}\NormalTok{(}\StringTok{'season'}\NormalTok{,}\StringTok{'size'}\NormalTok{,}\StringTok{'speed'}\NormalTok{,}\StringTok{'mxPH'}\NormalTok{,}\StringTok{'mnO2'}\NormalTok{,}\StringTok{'Cl'}\NormalTok{,}
                            \StringTok{'NO3'}\NormalTok{,}\StringTok{'NH4'}\NormalTok{,}\StringTok{'oPO4'}\NormalTok{,}\StringTok{'PO4'}\NormalTok{,}\StringTok{'Chla'}\NormalTok{,}\StringTok{'a1'}\NormalTok{,}\StringTok{'a2'}\NormalTok{,}
                            \StringTok{'a3'}\NormalTok{,}\StringTok{'a4'}\NormalTok{,}\StringTok{'a5'}\NormalTok{,}\StringTok{'a6'}\NormalTok{,}\StringTok{'a7'}\NormalTok{),}
                \DataTypeTok{na.strings=}\KeywordTok{c}\NormalTok{(}\StringTok{'XXXXXXX'}\NormalTok{))}
\KeywordTok{head}\NormalTok{(algae)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   season  size  speed mxPH mnO2     Cl    NO3     NH4    oPO4     PO4 Chla   a1
## 1 winter small medium 8.00  9.8 60.800  6.238 578.000 105.000 170.000 50.0  0.0
## 2 spring small medium 8.35  8.0 57.750  1.288 370.000 428.750 558.750  1.3  1.4
## 3 autumn small medium 8.10 11.4 40.020  5.330 346.667 125.667 187.057 15.6  3.3
## 4 spring small medium 8.07  4.8 77.364  2.302  98.182  61.182 138.700  1.4  3.1
## 5 autumn small medium 8.06  9.0 55.350 10.416 233.700  58.222  97.580 10.5  9.2
## 6 winter small   high 8.25 13.1 65.750  9.248 430.000  18.250  56.667 28.4 15.1
##     a2   a3  a4   a5   a6  a7
## 1  0.0  0.0 0.0 34.2  8.3 0.0
## 2  7.6  4.8 1.9  6.7  0.0 2.1
## 3 53.6  1.9 0.0  0.0  0.0 9.7
## 4 41.0 18.9 0.0  1.4  0.0 1.4
## 5  2.9  7.5 0.0  7.5  4.1 1.0
## 6 14.6  1.4 0.0 22.5 12.6 2.9
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(algae)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     season              size              speed                mxPH      
##  Length:200         Length:200         Length:200         Min.   :5.600  
##  Class :character   Class :character   Class :character   1st Qu.:7.700  
##  Mode  :character   Mode  :character   Mode  :character   Median :8.060  
##                                                           Mean   :8.012  
##                                                           3rd Qu.:8.400  
##                                                           Max.   :9.700  
##                                                           NA's   :1      
##       mnO2              Cl               NO3              NH4          
##  Min.   : 1.500   Min.   :  0.222   Min.   : 0.050   Min.   :    5.00  
##  1st Qu.: 7.725   1st Qu.: 10.981   1st Qu.: 1.296   1st Qu.:   38.33  
##  Median : 9.800   Median : 32.730   Median : 2.675   Median :  103.17  
##  Mean   : 9.118   Mean   : 43.636   Mean   : 3.282   Mean   :  501.30  
##  3rd Qu.:10.800   3rd Qu.: 57.824   3rd Qu.: 4.446   3rd Qu.:  226.95  
##  Max.   :13.400   Max.   :391.500   Max.   :45.650   Max.   :24064.00  
##  NA's   :2        NA's   :10        NA's   :2        NA's   :2         
##       oPO4             PO4              Chla               a1       
##  Min.   :  1.00   Min.   :  1.00   Min.   :  0.200   Min.   : 0.00  
##  1st Qu.: 15.70   1st Qu.: 41.38   1st Qu.:  2.000   1st Qu.: 1.50  
##  Median : 40.15   Median :103.29   Median :  5.475   Median : 6.95  
##  Mean   : 73.59   Mean   :137.88   Mean   : 13.971   Mean   :16.92  
##  3rd Qu.: 99.33   3rd Qu.:213.75   3rd Qu.: 18.308   3rd Qu.:24.80  
##  Max.   :564.60   Max.   :771.60   Max.   :110.456   Max.   :89.80  
##  NA's   :2        NA's   :2        NA's   :12                       
##        a2               a3               a4               a5        
##  Min.   : 0.000   Min.   : 0.000   Min.   : 0.000   Min.   : 0.000  
##  1st Qu.: 0.000   1st Qu.: 0.000   1st Qu.: 0.000   1st Qu.: 0.000  
##  Median : 3.000   Median : 1.550   Median : 0.000   Median : 1.900  
##  Mean   : 7.458   Mean   : 4.309   Mean   : 1.992   Mean   : 5.064  
##  3rd Qu.:11.375   3rd Qu.: 4.925   3rd Qu.: 2.400   3rd Qu.: 7.500  
##  Max.   :72.600   Max.   :42.800   Max.   :44.600   Max.   :44.400  
##                                                                     
##        a6               a7        
##  Min.   : 0.000   Min.   : 0.000  
##  1st Qu.: 0.000   1st Qu.: 0.000  
##  Median : 0.000   Median : 1.000  
##  Mean   : 5.964   Mean   : 2.495  
##  3rd Qu.: 6.925   3rd Qu.: 2.400  
##  Max.   :77.600   Max.   :31.600  
## 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{export}\NormalTok{(algae, }\DataTypeTok{file =} \StringTok{"data/algae.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{przygotowanie-danych}{%
\chapter{Przygotowanie danych}\label{przygotowanie-danych}}

Dane, kt贸re importujemy z zewntrznego 藕r贸da najczciej nie speniaj format贸w obowizujcych w \textbf{R}. Czsto zmienne zawieraj niedopuszczalne znaki szczeg贸lne, odstpy w nazwach, powt贸rzone nazwy kolumn, nazwy zmiennych zaczynajce si od liczby, czy puste wiersze lub kolumny. Przed przystpieniem do analizy zbioru nale偶y rozwa偶y ewentualne poprawki nazw zmiennych, czy usunicie pustych kolumn i wierszy. Niekt贸rych czynnoci mo偶na dokona ju偶 na etapie importu danych, stosujc pewne pakiety oraz nowe funkcjonalnoci rodowiska \textbf{RStudio}. W wikszoci przypadk贸w uchroni nas to od 偶mudnego przeksztacania typ贸w zmiennych. Oczywicie wszystkie te czynnoci czyszczenia danych mo偶na r贸wnie偶 dokona ju偶 po imporcie danych, za pomoc odpowiednich komend \textbf{R}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## przykadowe niepo偶dane nazwy zmiennych}
\NormalTok{test_df <-}\StringTok{ }\KeywordTok{as.data.frame}\NormalTok{(}\KeywordTok{matrix}\NormalTok{(}\KeywordTok{rnorm}\NormalTok{(}\DecValTok{18}\NormalTok{),}\DataTypeTok{ncol =} \DecValTok{6}\NormalTok{))}
\KeywordTok{names}\NormalTok{(test_df) <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"hIgHlo"}\NormalTok{, }\StringTok{"REPEAT VALUE"}\NormalTok{, }\StringTok{"REPEAT VALUE"}\NormalTok{,}
                    \StringTok{"% successful (2009)"}\NormalTok{,  }\StringTok{"abc@!*"}\NormalTok{, }\StringTok{""}\NormalTok{)}
\NormalTok{test_df}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##        hIgHlo REPEAT VALUE REPEAT VALUE % successful (2009)     abc@!*
## 1 -0.08763204    0.9738085   -0.6274221           -1.283824  0.3841384
## 2  0.20395352   -0.7589554   -0.1586312            1.241683 -0.8977376
## 3 -0.65478115   -0.8860280    0.3505120           -1.528660  1.1383163
##             
## 1 -0.1911037
## 2 -0.1132316
## 3 -0.4387924
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## do poprawy nazw zmiennych u偶yjemy funkcji make.names}
\KeywordTok{names}\NormalTok{(test_df) <-}\StringTok{ }\KeywordTok{make.names}\NormalTok{(}\KeywordTok{names}\NormalTok{(test_df))}
\NormalTok{test_df}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##        hIgHlo REPEAT.VALUE REPEAT.VALUE X..successful..2009.     abc...
## 1 -0.08763204    0.9738085   -0.6274221            -1.283824  0.3841384
## 2  0.20395352   -0.7589554   -0.1586312             1.241683 -0.8977376
## 3 -0.65478115   -0.8860280    0.3505120            -1.528660  1.1383163
##            X
## 1 -0.1911037
## 2 -0.1132316
## 3 -0.4387924
\end{verbatim}

Efekt kocowy cho skuteczny, to nie jest zadowalajcy. Czyszczenia nazw zmiennych mo偶na te偶 dokona stosujc funkcj \texttt{clean\_names} pakietu \textbf{janitor} (Firke \protect\hyperlink{ref-R-janitor}{2018}). Pozwala on r贸wnie偶 na usuwanie pustych wierszy i kolumn, znajdowanie zduplikowanych rekord贸w, itp.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(janitor)}
\NormalTok{test_df }\OperatorTok{%>%}\StringTok{ }\CommentTok{# aby na stae zmieni nazwy zmiennych trzeba podstawienia}
\StringTok{    }\KeywordTok{clean_names}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      h_ig_hlo repeat_value repeat_value_2 x_successful_2009        abc
## 1 -0.08763204    0.9738085     -0.6274221         -1.283824  0.3841384
## 2  0.20395352   -0.7589554     -0.1586312          1.241683 -0.8977376
## 3 -0.65478115   -0.8860280      0.3505120         -1.528660  1.1383163
##            x
## 1 -0.1911037
## 2 -0.1132316
## 3 -0.4387924
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# przykadowe dane}
\NormalTok{x <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{w1=}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{2}\NormalTok{,}\OtherTok{NA}\NormalTok{),}\DataTypeTok{w2=}\KeywordTok{c}\NormalTok{(}\OtherTok{NA}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{,}\OtherTok{NA}\NormalTok{), }\DataTypeTok{w3=}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\OtherTok{NA}\NormalTok{,}\DecValTok{1}\NormalTok{,}\OtherTok{NA}\NormalTok{))}
\NormalTok{x}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   w1 w2 w3
## 1  1 NA  1
## 2  4  2 NA
## 3  2  3  1
## 4 NA NA NA
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x }\OperatorTok{%>%}\StringTok{ }\KeywordTok{remove_empty}\NormalTok{(}\StringTok{"rows"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   w1 w2 w3
## 1  1 NA  1
## 2  4  2 NA
## 3  2  3  1
\end{verbatim}

\hypertarget{identyfikacja-brakuxf3w-danych}{%
\section{Identyfikacja brak贸w danych}\label{identyfikacja-brakuxf3w-danych}}

Zanim usuniemy jakiekolwiek braki w zbiorze, powinnimy je najpierw zidentyfikowa, okreli ich charakter, a dopiero potem ewentualnie podj decyzj o uzupenianiu brak贸w.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{algae <-}\StringTok{ }\NormalTok{rio}\OperatorTok{::}\KeywordTok{import}\NormalTok{(}\StringTok{"data/algae.csv"}\NormalTok{)}

\CommentTok{# najprociej jest wywoa summary}
\KeywordTok{summary}\NormalTok{(algae)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     season              size              speed                mxPH      
##  Length:200         Length:200         Length:200         Min.   :5.600  
##  Class :character   Class :character   Class :character   1st Qu.:7.700  
##  Mode  :character   Mode  :character   Mode  :character   Median :8.060  
##                                                           Mean   :8.012  
##                                                           3rd Qu.:8.400  
##                                                           Max.   :9.700  
##                                                           NA's   :1      
##       mnO2              Cl               NO3              NH4          
##  Min.   : 1.500   Min.   :  0.222   Min.   : 0.050   Min.   :    5.00  
##  1st Qu.: 7.725   1st Qu.: 10.981   1st Qu.: 1.296   1st Qu.:   38.33  
##  Median : 9.800   Median : 32.730   Median : 2.675   Median :  103.17  
##  Mean   : 9.118   Mean   : 43.636   Mean   : 3.282   Mean   :  501.30  
##  3rd Qu.:10.800   3rd Qu.: 57.824   3rd Qu.: 4.446   3rd Qu.:  226.95  
##  Max.   :13.400   Max.   :391.500   Max.   :45.650   Max.   :24064.00  
##  NA's   :2        NA's   :10        NA's   :2        NA's   :2         
##       oPO4             PO4              Chla               a1       
##  Min.   :  1.00   Min.   :  1.00   Min.   :  0.200   Min.   : 0.00  
##  1st Qu.: 15.70   1st Qu.: 41.38   1st Qu.:  2.000   1st Qu.: 1.50  
##  Median : 40.15   Median :103.29   Median :  5.475   Median : 6.95  
##  Mean   : 73.59   Mean   :137.88   Mean   : 13.971   Mean   :16.92  
##  3rd Qu.: 99.33   3rd Qu.:213.75   3rd Qu.: 18.308   3rd Qu.:24.80  
##  Max.   :564.60   Max.   :771.60   Max.   :110.456   Max.   :89.80  
##  NA's   :2        NA's   :2        NA's   :12                       
##        a2               a3               a4               a5        
##  Min.   : 0.000   Min.   : 0.000   Min.   : 0.000   Min.   : 0.000  
##  1st Qu.: 0.000   1st Qu.: 0.000   1st Qu.: 0.000   1st Qu.: 0.000  
##  Median : 3.000   Median : 1.550   Median : 0.000   Median : 1.900  
##  Mean   : 7.458   Mean   : 4.309   Mean   : 1.992   Mean   : 5.064  
##  3rd Qu.:11.375   3rd Qu.: 4.925   3rd Qu.: 2.400   3rd Qu.: 7.500  
##  Max.   :72.600   Max.   :42.800   Max.   :44.600   Max.   :44.400  
##                                                                     
##        a6               a7        
##  Min.   : 0.000   Min.   : 0.000  
##  1st Qu.: 0.000   1st Qu.: 0.000  
##  Median : 0.000   Median : 1.000  
##  Mean   : 5.964   Mean   : 2.495  
##  3rd Qu.: 6.925   3rd Qu.: 2.400  
##  Max.   :77.600   Max.   :31.600  
## 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## wywietl niekompletne wiersze}
\NormalTok{algae[}\OperatorTok{!}\KeywordTok{complete.cases}\NormalTok{(algae),] }\OperatorTok{%>%}\StringTok{ }\KeywordTok{head}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    season  size  speed mxPH mnO2   Cl   NO3 NH4 oPO4 PO4 Chla   a1  a2  a3   a4
## 28 autumn small   high  6.8 11.1 9.00 0.630  20  4.0  NA  2.7 30.3 1.9 0.0  0.0
## 38 spring small   high  8.0   NA 1.45 0.810  10  2.5 3.0  0.3 75.8 0.0 0.0  0.0
## 48 winter small    low   NA 12.6 9.00 0.230  10  5.0 6.0  1.1 35.5 0.0 0.0  0.0
## 55 winter small   high  6.6 10.8   NA 3.245  10  1.0 6.5   NA 24.3 0.0 0.0  0.0
## 56 spring small medium  5.6 11.8   NA 2.220   5  1.0 1.0   NA 82.7 0.0 0.0  0.0
## 57 autumn small medium  5.7 10.8   NA 2.550  10  1.0 4.0   NA 16.8 4.6 3.9 11.5
##     a5  a6  a7
## 28 2.1 1.4 2.1
## 38 0.0 0.0 0.0
## 48 0.0 0.0 0.0
## 55 0.0 0.0 0.0
## 56 0.0 0.0 0.0
## 57 0.0 0.0 0.0
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## policz niekompletne wiersze}
\KeywordTok{nrow}\NormalTok{(algae[}\OperatorTok{!}\KeywordTok{complete.cases}\NormalTok{(algae),])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 16
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## sprawdzenie liczby brak贸w w wierszach}
\KeywordTok{apply}\NormalTok{(algae, }\DecValTok{1}\NormalTok{, }\ControlFlowTok{function}\NormalTok{(x) }\KeywordTok{sum}\NormalTok{(}\KeywordTok{is.na}\NormalTok{(x)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19  20 
##   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 
##  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40 
##   0   0   0   0   0   0   0   1   0   0   0   0   0   0   0   0   0   1   0   0 
##  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60 
##   0   0   0   0   0   0   0   1   0   0   0   0   0   0   2   2   2   2   2   2 
##  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80 
##   2   6   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 
##  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 
##   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 
## 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 
##   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   0   0   0   0 
## 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 
##   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 
## 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 
##   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 
## 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 
##   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 
## 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 
##   0   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   6   0
\end{verbatim}

Wiele ciekawych funkcji do eksploracji danych znajduje si w pakiecie \textbf{DMwR} (Torgo \protect\hyperlink{ref-R-DMwR}{2013}), kt贸ry zosta przygotowany przy okazji publikacji ksi偶ki \emph{Data Mining with R}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## poszukiwanie wierszy zawierajcych wiele brak贸w}
\CommentTok{## w tym przypadku pr贸g wywietlania ustawiony jest na 0.2}
\CommentTok{## czyli 20% wszystkich kolumn}
\KeywordTok{library}\NormalTok{(DMwR)}
\KeywordTok{manyNAs}\NormalTok{(algae)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  62 199 
##  62 199
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## tworzenie zbioru pozbawionego wierszy zawierajcych wiele brak贸w}
\NormalTok{algae2 <-}\StringTok{ }\NormalTok{algae[}\OperatorTok{-}\KeywordTok{manyNAs}\NormalTok{(algae), ]}

\CommentTok{## sprawdzamy liczb wybrakowanych wierszy kt贸re pozostay}
\KeywordTok{nrow}\NormalTok{(algae2[}\OperatorTok{!}\KeywordTok{complete.cases}\NormalTok{(algae2),])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 14
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## usuwamy wszystkie wiersze z brakami}
\NormalTok{algae3 <-}\StringTok{ }\KeywordTok{na.omit}\NormalTok{(algae)}
    
\CommentTok{## wywietl wiersze z brakami}
\NormalTok{algae3[}\OperatorTok{!}\KeywordTok{complete.cases}\NormalTok{(algae3),] }\OperatorTok{%>%}\StringTok{ }\KeywordTok{head}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] season size   speed  mxPH   mnO2   Cl     NO3    NH4    oPO4   PO4   
## [11] Chla   a1     a2     a3     a4     a5     a6     a7    
## <0 rows> (or 0-length row.names)
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## liczba pozostaych wybrakowanych wierszy}
\KeywordTok{nrow}\NormalTok{(algae3[}\OperatorTok{!}\KeywordTok{complete.cases}\NormalTok{(algae3),])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## mo偶na oczywicie te偶 rcznie usuwa wiersze (nie polecam)}
\NormalTok{algae4 <-}\StringTok{ }\NormalTok{algae[}\OperatorTok{-}\KeywordTok{c}\NormalTok{(}\DecValTok{62}\NormalTok{,}\DecValTok{199}\NormalTok{),]}
\end{Highlighting}
\end{Shaded}

Mo偶na te偶 zbudowa funkcj, kt贸ra bdzie usuwaa braki danych wg naszego upodobania.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## najpierw budujemy funkcj i j kompilujemy aby R m贸g ja stosowa}
\CommentTok{## parametr prog ustala pr贸g odcicia wierszy}
\NormalTok{czysc.dane <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(dt, }\DataTypeTok{prog =} \DecValTok{0}\NormalTok{)\{}
\NormalTok{    licz.braki <-}\StringTok{ }\KeywordTok{apply}\NormalTok{(dt, }\DecValTok{1}\NormalTok{, }\ControlFlowTok{function}\NormalTok{(x) }\KeywordTok{sum}\NormalTok{(}\KeywordTok{is.na}\NormalTok{(x)))}
\NormalTok{    czyste.dt <-}\StringTok{ }\NormalTok{dt[}\OperatorTok{!}\NormalTok{(licz.braki}\OperatorTok{/}\KeywordTok{ncol}\NormalTok{(dt)}\OperatorTok{>}\NormalTok{prog), ]}
    \KeywordTok{return}\NormalTok{(czyste.dt)}
\NormalTok{\}}
    
\CommentTok{## potem j mo偶emy stosowa}
\NormalTok{algae4 <-}\StringTok{ }\KeywordTok{czysc.dane}\NormalTok{(algae)}
\KeywordTok{nrow}\NormalTok{(algae4[}\OperatorTok{!}\KeywordTok{complete.cases}\NormalTok{(algae4),])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## czycimy wiersze, kt贸rych liczba brak贸w przekracza 20% wszystkich kolumn}
\NormalTok{algae5 <-}\StringTok{ }\KeywordTok{czysc.dane}\NormalTok{(algae, }\DataTypeTok{prog =} \FloatTok{0.2}\NormalTok{)}
\KeywordTok{nrow}\NormalTok{(algae5[}\OperatorTok{!}\KeywordTok{complete.cases}\NormalTok{(algae5),])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 14
\end{verbatim}

Bardzo ciekawym narzdziem do znajdowania brak贸w danych jest funkcja \texttt{md.pattern} pakietu \textbf{mice} (van Buuren and Groothuis-Oudshoorn \protect\hyperlink{ref-R-mice}{2018}). Wskazuje on ile brak贸w wystpuje w ramach ka偶dej zmiennej.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(mice)}
\KeywordTok{md.pattern}\NormalTok{(algae)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{EksploracjaDanych_files/figure-latex/mice1-1.pdf}
\caption{\label{fig:mice1}Na czerwono zaznaczone s zmienne, kt贸re zwieraj braki danych. Liczba w wierszu po lewej stronie wykresu wskazuje ile wierszy w bazie ma dan charakterystyk, a liczba po prawej oznacza ile zmiennych byo \emph{wybrakowanych}}
\end{figure}

\begin{verbatim}
##     season size speed a1 a2 a3 a4 a5 a6 a7 mxPH mnO2 NO3 NH4 oPO4 PO4 Cl Chla
## 184      1    1     1  1  1  1  1  1  1  1    1    1   1   1    1   1  1    1
## 3        1    1     1  1  1  1  1  1  1  1    1    1   1   1    1   1  1    0
## 1        1    1     1  1  1  1  1  1  1  1    1    1   1   1    1   1  0    1
## 7        1    1     1  1  1  1  1  1  1  1    1    1   1   1    1   1  0    0
## 1        1    1     1  1  1  1  1  1  1  1    1    1   1   1    1   0  1    1
## 1        1    1     1  1  1  1  1  1  1  1    1    1   0   0    0   0  0    0
## 1        1    1     1  1  1  1  1  1  1  1    1    0   1   1    1   1  1    1
## 1        1    1     1  1  1  1  1  1  1  1    1    0   0   0    0   1  0    0
## 1        1    1     1  1  1  1  1  1  1  1    0    1   1   1    1   1  1    1
##          0    0     0  0  0  0  0  0  0  0    1    2   2   2    2   2 10   12
##       
## 184  0
## 3    1
## 1    1
## 7    2
## 1    1
## 1    6
## 1    1
## 1    6
## 1    1
##     33
\end{verbatim}

\hypertarget{zastux119powanie-brakuxf3w-danych}{%
\section{Zastpowanie brak贸w danych}\label{zastux119powanie-brakuxf3w-danych}}

Zastpowanie brak贸w danych (zwane tak偶e \emph{imputacj danych}) jest kolejnym etapem procesu przygotowania danych do analiz. Nie mo偶na jednak wyr贸偶ni uniwersalnego sposobu zastpowania brak贸w dla wszystkich mo偶liwych sytuacji. Wr贸d statystyk贸w panuje przekonanie, 偶e w przypadku wystpienia brak贸w danych mo偶na zastosowa trzy strategie:

\begin{itemize}
\tightlist
\item
  nic nie robi z brakami - co wydaje si niedorzeczne ale wcale takie nie jest, poniewa偶 istnieje wiele modeli statystycznych (np. drzewa decyzyjne), kt贸re wietnie radz sobie w sytuacji brak贸w danych. Niestety nie jest to spos贸b, kt贸ry mo偶na stosowa zawsze, poniewa偶 s r贸wnie偶 modele wymagajce kompletnoci danych jak na przykad sieci neuronowe.
\item
  usuwa braki wierszami\footnote{polega na usuwaniu wierszy zawierajcych braki} - to metoda, kt贸ra jest stosowana domylnie w przypadku kiedy tw贸rca modelu nie zadecyduje o innym sposobie obsugi luk. Metoda ta ma swoj niewtpliw zalet w postaci jasnej i prostej procedury, ale szczeg贸lnie w przypadku niewielkich zbior贸w mo偶e skutkowa obci偶eniem estymator贸w. Nie wiemy bowiem jaka warto faktycznie jest przypisana danej cesze. Jeli jest to warto bliska np. redniej, to nie wpynie znaczco na obci偶enie estymatora wartoci oczekiwanej. W przypadku, gdy r贸偶ni si ona znacznie od redniej tej cechy, to estymator mo偶e ju偶 wykazywa obci偶enie. Jego wielko zale偶y r贸wnie偶 od liczby usunitych element贸w. Nie jest zalecane usuwanie wielu wierszy ze zbioru danych i na podstawie okrojonego zbioru wyciganie wniosk贸w o populacji, poniewa偶 pr贸ba jest w贸wczas znaczco inna ni偶 populacja. Dodatkowo jeli estymatory s wyznaczane na podstawie zbioru wyra藕nie mniej licznego, to precyzja estymator贸w wyra偶ona wariancj spada. Reasumujc, jeli liczba wierszy z brakujcymi danymi jest niewielka w stosunku do caego zbioru, to usuwanie wierszy jest sensownym rozwizaniem.
\item
  uzupenianie brak贸w - to procedura polegajca na zastpowaniu brak贸w r贸偶nymi technikami. Jej niewtpliw zalet jest fakt posiadania kompletnych danych bez koniecznoci usuwania wierszy. Niestety wi偶e si to r贸wnie偶 z pewnymi wadami. Zbi贸r posiadajcy wiele brak贸w uzupenianych nawet bardzo wyrafinowanymi metodami mo偶e cechowa si zani偶on wariancj poszczeg贸lnych cech oraz tzw. przeuczeniem\footnote{wicej o zjawisku przeuczenia w dalszej czci ksi偶ki}.
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Uzupenianie redni - braki w zakresie danej zmiennej uzupeniamy redni tej zmiennej przypadk贸w uzupenionych.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{algae[}\KeywordTok{is.na}\NormalTok{(algae}\OperatorTok{$}\NormalTok{mxPH), ]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    season  size speed mxPH mnO2 Cl  NO3 NH4 oPO4 PO4 Chla   a1 a2 a3 a4 a5 a6
## 48 winter small   low   NA 12.6  9 0.23  10    5   6  1.1 35.5  0  0  0  0  0
##    a7
## 48  0
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m <-}\StringTok{ }\KeywordTok{mean}\NormalTok{(algae}\OperatorTok{$}\NormalTok{mxPH, }\DataTypeTok{na.rm =}\NormalTok{ T)}
\NormalTok{algae[}\KeywordTok{is.na}\NormalTok{(algae}\OperatorTok{$}\NormalTok{mxPH), }\StringTok{"mxPH"}\NormalTok{] <-}\StringTok{ }\NormalTok{m}
\NormalTok{algae[}\KeywordTok{is.na}\NormalTok{(algae}\OperatorTok{$}\NormalTok{mxPH), ]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] season size   speed  mxPH   mnO2   Cl     NO3    NH4    oPO4   PO4   
## [11] Chla   a1     a2     a3     a4     a5     a6     a7    
## <0 rows> (or 0-length row.names)
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Uzupenianie median - braki w zakresie danej zmiennej uzupeniamy median tej zmiennej przypadk贸w uzupenionych.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{algae }\OperatorTok{%>%}\StringTok{ }\KeywordTok{filter}\NormalTok{(}\KeywordTok{is.na}\NormalTok{(Chla)) }\OperatorTok{%>%}\StringTok{ }\NormalTok{head}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   season  size  speed mxPH mnO2 Cl   NO3 NH4 oPO4  PO4 Chla   a1  a2  a3   a4
## 1 winter small   high  6.6 10.8 NA 3.245  10    1  6.5   NA 24.3 0.0 0.0  0.0
## 2 spring small medium  5.6 11.8 NA 2.220   5    1  1.0   NA 82.7 0.0 0.0  0.0
## 3 autumn small medium  5.7 10.8 NA 2.550  10    1  4.0   NA 16.8 4.6 3.9 11.5
## 4 spring small   high  6.6  9.5 NA 1.320  20    1  6.0   NA 46.8 0.0 0.0 28.8
## 5 summer small   high  6.6 10.8 NA 2.640  10    2 11.0   NA 46.9 0.0 0.0 13.4
## 6 autumn small medium  6.6 11.3 NA 4.170  10    1  6.0   NA 47.1 0.0 0.0  0.0
##   a5  a6 a7
## 1  0 0.0  0
## 2  0 0.0  0
## 3  0 0.0  0
## 4  0 0.0  0
## 5  0 0.0  0
## 6  0 1.2  0
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{algae[}\KeywordTok{is.na}\NormalTok{(algae}\OperatorTok{$}\NormalTok{Chla), }\StringTok{"Chla"}\NormalTok{] <-}\StringTok{ }\KeywordTok{median}\NormalTok{(algae}\OperatorTok{$}\NormalTok{Chla, }\DataTypeTok{na.rm =}\NormalTok{ T)}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Wypenianie zmiennych typu wyliczeniowego, logicznego lub znakowego odbywa si najczciej przez dobranie w miejsce brakujcej wartoci, elementu powtarzajcego si najczciej wr贸d obiekt贸w obserwowanych. W pakiecie \textbf{DMwR} istnieje funkcja \texttt{centralImputation}, kt贸ra wypenia braki wartoci centraln (w przypadku zmiennych typu liczbowego - median, a dla wartoci logicznych, wyliczeniowych lub tekstowych - mod).
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{algae[}\DecValTok{48}\NormalTok{, }\StringTok{"season"}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "winter"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{algae[}\DecValTok{48}\NormalTok{, }\StringTok{"season"}\NormalTok{] <-}\StringTok{ }\OtherTok{NA}
\NormalTok{algae.uzup <-}\StringTok{ }\KeywordTok{centralImputation}\NormalTok{(algae)}
\NormalTok{algae.uzup[}\DecValTok{48}\NormalTok{,]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    season  size speed     mxPH mnO2 Cl  NO3 NH4 oPO4 PO4 Chla   a1 a2 a3 a4 a5
## 48 winter small   low 8.011734 12.6  9 0.23  10    5   6  1.1 35.5  0  0  0  0
##    a6 a7
## 48  0  0
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Jeszcze innym sposobem imputacji danych s algorytmy oparte o metod \(k\)-najbli偶szych ssiad贸w. Algorytm opiera si na prostej zasadzie, uzupeniania brakujcych wartoci median (w przypadku zmiennych ilociowych) lub mod (w przypadku zmiennych jakociowych) element贸w, kt贸re s \(k\)-tymi najbli偶szymi ssiadami w metryce
  \begin{equation}\label{knn}
   d(x,y)=\sqrt{\sum_{i=1}^{p}\delta_i(x_i,y_i)},
  \end{equation}
  gdzie \(\delta_i\) jest odlegoci pomidzy dwoma elementami ze wzgldu na \(i\)-t cech, okrelon nastpujco
  \begin{equation}\label{metryka}
   \delta_i(v_1, v_2)=\begin{cases}
       1,& \text{jeli zmienna jest jakociowa i }v_1\neq v_2\\
       0,& \text{jeli zmienna jest jakociowa i }v_1=v_2\\
       (v_1-v_2)^2,& \text{jeli zmienna jest ilociowa.}
   \end{cases}
  \end{equation}
  Odlegoci s mierzone dla zmiennych standaryzowanych. Istnieje te偶 odmiana z wagami, kt贸re malej wraz ze wzrostem odlegoci pomidzy ssiadem a uzupenianym elementem (np. \(w(d)=\exp(d)\)).
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{algae[}\DecValTok{48}\NormalTok{, ]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    season  size speed     mxPH mnO2 Cl  NO3 NH4 oPO4 PO4 Chla   a1 a2 a3 a4 a5
## 48   <NA> small   low 8.011734 12.6  9 0.23  10    5   6  1.1 35.5  0  0  0  0
##    a6 a7
## 48  0  0
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{algae <-}\StringTok{ }\NormalTok{algae }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{mutate_if}\NormalTok{(is.character, as.factor)}
\NormalTok{algae.uzup <-}\StringTok{ }\KeywordTok{knnImputation}\NormalTok{(algae, }\DataTypeTok{k =} \DecValTok{5}\NormalTok{, }\DataTypeTok{scale =}\NormalTok{ F, }\DataTypeTok{meth =} \StringTok{"median"}\NormalTok{)}
\NormalTok{algae.uzup[}\DecValTok{48}\NormalTok{,]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    season  size speed     mxPH mnO2 Cl  NO3 NH4 oPO4 PO4 Chla   a1 a2 a3 a4 a5
## 48 summer small   low 8.011734 12.6  9 0.23  10    5   6  1.1 35.5  0  0  0  0
##    a6 a7
## 48  0  0
\end{verbatim}

Istniej r贸wnie偶 du偶o bardziej zo偶one algorytmy imputacji danych oparte na bardziej wyrafinowanych technikach, takich jak: predykcja modelami liniowymi, nieliniowymi, analiza dyskryminacyjna, drzewa klasyfikacyjne. Dwa najbardziej znane pakiety zawierajce funkcje do imputacji w spos贸b zo偶ony, to \textbf{Amelia} i \textbf{mice}.

Imputacja danych z zastosowaniem pakietu \textbf{mice} wymaga podjcia kilku decyzji przed przystpieniem do uzupeniania danych:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Czy dane s MAR (ang. \emph{Missing At Random}) czy MNAR (ang. \emph{Missing Not At Random}), co oznacza, 偶e musimy si zastanowi jakie mogy by 藕r贸da brak贸w danych, przypadkowe czy systematyczne?
\item
  Nale偶y si zdecydowa na form imputacji, okrelajc struktur zale偶noci pomidzy cechami oraz rozkad bdu danej cechy?
\item
  Wybra zbi贸r danych, kt贸ry posu偶y nam za predyktory w imputacji (nie mog zawiera brak贸w).
\item
  Okrelenie, kt贸re niepene zmienne s funkcjami innych wybrakowanych zmiennych.
\item
  Okreli w jakiej kolejnoci dane bd imputowane.
\item
  Okreli parametry startowe imputacji (liczb iteracji, warunek zbie偶noci).
\item
  Okreli licz imputowanych zbior贸w.
\end{enumerate}

Ad 1. Wyr贸偶niamy nastpujce rodzaje brak贸w danych:

\begin{itemize}
\tightlist
\item
  MCAR (ang. \emph{Missing Completely At Random}) - z definicji to braki, kt贸rych pojawienie si jest kompletnie losowe. Przykadowo gdy osoba poproszona o wypenienie wieku w ankiecie bdzie rzuca monet czy wypeni t zmienn.
\item
  MAR - oznacza, 偶e obserwowane wartoci i wybrakowane maj inne rozkady ale da si je oszacowa na podstawie danych obserwowanych. Przykadowo cinienie ttnicze u os贸b, kt贸re nie wypeniy tej wartoci jest wy偶sze ni偶 u os贸b, kt贸re wpisay swoje cinienie. Okazuje si, 偶e osoby starsze z nadcinieniem nie wypeniay ankiety w tym punkcie.
\item
  MNAR - jeli nie jest speniony warunek MCAR i MAR, w贸wczas brak ma charakter nielosowy. Przykadowo respondenci osigajcy wy偶sze zarobki sukcesywnie nie wypeniaj pola ``zarobki'' i dodatkowo nie ma w ankiecie zmiennych, kt贸re pozwoliyby nam ustali, jakie to osoby.
\end{itemize}

Ad 2. Decyzja o algorytmie imputacji wynika bezporednio ze skali w jakiej jest mierzona dana zmienna. Ze wzgldu na rodzaj cechy u偶ywa bdziemy nastpujcych metod:

\begin{table}

\caption{\label{tab:methods}Zestaw metod imputacji danych stosowanych w pakiecie **mice**}
\centering
\begin{tabular}[t]{lll}
\toprule
method & type & description\\
\midrule
pmm & any & Predictive.mean.matching\\
midastouch & any & Weighted predictive mean matching\\
sample & any & Random sample from observed values\\
cart & any & Classification and regression trees\\
rf & any & Random forest imputations\\
\addlinespace
mean & numeric & Unconditional mean imputation\\
norm & numeric & Bayesian linear regression\\
norm.nob & numeric & Linear regression ignoring model error\\
norm.boot & numeric & Linear regression using bootstrap\\
norm.predict & numeric & Linear regression, predicted values\\
\addlinespace
quadratic & numeric & Imputation of quadratic terms\\
ri & numeric & Random indicator for nonignorable data\\
logreg & binary & Logistic regression\\
logreg.boot & binary & Logistic regression with bootstrap\\
polr & ordered & Proportional odds model\\
\addlinespace
polyreg & unordered & Polytomous logistic regression\\
lda & unordered & Linear discriminant analysis\\
2l.norm & numeric & Level-1 normal heteroscedastic\\
2l.lmer & numeric & Level-1 normal homoscedastic,
                                lmer\\
2l.pan & numeric & Level-1 normal homoscedastic, pan\\
\addlinespace
2l.bin & binary & Level-1 logistic, glmer\\
2lonly.mean & numeric & Level-2 class mean\\
2lonly.norm & numeric & Level-2 class normal\\
2lonly.pmm & any & Level-2 class predictive mean matching\\
\bottomrule
\end{tabular}
\end{table}

Ka偶dy z czterech typ贸w danych ma sw贸j domylny algorytm przeznaczony do imputacji:

\begin{itemize}
\tightlist
\item
  zmienna ilociowa - \texttt{pmm}
\item
  zmienna dychotomiczna (stany 0 lub 1) - \texttt{logreg}
\item
  zmienna typu wyliczeniowego (nieuporzdkowana) - \texttt{polyreg}
\item
  zmienna typu wyliczeniowego (uporzdkowana) - \texttt{polr}
\end{itemize}

Niewtpliw zalet metody \texttt{pmm} jest to, 偶e wartoci imputowane s ograniczone jedynie do obserwowanych wartoci. Metody \texttt{norm} i \texttt{norm.nob} uzupeniaj brakujce wartoci w oparciu o model liniowy. S one szybkie i efektywne w przypadku gdy reszty modelu s zbli偶one rozkadem do normalnoci. Druga z tych technik nie bierze pod uwag niepewnoci zwizanej z modelem imputujcym. Metoda \texttt{2L.norm} opiera si na dwupoziomowym heterogenicznym modelu liniowym (skupienia s wczone jako efekt do modelu). Technika \texttt{polyreg} korzysta z funkcji \texttt{multinom} pakietu \textbf{nnet} tworzcej model wielomianowy. \texttt{polr} opiera si o proporcjonalny model logitowy z pakietu \textbf{MASS}. \texttt{lda} to model dyskryminacyjny klasyfikujcy obiekty na podstawie prawdopodobiestw \emph{a posteriori}. Metoda \texttt{sample} zastpuje braki losowa wybranymi wartociami spor贸d wartoci obserwowanych.

Ad 3. Do ustalenia predyktor贸w w modelu \texttt{mice} su偶y funkcja \texttt{predictorMatrix}. Po pierwsze wywietla ona domylny ukad predyktor贸w wczanych do modelu. Mo偶na go dowolnie zmieni i podstawi do modelu imputujcego dane parametrem \texttt{predictorMatrix}. Zera wystpujce w kolejnych wierszach macierzy predyktor贸w oznaczaj pominicie tej zmiennej przy imputacji innej zmiennej. Jeli dodatkowo chcemy by jaka zmienna nie bya imputowana, to opr贸cz usunicia jej z listy predyktor贸w, nale偶y wymaza j z listy metod predykcji (\texttt{method}).

Og贸lne zalecenia co do tego jakie zmienne stosowa jako predyktory jest takie, 偶eby bra ich jak najwicej. Spowoduje to, 偶e bardziej prawdopodobny staje si brak typu MAR a nie MNAR. Z drugiej jednak strony, nierzadko zbiory zawieraj olbrzymi liczb zmiennych i wczanie ich wszystkich do modelu imputujcego nie bdzie miao sensu.

Zalecenia doboru zmiennych s nastpujce:

\begin{itemize}
\tightlist
\item
  we藕 wszystkie te zmienne, kt贸re s wczane do modelu waciwego, czyli tego za pomoc kt贸rego chcesz pozna struktur zale偶noci;
\item
  czasem do modelu imputujcego nale偶y te偶 wczy interakcje zmiennych z modelu waciwego;
\item
  dodaj zmienne, kt贸re mog mie wpyw na wybrakowane cechy;
\item
  wcz zmienne istotnie podnoszce poziom wyjanionej wariancji modelu;
\item
  na koniec usu te zmienne spor贸d predyktor贸w, kt贸re same zawieraj zbyt wiele brak贸w.
\end{itemize}

Ad 4-7. Decyzje podejmowane w tych punktach zale偶 istotnie od analizowanego zbioru i bd przedmiotem oddzielnych analiz w kontekcie rozwa偶anych zbior贸w i zada.

\BeginKnitrBlock{example}
\protect\hypertarget{exm:przyk21}{}{\label{exm:przyk21} }Dokonamy imputacji zbioru \texttt{airquality} z wykorzystaniem pakiet贸w \textbf{mice} i \textbf{VIM} (Templ et al. \protect\hyperlink{ref-R-VIM}{2019})
\EndKnitrBlock{example}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data <-}\StringTok{ }\NormalTok{airquality}
\KeywordTok{summary}\NormalTok{(data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      Ozone           Solar.R           Wind             Temp      
##  Min.   :  1.00   Min.   :  7.0   Min.   : 1.700   Min.   :56.00  
##  1st Qu.: 18.00   1st Qu.:115.8   1st Qu.: 7.400   1st Qu.:72.00  
##  Median : 31.50   Median :205.0   Median : 9.700   Median :79.00  
##  Mean   : 42.13   Mean   :185.9   Mean   : 9.958   Mean   :77.88  
##  3rd Qu.: 63.25   3rd Qu.:258.8   3rd Qu.:11.500   3rd Qu.:85.00  
##  Max.   :168.00   Max.   :334.0   Max.   :20.700   Max.   :97.00  
##  NA's   :37       NA's   :7                                       
##      Month            Day      
##  Min.   :5.000   Min.   : 1.0  
##  1st Qu.:6.000   1st Qu.: 8.0  
##  Median :7.000   Median :16.0  
##  Mean   :6.993   Mean   :15.8  
##  3rd Qu.:8.000   3rd Qu.:23.0  
##  Max.   :9.000   Max.   :31.0  
## 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# tworzymy dodatkowe braki danych}
\NormalTok{data[}\DecValTok{4}\OperatorTok{:}\DecValTok{10}\NormalTok{,}\DecValTok{3}\NormalTok{] <-}\StringTok{ }\KeywordTok{rep}\NormalTok{(}\OtherTok{NA}\NormalTok{,}\DecValTok{7}\NormalTok{)}
\NormalTok{data[}\DecValTok{1}\OperatorTok{:}\DecValTok{5}\NormalTok{,}\DecValTok{4}\NormalTok{] <-}\StringTok{ }\OtherTok{NA}
\KeywordTok{summary}\NormalTok{(data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      Ozone           Solar.R           Wind             Temp      
##  Min.   :  1.00   Min.   :  7.0   Min.   : 1.700   Min.   :57.00  
##  1st Qu.: 18.00   1st Qu.:115.8   1st Qu.: 7.400   1st Qu.:73.00  
##  Median : 31.50   Median :205.0   Median : 9.700   Median :79.00  
##  Mean   : 42.13   Mean   :185.9   Mean   : 9.806   Mean   :78.28  
##  3rd Qu.: 63.25   3rd Qu.:258.8   3rd Qu.:11.500   3rd Qu.:85.00  
##  Max.   :168.00   Max.   :334.0   Max.   :20.700   Max.   :97.00  
##  NA's   :37       NA's   :7       NA's   :7        NA's   :5      
##      Month            Day      
##  Min.   :5.000   Min.   : 1.0  
##  1st Qu.:6.000   1st Qu.: 8.0  
##  Median :7.000   Median :16.0  
##  Mean   :6.993   Mean   :15.8  
##  3rd Qu.:8.000   3rd Qu.:23.0  
##  Max.   :9.000   Max.   :31.0  
## 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{md.pattern}\NormalTok{(data)}
\end{Highlighting}
\end{Shaded}

\includegraphics{EksploracjaDanych_files/figure-latex/unnamed-chunk-3-1.pdf}

\begin{verbatim}
##     Month Day Temp Solar.R Wind Ozone   
## 104     1   1    1       1    1     1  0
## 34      1   1    1       1    1     0  1
## 3       1   1    1       1    0     1  1
## 1       1   1    1       1    0     0  2
## 4       1   1    1       0    1     1  1
## 1       1   1    1       0    1     0  2
## 1       1   1    1       0    0     1  2
## 3       1   1    0       1    1     1  1
## 1       1   1    0       1    0     1  2
## 1       1   1    0       0    0     0  4
##         0   0    5       7    7    37 56
\end{verbatim}

Do ilustracji brak贸w danych mo偶na zastosowa funkcje pakietu \textbf{VIM}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(VIM)}
\KeywordTok{aggr}\NormalTok{(data, }\DataTypeTok{numbers=}\OtherTok{TRUE}\NormalTok{, }
     \DataTypeTok{sortVars=}\OtherTok{TRUE}\NormalTok{, }
     \DataTypeTok{labels=}\KeywordTok{names}\NormalTok{(data), }
     \DataTypeTok{cex.axis=}\NormalTok{.}\DecValTok{7}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{EksploracjaDanych_files/figure-latex/unnamed-chunk-4-1.pdf}

\begin{verbatim}
## 
##  Variables sorted by number of missings: 
##  Variable      Count
##     Ozone 0.24183007
##   Solar.R 0.04575163
##      Wind 0.04575163
##      Temp 0.03267974
##     Month 0.00000000
##       Day 0.00000000
\end{verbatim}

Tak przedstawia si wykres rozrzutu zmiennych \texttt{Ozone} i \texttt{Solar.R} z uwzgldnieniem poo偶enia brak贸w danych.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{marginplot}\NormalTok{(data[}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{)])}
\end{Highlighting}
\end{Shaded}

\includegraphics{EksploracjaDanych_files/figure-latex/unnamed-chunk-5-1.pdf}

Dokonamy imputacji metod \texttt{pmm}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tempData <-}\StringTok{ }\KeywordTok{mice}\NormalTok{(data, }
                 \DataTypeTok{maxit=}\DecValTok{50}\NormalTok{, }
                 \DataTypeTok{meth=}\StringTok{'pmm'}\NormalTok{, }
                 \DataTypeTok{seed=}\DecValTok{44}\NormalTok{, }
                 \DataTypeTok{printFlag =}\NormalTok{ F)}
\KeywordTok{summary}\NormalTok{(tempData)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Class: mids
## Number of multiple imputations:  5 
## Imputation methods:
##   Ozone Solar.R    Wind    Temp   Month     Day 
##   "pmm"   "pmm"   "pmm"   "pmm"      ""      "" 
## PredictorMatrix:
##         Ozone Solar.R Wind Temp Month Day
## Ozone       0       1    1    1     1   1
## Solar.R     1       0    1    1     1   1
## Wind        1       1    0    1     1   1
## Temp        1       1    1    0     1   1
## Month       1       1    1    1     0   1
## Day         1       1    1    1     1   0
\end{verbatim}

Poniewa偶, funkcja \texttt{mice} domylnie dokonuje 5 kompletnych imputacji, mo偶emy si przekona jak bardzo r贸偶ni si poszczeg贸lne imputacje i zdecydowa si na jedn z nich.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{head}\NormalTok{(tempData}\OperatorTok{$}\NormalTok{imp}\OperatorTok{$}\NormalTok{Ozone)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     1  2  3  4  5
## 5  28 29 20 18 45
## 10 23 13  9 13 12
## 25 18 14 18 14  6
## 26 12 37  1 20 28
## 27 23  9 13 12 13
## 32 45 41 20 23 46
\end{verbatim}

Ostatecznie imputacji dokonujemy wybierajc jeden z zestaw贸w danych uzupeniajcych (np. pierwszy).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{completedData <-}\StringTok{ }\NormalTok{mice}\OperatorTok{::}\KeywordTok{complete}\NormalTok{(tempData, }\DecValTok{1}\NormalTok{)}
\KeywordTok{summary}\NormalTok{(completedData)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      Ozone           Solar.R           Wind             Temp      
##  Min.   :  1.00   Min.   :  7.0   Min.   : 1.700   Min.   :57.00  
##  1st Qu.: 18.00   1st Qu.:115.0   1st Qu.: 7.400   1st Qu.:73.00  
##  Median : 30.00   Median :207.0   Median : 9.700   Median :79.00  
##  Mean   : 41.46   Mean   :185.8   Mean   : 9.814   Mean   :78.24  
##  3rd Qu.: 61.00   3rd Qu.:259.0   3rd Qu.:11.500   3rd Qu.:85.00  
##  Max.   :168.00   Max.   :334.0   Max.   :20.700   Max.   :97.00  
##      Month            Day      
##  Min.   :5.000   Min.   : 1.0  
##  1st Qu.:6.000   1st Qu.: 8.0  
##  Median :7.000   Median :16.0  
##  Mean   :6.993   Mean   :15.8  
##  3rd Qu.:8.000   3rd Qu.:23.0  
##  Max.   :9.000   Max.   :31.0
\end{verbatim}

Za pomoc funkcji pakietu \texttt{mice} mo偶emy r贸wnie偶 przedstawi graficznie gdzie i jak zostay uzupenione dane.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{densityplot}\NormalTok{(tempData, }\OperatorTok{~}\NormalTok{Ozone}\OperatorTok{+}\NormalTok{Solar.R}\OperatorTok{+}\NormalTok{Wind}\OperatorTok{+}\NormalTok{Temp)}
\end{Highlighting}
\end{Shaded}

\includegraphics{EksploracjaDanych_files/figure-latex/unnamed-chunk-9-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{stripplot}\NormalTok{(tempData, Ozone}\OperatorTok{+}\NormalTok{Solar.R}\OperatorTok{+}\NormalTok{Wind}\OperatorTok{+}\NormalTok{Temp}\OperatorTok{~}\NormalTok{.imp, }\DataTypeTok{pch =} \DecValTok{20}\NormalTok{, }\DataTypeTok{cex =} \FloatTok{1.2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{EksploracjaDanych_files/figure-latex/unnamed-chunk-9-2.pdf}

\hypertarget{podziaux142-metod-data-mining}{%
\chapter{Podzia metod data mining}\label{podziaux142-metod-data-mining}}

\hypertarget{rodzaje-wnioskowania}{%
\section{Rodzaje wnioskowania}\label{rodzaje-wnioskowania}}

\emph{Data mining} to zestaw metod pozyskiwania wiedzy na podstawie danych. Ow wiedz zdobywamy w procesie wnioskowania na podstawie modeli. Wnioskowanie mo偶emy podzieli na dedukcyjne i indukcyjne. I tak z wnioskowaniem dedukcyjnym mamy do czynienia w贸wczas, gdy na podstawie obecnego stanu wiedzy potrafimy odpowiedzie na postawione pytanie dotyczce nowej wiedzy, stosujc reguy wnioskowania. O wnioskowaniem indukcyjnym powiemy, 偶e jest to metoda pozyskiwania wiedzy na podstawie informacji ze zbioru uczcego. Znajduje ono szerokie zastosowanie w data mining i charakteryzuje si omylnoci, poniewa偶 nawet najlepiej nauczony model na zbiorze uczcym nie zapewnia nam prawdziwoci odpowiedzi w przypadku nowych danych, a jedynie je uprawdopodabnia. Esencj wnioskowania indukcyjnego w zakresie data mining, jest poszukiwanie na podstawie danych uczcych modelu charakteryzujcego si najlepszymi waciwociami predykcyjnymi i dajcego si zastosowa do zupenie nowego zbioru danych.

Ka偶dy proces uczenia z wykorzystaniem wnioskowania indukcyjnego skada si z nastpujcych element贸w.

\hypertarget{dziedzina}{%
\subsection{Dziedzina}\label{dziedzina}}

\emph{Dziedzina} to zbi贸r wszystkich obiekt贸w pozostajcych w zainteresowaniu badacza, bdcych przedmiotem wnioskowania, oznaczana najczciej przez \(X\). Przykadowo mog to by zbiory os贸b, transakcji, urzdze, instytucji, itp.

\hypertarget{obserwacja}{%
\subsection{Obserwacja}\label{obserwacja}}

Ka偶dy element dziedziny \(x\in X\) nazywamy obserwacj. Obserwacj nazywa bdziemy zar贸wno rekordy danych ze zbioru uczcego, jak i ze zbioru testowego.

\hypertarget{atrybuty-obserwacji}{%
\subsection{Atrybuty obserwacji}\label{atrybuty-obserwacji}}

Ka偶dy obiekt z dziedziny \(x\in X\) mo偶na opisa zestawem cech (atrybut贸w), kt贸re w notacji matematycznej oznaczymy przez \(a:X\to A\), gdzie \(A\) jest przestrzeni wartoci atrybut贸w. Ka偶da obserwacja \(x\) posiadajca \(k\) cech da si wyrazi wektorowo jako \((a_1(x), a_2(x), \ldots, a_k(x))\). Dla wikszoci algorytm贸w uczenia maszynowego wyr贸偶nia si trzy typy atrybut贸w:

\begin{itemize}
\tightlist
\item
  \emph{nominalne} - posiadajce skoczon liczb stan贸w, kt贸re posiadaj porzdku;
\item
  \emph{porzdkowe} - posiadajce skoczon liczb stan贸w z zachowaniem porzdku;
\item
  \emph{cige} - przyjmujce wartoci numeryczne.
\end{itemize}

Czsto jeden z atrybut贸w spenia specjaln rol, poniewa偶 stanowi realizacj cechy, kt贸r traktujemy jako wyjciow (ang. \emph{target value attribute}). W tym przypadku powiemy o \textbf{nadzorowanym uczeniu maszynowym}. Jeli zmiennej wyjciowej nie ma dziedzinie, to m贸wimy o \textbf{nienadzorowanym uczeniu maszynowym}.

\hypertarget{zbiuxf3r-uczux105cy}{%
\subsection{Zbi贸r uczcy}\label{zbiuxf3r-uczux105cy}}

Zbiorem uczcym \(T\) (ang. \emph{training set}) nazywamy podzbi贸r \(D\) dziedziny \(X\) (czyli \(T\subseteq D\subseteq X\)), gdzie zbi贸r \(D\) stanowi og贸 dostpnych obserwacji z dziedziny \(X\). Zbi贸r uczcy zawiera informacje dotyczce badanego zjawiska, na podstawie kt贸rych, dokonuje si doboru modelu, selekcji cech istotnych z punktu widzenia wasnoci predykcyjnych lub jakoci klasyfikacji, budowy modelu oraz optymalizacji jego parametr贸w. W przypadku uczenia z nauczycielem (nadzorowanego) zbi贸r \(T\) zawiera informacj o wartociach atrybut贸w zmiennej wynikowej.

\hypertarget{zbiuxf3r-testowy}{%
\subsection{Zbi贸r testowy}\label{zbiuxf3r-testowy}}

Zbi贸r testowy \(T'\) (ang. \emph{test set}) bdcy dopenieniem zbioru uczcego do zbioru \(D\), czyli \(T'=D\setminus T\), stanowi zestaw danych su偶cy do oceny poprawnoci modelu nadzorowanego. W przypadku metod nienadzorowanych raczej nie stosuje si zbior贸w testowych.

\hypertarget{model}{%
\subsection{Model}\label{model}}

Model to narzdzie pozyskiwania wiedzy na podstawie zbioru uczcego. Nauczony model jest zbiorem regu \(f\), kt贸rego zadaniem jest oszacowanie wielkoci wartoci wynikowej lub odpowiednia klasyfikacja obiekt贸w. W zadaniu grupowania obiekt贸w (ang. \emph{clustering task}), celem modelu jest podanie grup mo偶liwie najbardziej jednorodnych przy zadanym zestawie zmiennych oraz ustalonej liczbie skupie (czasami wyznaczenie liczby skupie jest r贸wnie偶 czci zadania stawianego przed modelem).

\hypertarget{jakoux15bux107-dopasowania-modelu}{%
\subsection{Jako dopasowania modelu}\label{jakoux15bux107-dopasowania-modelu}}

Do oceny jakoci dopasowania modelu wykorzystuje si, w zale偶noci od zadania, wiele wsp贸czynnik贸w (np. dla zada regresyjnych s to bd rednio-kwadratowy - ang. \emph{Mean Square Error}, a dla zada klasyfikacyjnych - trafno - ang. \emph{Accuracy}). Mo偶emy m贸wi dw贸ch rodzajach dopasowania modeli:

\begin{itemize}
\tightlist
\item
  poziom dopasowania na zbiorze uczcym
\item
  poziom dopasowania na zbiorze testowym (oczywicie z punktu widzenia utylitarnoci modelu ten wsp贸czynnik jest wa偶niejszy).
\end{itemize}

W sytuacji, w kt贸rej model wykazuje dobre charakterystyki jakoci dopasowania na zbiorze uczcym ale sabe na testowym, m贸wimy o zjawisku przeuczenia modelu (ang. \emph{overfitting}). Oznacza to, 偶e model wskazuje predykcj poprawnie jedynie dla zbioru treningowego ale ma saba wasnoci generalizacyjne nowe przypadki danych. Takie model nie przedstawiaj znaczcej wartoci w odkrywaniu wiedzy w spos贸b indukcyjny.

Z drugiej strony parametry dopasowania modelu mog pokazywa sabe dopasowanie, zar贸wno na zbiorze uczcym, jak i testowym. W贸wczas r贸wnie偶 model nie jest u偶yteczny w pozyskiwaniu wiedzy na temat badanego zjawiska, a sytuacj tak nazywamy niedouczeniem (ang. \emph{underfitting}).

\begin{figure}
\includegraphics[width=6.1in]{images/unde_over_fitting} \caption{Przykady niedoucznia (wykresy 1 i 4), poprawego modelu (2 i 5) i przeuczenia (3 i 6). Pierwszy wiersz wykres贸w pokazuje klasyfikacj na podstawie modelu na zbiorze uczcym, a drugi na zbiorze testowym. Wykres na dole pokazuje zwizek pomidzy zo偶onoci modelu a wielkoci bdu predykcji. *殴r贸do*: https://cambridgecoding.wordpress.com/2016/03/24/misleading-modelling-overfitting-cross-validation-and-the-bias-variance-trade-off/}\label{fig:unnamed-chunk-10}
\end{figure}

\hypertarget{modele-regresyjne}{%
\section{Modele regresyjne}\label{modele-regresyjne}}

Jednym z rodzaj贸w zada bazujcym na wnioskowaniu indukcyjnym jest model regresyjny. Nale偶y on do grupy metod nadzorowanych, kt贸rych celem jest oszacowanie wartoci cechy wyjciowej (kt贸ra jest ilociowa) na podstawie zestawu predyktor贸w, kt贸re mog by ilociowe i jakociowe. Uczenie takich modeli odbywa si poprzez optymalizacj funkcji celu (np. \(MSE\)) na podstawie zbioru uczcego.

\hypertarget{modele-klasyfikacyjne}{%
\section{Modele klasyfikacyjne}\label{modele-klasyfikacyjne}}

Podobnie jak modele regresyjne, modele klasyfikacyjne nale偶 do grupy metod nadzorowanego uczenia maszynowego. Ich zadaniem jest waciwa klasyfikacja obiekt贸w na podstawie wielkoci predyktor贸w. Odpowiedzi modelu jest zawsze cecha typu jakociowego, natomiast predyktory mog mie dowolny typ. Wyr贸偶nia si klasyfikacj dwu i wielostanow. Lista modeli realizujcych klasyfikacj binarn jest nieco du偶sza ni偶 w przypadku modeli z wielostanow cech wynikow. Proces uczenia modelu klasyfikacyjnego r贸wnie偶 opiera si na optymalizacji funkcji celu. Tym razem s to zupenie inne miary jakoci dopasowania (np. trafno, czyli odsetek poprawnych klasyfikacji).

\hypertarget{modele-grupujux105ce}{%
\section{Modele grupujce}\label{modele-grupujux105ce}}

Bardzo szerok gam modeli nienadzorowanych stanowi metody analizy skupie. Ich zadaniem jest grupowanie obiekt贸w w mo偶liwie najbardziej jednorodne grupy, na podstawie wartoci atrybut贸w poddanych analizie. Poniewa偶 s to metody ``bez nauczyciela'', to ocena ich przydatnoci ma nieco inny charakter i cho istniej r贸偶ne wska藕niki jakoci grupowania, to trudno tu o obiektywne wskazanie najlepszego rozwizania.

\hypertarget{drzewa-decyzyjne}{%
\chapter{Drzewa decyzyjne}\label{drzewa-decyzyjne}}

\emph{Drzewo decyzyjne}\footnote{wygldem przypomina odwr贸cone drzewo, std nazwa} jest struktur hierarchiczn przedstawiajc model klasyfikacyjny lub regresyjny. Stosowane s szczeg贸lnie czsto w贸wczas, gdy funkcyjna posta zwizku pomidzy predyktorami a zmienn wynikow jest nieznana lub ci偶ka do ustalenia.
Ka偶de drzewo decyzyjne skada si z korzenia (ang. \emph{root}), wz贸w (ang. \emph{nodes}) i lici (ang. \emph{leaves}). Korzeniem nazywamy pocztkowy wze drzewa, z kt贸rego poprzez podziay (ang. \emph{splits}) powstaj kolejne wzy potomne. Kocowe wzy, kt贸re nie podlegaj podziaom nazywamy limi, a linie czce wzy nazywamy gaziami (ang. \emph{branches}).

Jeli drzewo su偶y do zada klasyfikacyjnych, to licie zawieraj informacj o tym, kt贸ra klasa w danym cigu podzia贸w jest najbardziej prawdopodobna. Natomiast, jeli drzewo jest regresyjne, to licie zawieraj warunkowe miary tendencji centralnej (najczciej redni) wartoci zmiennej wynikowej. Warunek stanowi szereg podzia贸w doprowadzajcy do danego wza terminalnego (licia). W obu przypadkach (klasyfikacji i regresji) drzewo ``d偶y'' do takiego podziau by kolejne wzy, a co za tym idzie r贸wnie偶 licie, byy ja najbardziej jednorodne ze wzgldu na zmienn wynikow.

\begin{figure}
\centering
\includegraphics{EksploracjaDanych_files/figure-latex/unnamed-chunk-11-1.pdf}
\caption{\label{fig:unnamed-chunk-11}Przykad dziaania drzewa regresyjnego. Wykes w lewym g贸rnym rogu pokazuje prawdziw zale偶no, wyres po prawej stronie jest ilustracj drzewa decyzyjnego, a wykres w lewym dolnym rogu pokazuje dyskretyzacj przestrzeni dokonan przez drzewo, czyli spos贸b jego dziaania.}
\end{figure}

\hypertarget{wux119zux142y-i-gaux142ux119zie}{%
\section{Wzy i gazie}\label{wux119zux142y-i-gaux142ux119zie}}

Ka偶dy podzia rozdziela dziedzin \(X\) na dwa lub wicej podobszar贸w dziedziny i w贸wczas ka偶da obserwacja wza nadrzdnego jest przyporzdkowana wzom potomnym. Ka偶dy odchodzcy wze potomny jest poczony gazi, kt贸ra to wi偶e si cile z mo偶liwymi wynikami podziau. Ka偶dy \(\mathbf{n}\)-ty wze mo偶na opisa jako podzbi贸r dziedziny w nastpujcy spos贸b
\begin{equation}
    X_{\mathbf{n}}=\{x\in X|t_1(x)=r_1,t_2(x)=r_2,\ldots,t_k(x)=r_k\},
\end{equation}
gdzie \(t_1,t_2,\ldots,t_k\) s podziaami, kt贸re przeprowadzaj \(x\) w obszary \(r_1, r_2,\ldots, r_k\). Przez
\begin{equation}
    S_{\mathbf{n}, t=r}=\{x\in S|t(x)=r\}
\end{equation}
rozumiemy, 偶e dokonano takiego cigu podzia贸w zbioru \(S\), 偶e jego wartoci znalazy si w \(\mathbf{n}\)-tym w藕le.

\hypertarget{rodzaje-reguux142-podziaux142u}{%
\section{Rodzaje regu podziau}\label{rodzaje-reguux142-podziaux142u}}

Najczciej wystpujce reguy podziau w drzewach decyzyjnych s jednowymiarowe, czyli warunek podziau jest generowany na podstawie jednego atrybutu. Istniej podziay wielowymiarowe ale ze wzgldu na zo偶ono obliczeniow s rzadziej stosowane.

\hypertarget{podziaux142y-dla-atrybutuxf3w-ze-skali-nominalnej}{%
\subsection{Podziay dla atrybut贸w ze skali nominalnej}\label{podziaux142y-dla-atrybutuxf3w-ze-skali-nominalnej}}

Istniej dwa typy regu podziau dla skali nominalnej:

\begin{itemize}
\tightlist
\item
  oparte na wartoci atrybutu (ang. \emph{value based}) - w贸wczas funkcja testowa przyjmuje posta \(t(x)=a(x)\), czyli podzia generuj wartoci atrybutu;
\item
  oparte na r贸wnoci (ang. \emph{equality based}) - gdzie funkcja testowa jest zdefiniowana jako
  \begin{equation}
    t(x)= \begin{cases}
        1, &\text{ gdy } a(x)=\nu\\
        0, & \text{ w przeciwnym przypadku},
    \end{cases}
  \end{equation}
  gdzie \(\nu\in A\) i \(A\) jest zbiorem mo偶liwych wartoci \(a\). W tym przypadku podzia jest dychotomiczny, albo obiekt ma warto atrybutu r贸wn \(\nu\), albo go nie ma.
\end{itemize}

\hypertarget{podziaux142y-dla-atrybutuxf3w-ze-skali-ciux105gux142ej}{%
\subsection{Podziay dla atrybut贸w ze skali cigej}\label{podziaux142y-dla-atrybutuxf3w-ze-skali-ciux105gux142ej}}

Reguy podziau stosowane do skali cigej, to:

\begin{itemize}
\tightlist
\item
  oparta na nier贸wnociach (ang. \emph{inequality based}) - zdefiniowana jako
  \begin{equation}
  t(x) = \begin{cases}
    1, &\text{ gdy }a(x)\leq \nu\\
    0, & \text{w przeciwnym przypadku},
    \end{cases}
  \end{equation}
  gdzie \(\nu\in A\);
\item
  przedziaowa (ang. \emph{interval based}) - zdefiniowana jako
  \begin{equation}
    t(x) = \begin{cases}
        1, &\text{ gdy }a(x) \in I_1\\
        2, &\text{ gdy }a(x) \in I_2\\
        \vdots & \\
        k, &\text{ gdy }a(x) \in I_k\\
    \end{cases}
  \end{equation}
  gdzie \(I_1,I_2,\ldots,I_k\subset A\) stanowi rozczny podzia (przedziaami) przeciwdziedziny \(A\).
\end{itemize}

\hypertarget{podziaux142y-dla-atrybutuxf3w-ze-skali-porzux105dkowej}{%
\subsection{Podziay dla atrybut贸w ze skali porzdkowej}\label{podziaux142y-dla-atrybutuxf3w-ze-skali-porzux105dkowej}}

Podziay te mog wykorzystywa oba wczeniej wspomniane typy, w zale偶noci od potrzeb.

\hypertarget{algorytm-budowy-drzewa}{%
\section{Algorytm budowy drzewa}\label{algorytm-budowy-drzewa}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  stw贸rz pocztkowy wze (korze) i oznacz go jako \emph{otwarty};
\item
  przypisz wszystkie mo偶liwe rekordy do wza pocztkowego;
\item
  \textbf{dop贸ki} istniej otwarte wzy \textbf{wykonuj}:

  \begin{itemize}
  \tightlist
  \item
    wybierz wze \(\mathbf{n}\), wyznacz potrzebne statystyki opisowe zmiennej zale偶nej dla tego wza i przypisz warto docelow;
  \item
    \textbf{jeli} kryterium zatrzymania podziau jest spenione dla wza \(n\), \textbf{to} oznacz go za \textbf{zamknity};
  \item
    \textbf{w przeciwnym przypadku} wybierz podzia \(r\) element贸w wza \(\mathbf{n}\), i dla ka偶dego podzbioru podziau stw贸rz wze ni偶szego rzdu (potomka) \(\mathbf{n}_r\) oraz oznacz go jako \emph{otwarty};
  \item
    nastpnie przypisz wszystkie przypadki generowane podziaem \(r\) do odpowiednich wz贸w potomk贸w \(\mathbf{n}_r\);
  \item
    oznacza wze \(\mathbf{n}\) jako \emph{zamknity}.
  \end{itemize}
\end{enumerate}

Spos贸b przypisywania wartoci docelowej wi偶e si cile z rodzajem drzewa. W drzewach regresyjnych chodzi o wyliczenie redniej lub mediany dla obserwacji ujtych w danym w藕le. Natomiast w przypadku drzewa klasyfikacyjnego, wyznacza si wartoci prawdopodobiestw przynale偶noci obserwacji znajdujcej si w danym w藕le do poszczeg贸lnych klas
\begin{equation}
    \P(d|\mathbf{n})=\P_{T_\mathbf{n}}(d)=\frac{|T_\mathbf{n}^d|}{|T_\mathbf{n}|},
\end{equation}
gdzie \(T_\mathbf{n}\) oznaczaj obserwacje zbioru uczcego znajdujce si w w藕le \(\mathbf{n}\), a \(T_\mathbf{n}^d\) oznacza dodatkowo podzbi贸r zbioru uczcego w \(\mathbf{n}\) w藕le, kt贸re nale偶 do klasy \(d\). Oczywicie klasyfikacja na podstawie otrzymanych prawdopodobiestw w danym w藕le jest dokonana przez wyb贸r klasy charakteryzujcej si najwy偶szym prawdopodobiestwem.

\hypertarget{kryteria-zatrzymania}{%
\section{Kryteria zatrzymania}\label{kryteria-zatrzymania}}

Kryterium zatrzymania jest warunkiem, kt贸ry decyduje o tym, 偶e dany wze uznajemy za zamknity i nie dokonujemy dalszego jego podziau. Wyr贸偶niamy nastpujce kryteria zatrzymania:

\begin{itemize}
\tightlist
\item
  jednorodno wza - w przypadku drzewa klasyfikacyjnego mo偶e zdarzy si sytuacja, 偶e wszystkie obserwacje wza bd pochodziy z jednej klasy. W贸wczas nie ma sensu dokonywa dalszego podziau wza;
\item
  wze jest pusty - zbi贸r przypisanych obserwacji zbioru uczcego do \(\mathbf{n}\)-tego wza jest pusty;
\item
  brak regu podziau - wszystkie reguy podziau zostay wykorzystane, zatem nie da si stworzy potomnych wz贸w, kt贸re charakteryzowayby si wiksz homogenicznoci;
\end{itemize}

Warunki ujte w pierwszych dw贸ch kryteriach mog by nieco zagodzone, poprzez zatrzymanie podzia贸w w贸wczas, gdy prawdopodobiestwo przynale偶enia do pewnej klasy przekroczy ustalony pr贸g lub gdy liczebno wza spadnie poni偶ej ustalonej wartoci.

W literaturze tematu istnieje jeszcze jedno czsto stosowane kryterium zatrzymania oparte na wielkoci drzewa. Wze potomny ustala si jako zamknity, gdy dugo cie偶ki dojcia do nie go przekroczy ustalon warto.

\hypertarget{reguux142y-podziaux142u}{%
\section{Reguy podziau}\label{reguux142y-podziaux142u}}

Wa偶nym elementem algorytmu tworzenia drzewa regresyjnego jest \emph{regua podziau}. Dobierana jest w taki spos贸b aby zmaksymalizowa zdolnoci generalizacyjne drzewa. Zo偶ono drzewa mierzona jest najczciej przecitn liczb podzia贸w potrzebnych do dotarcia do licia zaczynajc od korzenia. Licie s najczciej tworzone w贸wczas gdy dyspersja wartoci wynikowej jest stosunkowo maa lub wze zawiera w miar homogeniczne obserwacje ze wzgldu na przynale偶no do klasy zmiennej wynikowej. W przypadku drzew regresyjnych zmienno na poziomie wz贸w jest dobr miar su偶c do definiowania podziau w w藕le. I tak, jeli pewien podzia generuje nam stosunkowo mae dyspersje wartoci docelowych w wzach potomnych, to mo偶na ten podzia uzna za waciwy. Jeli \(T_n\) oznacza zbi贸r rekord贸w nale偶cych do wza \(n\), a \(T_{n,t=r}\) s podzbiorami generowanymi przez podzia \(r\) w wzach potomnych dla \(n\), to dyspersj wartoci docelowej \(f\) bdziemy oznaczali nastpujco
\begin{equation}\label{dyspersja}
     \operatorname{disp}_{T_{n,t=r}}(f).
\end{equation}

Regu podziau mo偶emy okrela poprzez minimalizacj redniej wa偶onej dyspersji wartoci docelowej nastpujcej postaci
\begin{equation}\label{reg_podz}
        \operatorname{disp}_n(f|t)=\sum_{r\in R_t}\frac{|T_{n,t=r}|}{|T_n|}\operatorname{disp}_{T_{n,t=r}}(f),
\end{equation}
gdzie \(|\  |\) oznacza moc zbioru, a \(R_t\) zbi贸r wszystkich mo偶liwych wartoci reguy podziau. Czasami wygodniej bdzie maksymalizowa przyrost dyspersji (lub spadek)
\begin{equation}\label{przyrost}
        \bigtriangleup \operatorname{disp}_n(f|t)=\operatorname{disp}_n(f)-\sum_{r\in R_t}\frac{|T_{n,t=r}|}{|T_n|}\operatorname{disp}_{T_{n,t=r}}(f).
\end{equation}

Miar heterogenicznoci wz贸w ze wzgldu na zmienn wynikow (ang. \emph{impurity}) w drzewach klasyfikacyjnych, kt贸ra pozwala na tworzenie kolejnych podzia贸w wza, s najczciej wska藕nik Gini'ego i entropia (Breiman \protect\hyperlink{ref-breiman1998}{1998}).

Entropi podzbioru uczcego w w藕le \(\mathbf{n}\), wyznaczamy wg wzoru
\begin{equation}
E_{T_{\mathbf{n}}}(c|t) = \sum_{x\in R_t} \frac{|T_{\mathbf{n}, t=r}|}{|T_{\mathbf{n}}|}E_{T_{\mathbf{n}, t=r}}(c),
\end{equation}
gdzie \(t\) jest podziaem (kandydatem), \(r\) potencjalnym wynikiem podziau \(t\), \(c\) jest oznaczeniem klasy zmiennej wynikowej, a
\begin{equation}
    E_{T_{\mathbf{n}, t=r}}(c) = \sum_{d\in C}-\P_{T_{\mathbf{n}, t=r}}(c=d)\log\P_{T_{\mathbf{n}, t=r}}(c=d),
\end{equation}
przy czym
\begin{equation}
    \P_{T_{\mathbf{n}, t=r}}(c=d)= \P_{T_{\mathbf{n}}}(c=d|t=r).
\end{equation}

Podobnie definiuje si indeks Gini'ego
\begin{equation}
Gi_{T_{\mathbf{n}}}(c|t) = \sum_{x\in R_t} \frac{|T_{\mathbf{n}, t=r}|}{|T_{\mathbf{n}}|}Gi_{T_{\mathbf{n}, t=r}}(c),
\end{equation}
gdzie
\begin{equation}
    Gi_{T_{\mathbf{n}, t=r}}(c) = \sum_{d\in C}\P_{T_{\mathbf{n}, t=r}}(c=d)\cdot(1-\P_{T_{\mathbf{n}, t=r}}(c=d))= 1-\sum_{d\in C}\P^2_{T_{\mathbf{n}, t=r}}(c=d).
\end{equation}
Dla tak zdefiniowanych miar ``nieczystoci'' wz贸w, podziau dokonujemy w taki spos贸b, aby zminimalizowa wsp贸czynnik Gini'ego lub entropi. Im ni偶sze miary nieczystoci, tym bardziej obserwacje znajdujce si w w藕le s monokultur\footnote{prawie wszystkie s w jednej klasie}. Nierzadko korzysta si r贸wnie偶 z wsp贸czynnika przyrostu informacji (ang. \emph{information gain})
\begin{equation}
    \Delta E_{T_{\mathbf{n}}}(c|t)=E_{T_{\mathbf{n}}}(c)-E_{T_{\mathbf{n}}}(c|t).
\end{equation}
Istnieje r贸wnie偶 jego odpowiednik dla indeksu Gini'ego. W obu przypadkach optymalnego podziau szukamy poprzez maksymalizacj przyrostu informacji.

\hypertarget{przycinanie-drzewa-decyzyjnego}{%
\section{Przycinanie drzewa decyzyjnego}\label{przycinanie-drzewa-decyzyjnego}}

Uczenie drzewa decyzyjnego wi偶e si z ryzykiem przeuczenia modelu (podobnie jak to si ma w przypadku innych modeli predykcyjnych). Wczeniej przytoczone reguy zatrzymania (np. gboko drzewa czy zatrzymanie przy osigniciu jednorodnoci na zadanym poziomie) pomagaj kontrolowa poziom generalizacji drzewa ale czasami bdzie dodatkowo potrzebne przycicie drzewa, czyli usunicie pewnych podzia贸w, a co za tym idzie, r贸wnie偶 lici (wz贸w).

\hypertarget{przycinanie-redukujux105ce-bux142ux105d}{%
\subsection{Przycinanie redukujce bd}\label{przycinanie-redukujux105ce-bux142ux105d}}

Jedn ze strategii przycinania drzewa jest przycinanie redukujce bd (ang. \emph{reduced error pruning}). Polega ono na por贸wnaniu bd贸w (najczciej u偶ywana jest miara odsetka bdnych klasyfikacji lub MSE) licia \(\mathbf{l}\) i wza do kt贸rego drzewo przycinamy \(\mathbf{n}\) na cakiem nowym zbiorze uczcym \(R\). Niech \(e_R(\mathbf{l})\) i \(e_R(\mathbf{n})\) oznaczaj odpowiednio bdy na zbiorze \(R\) licia i wza. Przez bd wza rozumiemy bd pod-drzewa o korzeniu \(\mathbf{n}\). W贸wczas jeli zachodzi warunek
\begin{equation}
    e_R(\mathbf{l})\leq e_R(\mathbf{n}), 
\end{equation}
to zaleca si zastpi wze \(\mathbf{n}\) liciem \(\mathbf{l}\).

\hypertarget{przycinanie-minimalizujux105ce-bux142ux105d}{%
\subsection{Przycinanie minimalizujce bd}\label{przycinanie-minimalizujux105ce-bux142ux105d}}

Przycinanie minimalizujce bd opiera si na spostrze偶eniu, 偶e bd drzewa przycitego charakteryzuje si zbyt pesymistyczn ocen i dlatego wymaga korekty. Wze drzewa klasyfikacyjnego \(\mathbf{n}\) zastpujemy liciem \(\mathbf{l}\), jeli
\begin{equation}
    \hat{e}_T(\mathbf{l})\leq \hat{e}_T(\mathbf{n}),
\end{equation}
gdzie
\begin{equation}
    \hat{e}_T(\mathbf{n})=\sum_{\mathbf{n}'\in N(\mathbf{n})}\frac{|T_{\mathbf{n}'}|}{|T_\mathbf{n}|}\hat{e}_T(\mathbf{n}'),
\end{equation}
a \(N(\mathbf{n})\) jest zbiorem wszystkich mo偶liwych wz贸w potomnych wza \(\mathbf{n}\) i \begin{equation}
    \hat{e}_T(\mathbf{l})=1-\frac{|\{x\in T_\mathbf{l}|c(x)=d_{\mathbf{l}}\}|+mp}{|T_\mathbf{l}|+m},
\end{equation}
gdzie \(p\) jest prawdopodobiestwem przynale偶noci do klasy \(d_{\mathbf{l}}\) ustalona na podstawie zewntrznej wiedzy (gdy jej nie posiadamy przyjmujemy \(p=1/|C|\)).

W przypadku drzewa regresyjnego znajdujemy wiele analogii, poniewa偶 jeli dla pewnego zbioru rekord贸w \(T\) speniony jest warunek
\begin{equation}\label{kryterium1}
    \operatorname{mse}_T(\mathbf{l})\leq\operatorname{mse}_T(\mathbf{n}),
\end{equation}
gdzie \(\mathbf{l}\) i \(\mathbf{n}\) oznaczaj odpowiednio li i wze, to w贸wczas zastpujemy wze \(\mathbf{n}\) przez li \(\mathbf{l}\).

Estymatory wyznaczone na podstawie niewielkiej pr贸by, mog by obarczone znaczcym bdem. Wyliczanie bdu rednio-kwadratowego dla podzbioru nowych wartoci mo偶e si charakteryzowa takim obci偶eniem. Dlatego stosuje si statystyki opisowe z poprawk, kt贸rej pochodzenie mo偶e mie trzy 藕r贸da: wiedza merytoryczna na temat szukanej wartoci, zao偶e modelu lub na podstawie wylicze opartych o cay zbi贸r wartoci.

Skorygowany estymator bdu rednio-kwadratowego ma nastpujc posta
\begin{equation}\label{mse}
        \widehat{\operatorname{mse}}_T(\mathbf{l})=\frac{\sum_{x\in T}(f(x)-m_{\mathbf{l},m,m_0}(f))^2+mS_0^2}{|T_\mathbf{l}|+m},
\end{equation}
gdzie
\begin{equation}\label{poprawka}
        m_{\mathbf{l},m,m_0}(f)=\frac{\sum_{x\in T_\mathbf{l}}f(x)+mm_0}{|T_\mathbf{l}|+m},
\end{equation}
a \(m_0\) i \(S_0^2\) s redni i wariancj wyznaczonymi na caej pr贸bie uczcej.
Bd rednio-kwadratowy wza \(\mathbf{n}\) ma posta
\begin{equation}\label{propagacja}
        \widehat{\operatorname{mse}}_T(\mathbf{n})=\sum_{\mathbf{n}'\in N(\mathbf{n})}\frac{|T_{\mathbf{n}'}|}{|T_\mathbf{n}|}\widehat{\operatorname{mse}}_T(\mathbf{n}').
\end{equation}
W贸wczas kryterium podcicia mo偶na zapisa w nastpujcy spos贸b
\begin{equation}\label{kryterium2}
        \widehat{\operatorname{mse}}_T(\mathbf{l}) \leq \widehat{\operatorname{mse}}_T(\mathbf{n})
\end{equation}

\hypertarget{przycinanie-ze-wzglux119du-na-wspuxf3ux142czynnik-zux142oux17conoux15bci-drzewa}{%
\subsection{Przycinanie ze wzgldu na wsp贸czynnik zo偶onoci drzewa}\label{przycinanie-ze-wzglux119du-na-wspuxf3ux142czynnik-zux142oux17conoux15bci-drzewa}}

Przycinanie ze wzgldu na wsp贸czynnik zo偶onoci drzewa (ang. \emph{cost-complexity pruning}) polega na wprowadzeniu ``kary'' za zwikszon zo偶ono drzewa. Drzewa klasyfikacyjne przycinamy gdy speniony jest warunek
\begin{equation}
    e_T(\mathbf{l})\leq e_T(\mathbf{n})+\alpha C(\mathbf{n}),
\end{equation}
gdzie \(C(\mathbf{n})\) oznacza zo偶ono drzewa mierzon liczb lici, a \(\alpha\) parametrem wagi kary za zo偶ono drzewa.

Wspomniane kryterium przycicia dla drzew regresyjnych bazuje na wzgldnym bdzie rednio-kwadratowym (ang. \emph{relative square error}), czyli
\begin{equation}\label{rse}
        \widehat{\operatorname{rse}}_T(\mathbf{n})=\frac{|T|\widehat{\operatorname{mse}}_T(\mathbf{n})}{(|T|-1)S^2_T(f)},
\end{equation}
gdzie \(T\) oznacza podzbi贸r \(X\), \(S^2_T\) wariancj na zbiorze \(T\).
W贸wczas kryterium podcicia wyglda nastpujco
\begin{equation}\label{kryterium3}
    \widehat{\operatorname{rse}}_T(\mathbf{l})\leq \widehat{\operatorname{rse}}_T(\mathbf{n})+\alpha C(\mathbf{n}).
\end{equation}

\hypertarget{obsux142uga-brakuxf3w-danych}{%
\section{Obsuga brak贸w danych}\label{obsux142uga-brakuxf3w-danych}}

Drzewa decyzyjne wyjtkowo dobrze radz sobie z obsuga zbior贸w z brakami. Stosowane s g贸wnie dwie strategie:

\begin{itemize}
\tightlist
\item
  udzia贸w obserwacji (ang. \emph{fractional instances}) - rozwa偶ane s wszystkie mo偶liwe podziay dla brakujcej obserwacji i przypisywana jest im odpowiednia waga lub prawdopodobiestwo, w oparciu o zaobserwowany rozkad znanych obserwacji. Te same wagi s stosowane do predykcji wartoci na podstawie drzewa z brakami danych.
\item
  podzia贸w zastpczych (ang. \emph{surrogate splits}) - jeli wynik podziau nie mo偶e by ustalony dla obserwacji z brakami, to u偶ywany jest podzia zastpczy (pierwszy), jeli i ten nie mo偶e zosta ustalony, to stosuje si kolejny. Kolejne podziay zastpcze s generowane tak, aby wynik podziau mo偶liwie najbardziej przypomina podzia waciwy.
\end{itemize}

\hypertarget{zalety-i-wady}{%
\section{Zalety i wady}\label{zalety-i-wady}}

\hypertarget{zalety}{%
\subsection{Zalety}\label{zalety}}

\begin{itemize}
\tightlist
\item
  atwe w interpretacji;
\item
  nie wymagaj 偶mudnego przygotowania danych (brak standaryzacji, wprowadzania zmiennych binarnych, dopuszcza wystpowanie brak贸w danych);
\item
  dziaa na obu typach zmiennych - jakociowych i ilociowych;
\item
  dopuszcza nieliniowo zwizku midzy zmienn wynikow a predyktorami;
\item
  odporny na odstpstwa od zao偶e;
\item
  pozwala na obsug du偶ych zbior贸w danych.
\end{itemize}

\hypertarget{wady}{%
\subsection{Wady}\label{wady}}

\begin{itemize}
\tightlist
\item
  brak jawnej postaci zale偶noci;
\item
  zale偶no struktury drzewa od u偶ytego algorytmu;
\item
  przegrywa jakoci predykcji z innymi metodami nadzorowanego uczenia maszynowego.
\end{itemize}

\BeginKnitrBlock{example}
\protect\hypertarget{exm:przyk41}{}{\label{exm:przyk41} }Przykadem zastosowania drzew decyzyjnych bdzie klasyfikacja irys贸w na podstawie dugoci i szerokoci kielicha i patka.
\EndKnitrBlock{example}

Przykadem zastosowania drzew decyzyjnych bdzie klasyfikacja irys贸w na podstawie dugoci i szerokoci kielicha i patka.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tidyverse) }
\KeywordTok{library}\NormalTok{(rpart) }\CommentTok{# pakiet do tworzenia drzew typu CART}
\KeywordTok{library}\NormalTok{(rpart.plot) }\CommentTok{# pakiet do rysowania drzew}
\end{Highlighting}
\end{Shaded}

Ka偶de zadanie ucznia maszynowego zaczynamy od czyszczenia danych i odpowiedniego ich przygotowania ale w tym przypadku skupimy si jedynie na budowie, optymalizacji i ewaluacji modelu.

\textbf{Podzia zbioru na pr贸b uczc i testow}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{2019}\NormalTok{)}
\NormalTok{dt.train <-}\StringTok{ }\NormalTok{iris }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{sample_frac}\NormalTok{(}\DataTypeTok{size =} \FloatTok{0.7}\NormalTok{)}
\NormalTok{dt.test <-}\StringTok{ }\KeywordTok{setdiff}\NormalTok{(iris, dt.train)}
\KeywordTok{str}\NormalTok{(dt.train)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 'data.frame':    105 obs. of  5 variables:
##  $ Sepal.Length: num  4.8 6.7 6.2 5.4 7.7 5 5.7 5 6.3 6.8 ...
##  $ Sepal.Width : num  3.4 3.3 2.2 3.9 2.6 2 3.8 3 3.4 3 ...
##  $ Petal.Length: num  1.9 5.7 4.5 1.3 6.9 3.5 1.7 1.6 5.6 5.5 ...
##  $ Petal.Width : num  0.2 2.1 1.5 0.4 2.3 1 0.3 0.2 2.4 2.1 ...
##  $ Species     : Factor w/ 3 levels "setosa","versicolor",..: 1 3 2 1 3 2 1 1 3 3 ...
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{str}\NormalTok{(dt.test)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 'data.frame':    45 obs. of  5 variables:
##  $ Sepal.Length: num  4.4 5.4 4.8 4.3 5.7 5.1 5.2 5.2 5.2 4.9 ...
##  $ Sepal.Width : num  2.9 3.7 3 3 4.4 3.8 3.5 3.4 4.1 3.1 ...
##  $ Petal.Length: num  1.4 1.5 1.4 1.1 1.5 1.5 1.5 1.4 1.5 1.5 ...
##  $ Petal.Width : num  0.2 0.2 0.1 0.1 0.4 0.3 0.2 0.2 0.1 0.2 ...
##  $ Species     : Factor w/ 3 levels "setosa","versicolor",..: 1 1 1 1 1 1 1 1 1 1 ...
\end{verbatim}

\textbf{Budowa drzewa}

Budowy drzewa dokonujemy za pomoc funkcji \texttt{rpart} pakietu \textbf{rpart} (Therneau and Atkinson \protect\hyperlink{ref-R-rpart}{2018}) stosujc zapis formuy zale偶noci. Drzewo zostanie zbudowane z uwzgldnieniem kilku kryteri贸w zatrzymania:

\begin{itemize}
\tightlist
\item
  minimalna liczebno wza, kt贸ry mo偶e zosta podzielony to 10 - ze wzgldu na ma liczebno zbioru uczcego;
\item
  minimalna liczebno licia to 5 - aby nie dopuci do przeuczenia modelu;
\item
  maksymalna gboko drzewa to 4 - aby nie dopuci do przeuczenia modelu.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mod.rpart <-}\StringTok{ }\KeywordTok{rpart}\NormalTok{(Species}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data =}\NormalTok{ dt.train, }
                   \DataTypeTok{control =} \KeywordTok{rpart.control}\NormalTok{(}\DataTypeTok{minsplit =} \DecValTok{10}\NormalTok{,}
                                           \DataTypeTok{minbucket =} \DecValTok{5}\NormalTok{,}
                                           \DataTypeTok{maxdepth =} \DecValTok{4}\NormalTok{))}
\KeywordTok{summary}\NormalTok{(mod.rpart)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Call:
## rpart(formula = Species ~ ., data = dt.train, control = rpart.control(minsplit = 10, 
##     minbucket = 5, maxdepth = 4))
##   n= 105 
## 
##           CP nsplit  rel error    xerror       xstd
## 1 0.51470588      0 1.00000000 1.1176471 0.06737554
## 2 0.39705882      1 0.48529412 0.4852941 0.06995514
## 3 0.02941176      2 0.08823529 0.1617647 0.04614841
## 4 0.01000000      3 0.05882353 0.1617647 0.04614841
## 
## Variable importance
##  Petal.Width Petal.Length Sepal.Length  Sepal.Width 
##           33           32           20           15 
## 
## Node number 1: 105 observations,    complexity param=0.5147059
##   predicted class=virginica   expected loss=0.647619  P(node) =1
##     class counts:    35    33    37
##    probabilities: 0.333 0.314 0.352 
##   left son=2 (35 obs) right son=3 (70 obs)
##   Primary splits:
##       Petal.Length < 2.6  to the left,  improve=35.03810, (0 missing)
##       Petal.Width  < 0.8  to the left,  improve=35.03810, (0 missing)
##       Sepal.Length < 5.45 to the left,  improve=25.60255, (0 missing)
##       Sepal.Width  < 3.35 to the right, improve=14.70881, (0 missing)
##   Surrogate splits:
##       Petal.Width  < 0.8  to the left,  agree=1.000, adj=1.000, (0 split)
##       Sepal.Length < 5.45 to the left,  agree=0.933, adj=0.800, (0 split)
##       Sepal.Width  < 3.35 to the right, agree=0.848, adj=0.543, (0 split)
## 
## Node number 2: 35 observations
##   predicted class=setosa      expected loss=0  P(node) =0.3333333
##     class counts:    35     0     0
##    probabilities: 1.000 0.000 0.000 
## 
## Node number 3: 70 observations,    complexity param=0.3970588
##   predicted class=virginica   expected loss=0.4714286  P(node) =0.6666667
##     class counts:     0    33    37
##    probabilities: 0.000 0.471 0.529 
##   left son=6 (37 obs) right son=7 (33 obs)
##   Primary splits:
##       Petal.Width  < 1.75 to the left,  improve=24.297670, (0 missing)
##       Petal.Length < 4.75 to the left,  improve=24.174190, (0 missing)
##       Sepal.Length < 5.75 to the left,  improve= 4.483555, (0 missing)
##       Sepal.Width  < 2.55 to the left,  improve= 3.793760, (0 missing)
##   Surrogate splits:
##       Petal.Length < 4.75 to the left,  agree=0.886, adj=0.758, (0 split)
##       Sepal.Length < 6.15 to the left,  agree=0.671, adj=0.303, (0 split)
##       Sepal.Width  < 2.65 to the left,  agree=0.671, adj=0.303, (0 split)
## 
## Node number 6: 37 observations,    complexity param=0.02941176
##   predicted class=versicolor  expected loss=0.1351351  P(node) =0.352381
##     class counts:     0    32     5
##    probabilities: 0.000 0.865 0.135 
##   left son=12 (31 obs) right son=13 (6 obs)
##   Primary splits:
##       Petal.Length < 4.95 to the left,  improve=4.0464980, (0 missing)
##       Petal.Width  < 1.35 to the left,  improve=1.0296010, (0 missing)
##       Sepal.Width  < 3.05 to the right, improve=0.2615519, (0 missing)
##       Sepal.Length < 5.95 to the left,  improve=0.1828101, (0 missing)
## 
## Node number 7: 33 observations
##   predicted class=virginica   expected loss=0.03030303  P(node) =0.3142857
##     class counts:     0     1    32
##    probabilities: 0.000 0.030 0.970 
## 
## Node number 12: 31 observations
##   predicted class=versicolor  expected loss=0.03225806  P(node) =0.2952381
##     class counts:     0    30     1
##    probabilities: 0.000 0.968 0.032 
## 
## Node number 13: 6 observations
##   predicted class=virginica   expected loss=0.3333333  P(node) =0.05714286
##     class counts:     0     2     4
##    probabilities: 0.000 0.333 0.667
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{rpart.plot}\NormalTok{(mod.rpart)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{EksploracjaDanych_files/figure-latex/unnamed-chunk-15-1.pdf}
\caption{\label{fig:unnamed-chunk-15}Obraz drzewa klasyfikacyjnego.}
\end{figure}

Powy偶szy wykres przedstawia struktur drzewa klasyfikacyjnego. Kolorami s oznaczone klasy, kt贸re w danym w藕le dominuj. Nasycenie barwy decyduje o sile tej dominacji. W ka偶dym w藕le podana jest klasa, do kt贸rej najprawdopodobniej nale偶 jego obserwacje. Ponadto podane s proporcje przynale偶noci do klas zmiennej wynikowej oraz procent obserwacji zbioru uczcego nale偶cych do danego wza. Pod ka偶dym wzem podana jest regua podziau.

\textbf{Przycinanie drzewa}

Zanim przystpimy do przycinania drzewa nale偶y sprawdzi, jakie s zdolnoci generalizacyjne modelu. Oceny tej dokonujemy najczciej sprawdzajc macierz klasyfikacji.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pred.prob <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(mod.rpart, }
                     \DataTypeTok{newdata =}\NormalTok{ dt.test)}
\NormalTok{pred.prob[}\DecValTok{10}\OperatorTok{:}\DecValTok{20}\NormalTok{,]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    setosa versicolor  virginica
## 10      1  0.0000000 0.00000000
## 11      1  0.0000000 0.00000000
## 12      1  0.0000000 0.00000000
## 13      1  0.0000000 0.00000000
## 14      1  0.0000000 0.00000000
## 15      1  0.0000000 0.00000000
## 16      0  0.9677419 0.03225806
## 17      0  0.9677419 0.03225806
## 18      0  0.9677419 0.03225806
## 19      0  0.9677419 0.03225806
## 20      0  0.9677419 0.03225806
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pred.class <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(mod.rpart, }
                      \DataTypeTok{newdata =}\NormalTok{ dt.test,}
                      \DataTypeTok{type =} \StringTok{"class"}\NormalTok{)}
\NormalTok{pred.class}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##          1          2          3          4          5          6          7 
##     setosa     setosa     setosa     setosa     setosa     setosa     setosa 
##          8          9         10         11         12         13         14 
##     setosa     setosa     setosa     setosa     setosa     setosa     setosa 
##         15         16         17         18         19         20         21 
##     setosa versicolor versicolor versicolor versicolor versicolor versicolor 
##         22         23         24         25         26         27         28 
## versicolor versicolor versicolor versicolor versicolor versicolor versicolor 
##         29         30         31         32         33         34         35 
## versicolor versicolor versicolor versicolor  virginica  virginica  virginica 
##         36         37         38         39         40         41         42 
##  virginica  virginica  virginica  virginica  virginica  virginica  virginica 
##         43         44         45 
##  virginica  virginica  virginica 
## Levels: setosa versicolor virginica
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tab <-}\StringTok{ }\KeywordTok{table}\NormalTok{(}\DataTypeTok{predykcja =}\NormalTok{ pred.class, }\DataTypeTok{obserwacja =}\NormalTok{ dt.test}\OperatorTok{$}\NormalTok{Species)}
\NormalTok{tab}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##             obserwacja
## predykcja    setosa versicolor virginica
##   setosa         15          0         0
##   versicolor      0         17         0
##   virginica       0          0        13
\end{verbatim}

Jak wida z powy偶szej tabeli, model cakiem dobrze radzi sobie z poprawn klasyfikacj obserwacji do odpowiednich kategorii.

W dalszej kolejnoci sprawdzimy, czy nie jest konieczne przycicie drzewa. Jednym z kryteri贸w przycinania drzewa jest przycinanie ze wzgldu na zo偶ono drzewa. W tym przypadku jest wyra偶ony parametrem \texttt{cp}. Istnieje powszechnie stosowana regua jednego odchylenia standardowego, kt贸ra m贸wi, 偶e drzewo nale偶y przyci w贸wczas, gdy bd oszacowany na podstawie sprawdzianu krzy偶owego (\texttt{xerror}), pierwszy raz zejdzie poni偶ej poziomu wyznaczonego przez najni偶sz warto bdu powikszonego o odchylenie standardowe tego bdu (\texttt{xstd}). Na podstawie poni偶szej tabeli mo偶na ustali, 偶e poziomem odcicia jest warto \(0.16176+0.046148=0.207908\). Pierwszy raz bd przyjmuje warto mniejsz od \(0.16176\) po drugim podziale (\texttt{nsplit=2}). Temu poziomowi odpowiada \texttt{cp} o wartoci \(0.029412\) i to jest zo偶ono drzewa, kt贸r powinnimy przyj do przycicia drzewa.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{printcp}\NormalTok{(mod.rpart)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Classification tree:
## rpart(formula = Species ~ ., data = dt.train, control = rpart.control(minsplit = 10, 
##     minbucket = 5, maxdepth = 4))
## 
## Variables actually used in tree construction:
## [1] Petal.Length Petal.Width 
## 
## Root node error: 68/105 = 0.64762
## 
## n= 105 
## 
##         CP nsplit rel error  xerror     xstd
## 1 0.514706      0  1.000000 1.11765 0.067376
## 2 0.397059      1  0.485294 0.48529 0.069955
## 3 0.029412      2  0.088235 0.16176 0.046148
## 4 0.010000      3  0.058824 0.16176 0.046148
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plotcp}\NormalTok{(mod.rpart)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{EksploracjaDanych_files/figure-latex/unnamed-chunk-17-1.pdf}
\caption{\label{fig:unnamed-chunk-17}Na wykresie bd贸w punkt odcicia zaznaczony jest lini przerywan}
\end{figure}

Przycite drzewo wyglda nastpujco:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mod.rpart2 <-}\StringTok{ }\KeywordTok{prune}\NormalTok{(mod.rpart, }\DataTypeTok{cp =} \FloatTok{0.029412}\NormalTok{)}
\KeywordTok{summary}\NormalTok{(mod.rpart2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Call:
## rpart(formula = Species ~ ., data = dt.train, control = rpart.control(minsplit = 10, 
##     minbucket = 5, maxdepth = 4))
##   n= 105 
## 
##          CP nsplit  rel error    xerror       xstd
## 1 0.5147059      0 1.00000000 1.1176471 0.06737554
## 2 0.3970588      1 0.48529412 0.4852941 0.06995514
## 3 0.0294120      2 0.08823529 0.1617647 0.04614841
## 
## Variable importance
##  Petal.Width Petal.Length Sepal.Length  Sepal.Width 
##           34           31           20           15 
## 
## Node number 1: 105 observations,    complexity param=0.5147059
##   predicted class=virginica   expected loss=0.647619  P(node) =1
##     class counts:    35    33    37
##    probabilities: 0.333 0.314 0.352 
##   left son=2 (35 obs) right son=3 (70 obs)
##   Primary splits:
##       Petal.Length < 2.6  to the left,  improve=35.03810, (0 missing)
##       Petal.Width  < 0.8  to the left,  improve=35.03810, (0 missing)
##       Sepal.Length < 5.45 to the left,  improve=25.60255, (0 missing)
##       Sepal.Width  < 3.35 to the right, improve=14.70881, (0 missing)
##   Surrogate splits:
##       Petal.Width  < 0.8  to the left,  agree=1.000, adj=1.000, (0 split)
##       Sepal.Length < 5.45 to the left,  agree=0.933, adj=0.800, (0 split)
##       Sepal.Width  < 3.35 to the right, agree=0.848, adj=0.543, (0 split)
## 
## Node number 2: 35 observations
##   predicted class=setosa      expected loss=0  P(node) =0.3333333
##     class counts:    35     0     0
##    probabilities: 1.000 0.000 0.000 
## 
## Node number 3: 70 observations,    complexity param=0.3970588
##   predicted class=virginica   expected loss=0.4714286  P(node) =0.6666667
##     class counts:     0    33    37
##    probabilities: 0.000 0.471 0.529 
##   left son=6 (37 obs) right son=7 (33 obs)
##   Primary splits:
##       Petal.Width  < 1.75 to the left,  improve=24.297670, (0 missing)
##       Petal.Length < 4.75 to the left,  improve=24.174190, (0 missing)
##       Sepal.Length < 5.75 to the left,  improve= 4.483555, (0 missing)
##       Sepal.Width  < 2.55 to the left,  improve= 3.793760, (0 missing)
##   Surrogate splits:
##       Petal.Length < 4.75 to the left,  agree=0.886, adj=0.758, (0 split)
##       Sepal.Length < 6.15 to the left,  agree=0.671, adj=0.303, (0 split)
##       Sepal.Width  < 2.65 to the left,  agree=0.671, adj=0.303, (0 split)
## 
## Node number 6: 37 observations
##   predicted class=versicolor  expected loss=0.1351351  P(node) =0.352381
##     class counts:     0    32     5
##    probabilities: 0.000 0.865 0.135 
## 
## Node number 7: 33 observations
##   predicted class=virginica   expected loss=0.03030303  P(node) =0.3142857
##     class counts:     0     1    32
##    probabilities: 0.000 0.030 0.970
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{rpart.plot}\NormalTok{(mod.rpart2)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{EksploracjaDanych_files/figure-latex/unnamed-chunk-18-1.pdf}
\caption{\label{fig:unnamed-chunk-18}Drzewo klasyfikacyjne po przyciciu}
\end{figure}

\textbf{Ocena dopasowania modelu}

Na koniec budowy modelu nale偶y sprawdzi jego jako na zbiorze testowym.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pred.class2 <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(mod.rpart2,}
                       \DataTypeTok{newdata =}\NormalTok{ dt.test,}
                       \DataTypeTok{type =} \StringTok{"class"}\NormalTok{)}
\NormalTok{tab2 <-}\StringTok{ }\KeywordTok{table}\NormalTok{(}\DataTypeTok{predykcja =}\NormalTok{ pred.class2, }\DataTypeTok{obserwacja =}\NormalTok{ dt.test}\OperatorTok{$}\NormalTok{Species)}
\NormalTok{tab2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##             obserwacja
## predykcja    setosa versicolor virginica
##   setosa         15          0         0
##   versicolor      0         17         0
##   virginica       0          0        13
\end{verbatim}

Mimo przycicia drzewa, klasyfikacja pozostaje na niezmienionym poziomie. Odsetek poprawnych klasyfikacji mo偶emy oszacowa za pomoc

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{round}\NormalTok{(}\KeywordTok{sum}\NormalTok{(}\KeywordTok{diag}\NormalTok{(tab2))}\OperatorTok{/}\KeywordTok{sum}\NormalTok{(tab2)}\OperatorTok{*}\DecValTok{100}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 100
\end{verbatim}

\hypertarget{inne-algorytmy-budowy-drzew-decyzyjnych-implementowane-w-r}{%
\section{\texorpdfstring{Inne algorytmy budowy drzew decyzyjnych implementowane w \textbf{R}}{Inne algorytmy budowy drzew decyzyjnych implementowane w R}}\label{inne-algorytmy-budowy-drzew-decyzyjnych-implementowane-w-r}}

Opr贸cz najbardziej znanego algorytmu CART implementowanego w postaci funkcji pakietu \textbf{rpart}, istniej r贸wnie偶 inne algorytmy, kt贸re znalazy swoje implementacje w R. S to:

\begin{itemize}
\tightlist
\item
  \emph{CHAID}\footnote{Chi-square automatic interaction detection} - algorytm przeznaczony do budowy drzew klasyfikacyjnych, gdzie zar贸wno zmienna wynikowa, jak i zmienne niezale偶ne musz by ze skali jakociowej. G贸wn r贸偶nic w stosunku do drzew typu CART jest spos贸b budowy podzia贸w, oparty na tecie niezale偶noci \(\chi^2\) Pearsona. Wyboru reguy podziau dokonuje si poprzez testowanie niezale偶noci zmiennej niezale偶nej z predyktorami. Regua o najwikszej wartoci statystyki \(\chi^2\) jest stosowana w pierwszej kolejnoci. Implementacja tego algorytmu znajduje si w pakiecie \textbf{CHAID}\footnote{brak w oficjalnej dystrybucji CRAN} (funkcja do tworzenia drzewa o tej samej nazwie \texttt{chaid}) (Team \protect\hyperlink{ref-R-CHAID}{2015}).
\item
  \emph{Ctree}\footnote{Conditional Inference Trees} - algorytm zbli偶ony zasad dziaania do CHAID, poniewa偶 r贸wnie偶 wykorzystuje testowanie do wyboru reguy podziau. R贸偶ni si jednak tym, 偶e mo偶e by stosowany do zmiennych dowolnego typu oraz tym, 偶e mo偶e by zar贸wno drzewem klasyfikacyjnym jak i regresyjnym. Implementacj R-ow mo偶na znale藕 w pakietach \textbf{party} (Hothorn, Hornik, and Zeileis \protect\hyperlink{ref-R-party}{2006}) lub \textbf{partykit} (Hothorn and Zeileis \protect\hyperlink{ref-R-partykit}{2015}) - funkcj do tworzenia modelu jest \texttt{ctree}.
\item
  \emph{C4.5} - algorytm stworzony przez Quinlan (\protect\hyperlink{ref-quinlan1993}{1993}) w oparciu, o r贸wnie偶 jego autorstwa, algorytm ID3. Su偶y jedynie do zada klasyfikacyjnych. W du偶ym uproszczeniu, dob贸r regu podziau odbywa si na podstawie przyrostu informacji (patrz \protect\hyperlink{reguux142y-podziaux142u}{Reguy podziau}). W przeciwiestwie do pierwotnego algorytmu ID3, C4.5 nie raczej nie przeucza drzew. Implementacja R-owa znajduje si w pakiecie \textbf{RWeka} (Hornik, Buchta, and Zeileis \protect\hyperlink{ref-R-Rweka}{2009}) - funkcja do budowy drzewa to \texttt{J48}.
\item
  \emph{C5.0} - kolejny algorytm autorstwa Kuhn and Quinlan (\protect\hyperlink{ref-R-C50}{2018}) jest usprawnieniem algorytmu C4.5, generujcym mniejsze drzewa automatycznie przycinane na podstawie zo偶onoci drzewa. Su偶y jedynie do zada klasyfikacyjnych. Jest szybszy od poprzednika i pozwala na zastosowanie metody \emph{boosting}\footnote{budowa klasyfikatora w oparciu o proces iteracyjny, w kt贸rym kolejne w kolejnych iteracjach budowane s proste drzewa i przypisywane s im wagi - im gorszy klasyfikator, tym wiksza waga - po to aby nauczy drzewo klasyfikowa ``trudne'' przypadki}. Implementacja R-owa znajduje si w pakiecie \emph{C50}, a funkcja do budowy drzewa to \texttt{C5.0}.
\end{itemize}

\BeginKnitrBlock{example}
\protect\hypertarget{exm:przyk42}{}{\label{exm:przyk42} }W celu por贸wnania wynik贸w klasyfikacji na podstawie drzew decyzyjnych o r贸偶nych algorytmach, zostan nauczone modele w oparciu o funkcje \texttt{ctree}, \texttt{J48} i \texttt{C5.0} dla tego samego zestawu danych co w przykadzie wczeniejszym \ref{exm:przyk41}.
\EndKnitrBlock{example}

\begin{itemize}
\tightlist
\item
  \textbf{Drzewo \texttt{ctree}}
\end{itemize}

Na pocztek ustalamy parametry ograniczajce rozrost drzewa podobne jak w poprzednim przykadzie.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(partykit)}
\NormalTok{tree2 <-}\StringTok{ }\KeywordTok{ctree}\NormalTok{(Species}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data =}\NormalTok{ dt.train,}
               \DataTypeTok{control =} \KeywordTok{ctree_control}\NormalTok{(}\DataTypeTok{minsplit =} \DecValTok{10}\NormalTok{,}
                                       \DataTypeTok{minbucket =} \DecValTok{5}\NormalTok{,}
                                       \DataTypeTok{maxdepth =} \DecValTok{4}\NormalTok{))}
\NormalTok{tree2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Model formula:
## Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width
## 
## Fitted party:
## [1] root
## |   [2] Petal.Length <= 1.9: setosa (n = 35, err = 0.0%)
## |   [3] Petal.Length > 1.9
## |   |   [4] Petal.Width <= 1.7
## |   |   |   [5] Petal.Length <= 4.9: versicolor (n = 31, err = 3.2%)
## |   |   |   [6] Petal.Length > 4.9: virginica (n = 6, err = 33.3%)
## |   |   [7] Petal.Width > 1.7: virginica (n = 33, err = 3.0%)
## 
## Number of inner nodes:    3
## Number of terminal nodes: 4
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(tree2)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{EksploracjaDanych_files/figure-latex/ctree-1.pdf}
\caption{\label{fig:ctree}Wykres drzewa decyzyjnego zbudowanego metod ctree}
\end{figure}

Wydaje si, 偶e drzewo nie jest optymalne, poniewa偶 w w藕le 6 obserwacje z grup \texttt{versicolor} i \texttt{virginica} s nieco pomieszane. Ostateczne oceny dokonujemy na podstawie pr贸by testowej.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pred2 <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(tree2, }\DataTypeTok{newdata =}\NormalTok{ dt.test)}
\NormalTok{tab <-}\StringTok{ }\KeywordTok{table}\NormalTok{(}\DataTypeTok{predykcja =}\NormalTok{ pred2, }\DataTypeTok{obserwacja =}\NormalTok{ dt.test}\OperatorTok{$}\NormalTok{Species)}
\NormalTok{tab}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##             obserwacja
## predykcja    setosa versicolor virginica
##   setosa         15          0         0
##   versicolor      0         17         0
##   virginica       0          0        13
\end{verbatim}

Dopiero ocena jakoci klasyfikacji na podstawie pr贸by testowej pokazuje, 偶e model zbudowany za pomoc \texttt{ctree} daje podobn precyzj jak \texttt{rpart} przycity.

\begin{itemize}
\tightlist
\item
  \textbf{Drzewo \texttt{J48}}
\end{itemize}

W tym przypadku model sam poszukuje optymalnego rozwizania przycinajc si automatycznie.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(RWeka)}
\NormalTok{tree3 <-}\StringTok{ }\KeywordTok{J48}\NormalTok{(Species}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data =}\NormalTok{ dt.train)}
\NormalTok{tree3}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## J48 pruned tree
## ------------------
## 
## Petal.Width <= 0.6: setosa (35.0)
## Petal.Width > 0.6
## |   Petal.Width <= 1.7
## |   |   Petal.Length <= 4.9: versicolor (31.0/1.0)
## |   |   Petal.Length > 4.9
## |   |   |   Petal.Width <= 1.5: virginica (3.0)
## |   |   |   Petal.Width > 1.5: versicolor (3.0/1.0)
## |   Petal.Width > 1.7: virginica (33.0/1.0)
## 
## Number of Leaves  :  5
## 
## Size of the tree :   9
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(tree3)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{EksploracjaDanych_files/figure-latex/J48-1.pdf}
\caption{\label{fig:J48}Wykres drzewa decyzyjnego zbudowanego metod J48}
\end{figure}

Drzewo jest nieco bardziej rozbudowane ni偶 \texttt{tree2} i \texttt{mod.rpart2}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(tree3)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## === Summary ===
## 
## Correctly Classified Instances         102               97.1429 %
## Incorrectly Classified Instances         3                2.8571 %
## Kappa statistic                          0.9571
## Mean absolute error                      0.0331
## Root mean squared error                  0.1286
## Relative absolute error                  7.4482 %
## Root relative squared error             27.2918 %
## Total Number of Instances              105     
## 
## === Confusion Matrix ===
## 
##   a  b  c   <-- classified as
##  35  0  0 |  a = setosa
##   0 32  1 |  b = versicolor
##   0  2 35 |  c = virginica
\end{verbatim}

Podsumowanie dopasowania drzewa na pr贸bie uczcej jest bardzo dobre, bo poprawnych klasyfikacji jest ponad 97\%. Oceny dopasowania i tak dokonujemy na zbiorze testowym.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pred3 <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(tree3, }\DataTypeTok{newdata =}\NormalTok{ dt.test)}
\NormalTok{tab <-}\StringTok{ }\KeywordTok{table}\NormalTok{(}\DataTypeTok{predykcja =}\NormalTok{ pred3, }\DataTypeTok{obserwacja =}\NormalTok{ dt.test}\OperatorTok{$}\NormalTok{Species)}
\NormalTok{tab}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##             obserwacja
## predykcja    setosa versicolor virginica
##   setosa         15          0         0
##   versicolor      0         17         0
##   virginica       0          0        13
\end{verbatim}

Otrzymujemy identyczn macierz klasyfikacji jak w poprzednich przypadkach.

\begin{itemize}
\tightlist
\item
  \textbf{Drzewo \texttt{C50}}
\end{itemize}

Tym razem r贸wnie偶 nie trzeba ustawia parametr贸w drzewa, poniewa偶 algorytm dziaa tak aby zapobiec rozrostowi drzewa przy jednoczesnej wysokiej poprawnoci klasyfikacji.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(C50)}
\NormalTok{tree4 <-}\StringTok{ }\KeywordTok{C5.0}\NormalTok{(Species}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data =}\NormalTok{ dt.train)}
\KeywordTok{summary}\NormalTok{(tree4)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## C5.0.formula(formula = Species ~ ., data = dt.train)
## 
## 
## C5.0 [Release 2.07 GPL Edition]      Wed Mar 11 21:12:14 2020
## -------------------------------
## 
## Class specified by attribute `outcome'
## 
## Read 105 cases (5 attributes) from undefined.data
## 
## Decision tree:
## 
## Petal.Length <= 1.9: setosa (35)
## Petal.Length > 1.9:
## :...Petal.Width > 1.7: virginica (33/1)
##     Petal.Width <= 1.7:
##     :...Petal.Length <= 4.9: versicolor (31/1)
##         Petal.Length > 4.9: virginica (6/2)
## 
## 
## Evaluation on training data (105 cases):
## 
##      Decision Tree   
##    ----------------  
##    Size      Errors  
## 
##       4    4( 3.8%)   <<
## 
## 
##     (a)   (b)   (c)    <-classified as
##    ----  ----  ----
##      35                (a): class setosa
##            30     3    (b): class versicolor
##             1    36    (c): class virginica
## 
## 
##  Attribute usage:
## 
##  100.00% Petal.Length
##   66.67% Petal.Width
## 
## 
## Time: 0.0 secs
\end{verbatim}

Otrzymujemy identyczne drzewo jak w przypadku zastosowania algorytmu \texttt{ctree}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(tree4)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{EksploracjaDanych_files/figure-latex/C50-1.pdf}
\caption{\label{fig:C50}Wykres drzewa decyzyjnego zbudowanego metod C5.0}
\end{figure}

Dla pewnoci przeprowadzimy sprawdzenie na zbiorze testowym.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pred4 <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(tree4, }\DataTypeTok{newdata =}\NormalTok{ dt.test)}
\NormalTok{tab <-}\StringTok{ }\KeywordTok{table}\NormalTok{(}\DataTypeTok{predykcja =}\NormalTok{ pred4, }\DataTypeTok{obserwacja =}\NormalTok{ dt.test}\OperatorTok{$}\NormalTok{Species)}
\NormalTok{tab}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##             obserwacja
## predykcja    setosa versicolor virginica
##   setosa         15          0         0
##   versicolor      0         17         0
##   virginica       0          0        13
\end{verbatim}

\hypertarget{pochodne-drzew-decyzyjnych}{%
\chapter{Pochodne drzew decyzyjnych}\label{pochodne-drzew-decyzyjnych}}

Przykad zastosowania drzew decyzyjnych na zbiorze \texttt{iris} w poprzednich \protect\hyperlink{przyk41}{przykadach} mo偶e skania do przypuszczenia, 偶e drzewa decyzyjne zawsze dobrze radz sobie z predykcj wartoci wynikowej. Niestety w przykadach nieco bardziej skomplikowanych, gdzie chocia偶by klasy zmiennej wynikowej nie s tak wyra藕nie separowalne, drzewa decyzyjne wypadaj gorzej w por贸wnaniu z innymi modelami nadzorowanego uczenia maszynowego.

I tak u podstaw metod bazujcych na prostych drzewach decyzyjnych sta pomys, 偶e skoro jedno drzewo nie ma wystarczajcych wasnoci predykcyjnych, to mo偶e zastosowanie wielu drzew poczonych w pewien spos贸b poprawi je. Tak powstay metody \emph{bagging}, \emph{random forest} i \emph{boosting}\footnote{chyba tylko dla drugiej metody istniej dobre polskie tumaczenie nazwy - las losowy}. Nale偶y zaznaczy, 偶e metody znajduj swoje zastosowanie r贸wnie偶 w innych modelach nadzorowanego uczenia maszynowego.

\hypertarget{bagging}{%
\section{Bagging}\label{bagging}}

Technika ta zostaa wprowadzona przez Breiman (\protect\hyperlink{ref-breiman1996}{1996}) i ma na celu zmniejszenie wariancji modelu pojedynczego drzewa. Podobnie jak technika \emph{bootstrap}, w kt贸rej statystyki s wyliczane na wielu pr贸bach pobranych z tego samego rozkadu (pr贸by), w metodzie bagging losuje si wiele pr贸b ze zbioru uczcego (najczciej poprzez wielokrotne losowanie pr贸by o rozmiarze zbioru uczcego ze zwracaniem), a nastpnie dla ka偶dej pr贸by bootstrapowej buduje si drzewo. W ten spos贸b otrzymujemy \(B\) drzew decyzyjnych \(\hat{f}^1(x), \hat{f}^2(x),\ldots, \hat{f}^B(x)\). Na koniec poprzez urednienie otrzymujemy model charakteryzujcy si wiksz precyzj
\begin{equation}
    \hat{f}_{bag}(x)=\frac1B\sum_{b=1}^B\hat{f}^b(x).
\end{equation}

Poniewa偶 podczas budowy drzew na podstawie pr贸b bootstrapowych nie kontrolujemy zo偶onoci, to w rezultacie ka偶de z drzew mo偶e charakteryzowa si du偶 wariancj. Poprzez urednianie wynik贸w pojedynczych drzew otrzymujemy mniejsze obci偶enie ale r贸wnie偶 przy dostatecznie du偶ej liczbie pr贸b (\(B\) czsto liczy si w setkach, czy tysicach) zmniejszamy wariancj ``redniej'' predykcji z drzew. Oczywicie metod t trzeba dostosowa do zada klasyfikacyjnych, poniewa偶 nie istnieje rednia klasyfikacji z wielu drzew. W miejsce redniej stosuje si mod, czyli warto dominujc.

Przyjrzyjmy si jak maszyna losuje obserwacje ze zwracaniem

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n <-}\StringTok{ }\OtherTok{NULL}
\NormalTok{m <-}\StringTok{ }\OtherTok{NULL}
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\DecValTok{1000}\NormalTok{)\{}
\NormalTok{    x <-}\StringTok{ }\KeywordTok{sample}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{500}\NormalTok{, }\DataTypeTok{size =} \DecValTok{500}\NormalTok{, }\DataTypeTok{replace =}\NormalTok{ T)}
\NormalTok{    y <-}\StringTok{ }\KeywordTok{setdiff}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{500}\NormalTok{, x)}
\NormalTok{    z <-}\StringTok{ }\KeywordTok{unique}\NormalTok{(x)}
\NormalTok{    n[i] <-}\StringTok{ }\KeywordTok{length}\NormalTok{(z)}
\NormalTok{    m[i] <-}\StringTok{ }\KeywordTok{length}\NormalTok{(y)}
\NormalTok{\}}
\KeywordTok{mean}\NormalTok{(n)}\OperatorTok{/}\DecValTok{500}\OperatorTok{*}\DecValTok{100}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 63.2802
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{mean}\NormalTok{(m)}\OperatorTok{/}\DecValTok{500}\OperatorTok{*}\DecValTok{100}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 36.7198
\end{verbatim}

Faktycznie uczenie modelu metod bagging odbywa si rednio na 2/3 obserwacji zbioru uczcego wylosowanych do pr贸b bootstrapowych, a pozostaa 1/3 (ang. \emph{out-of-bag}) jest wykorzystana do oceny jakoci predykcji.

Niewtpliw zalet drzew decyzyjnych bya ich atwa interpretacja. W przypadku metody bagging jest ona znacznie utrudniona, poniewa偶 jej wynik skada si z agregacji wielu drzew. Mo偶na natomiast oceni wa偶no predyktor贸w (ang. \emph{variable importance}). I tak, przez obserwacj spadku \(RSS\) dla baggingu regresyjnego przy zastosowaniu danego predyktora w podziaach drzewa i urednieniu wyniku otrzymamy wska藕nik wa偶noci predyktora du偶o lepszy ni偶 dla pojedynczego drzewa. W przypadku baggingu klasyfikacyjnego w miejsce \(RSS\) stosujemy indeks Gini'ego.

Implementacja R-owa metody bagging znajduje si w pakiecie \textbf{ipred}, a funkcja do budowy modelu nazywa si \texttt{bagging} (Peters and Hothorn \protect\hyperlink{ref-R-ipred}{2018}). Mo偶na r贸wnie偶 stosowa funkcj \texttt{randomForest} pakietu \textbf{randomForest} (Liaw and Wiener \protect\hyperlink{ref-R-las}{2002}) - powody takiego dziaania wyjani si w podrozdziale \protect\hyperlink{lasy-losowe}{Lasy losowe}.

\BeginKnitrBlock{example}
\protect\hypertarget{exm:przyk51}{}{\label{exm:przyk51} }Tym razem cel zadania jest regresyjny i polega na ustaleniu miary tendencji centralnej ceny mieszka w Bostonie na podstawie zmiennych umieszczonych w zbiorze \texttt{Boston} pakietu \textbf{MASS} (Venables and Ripley \protect\hyperlink{ref-R-MASS}{2002}). Zmienn zale偶n bdzie mediana cen mieszka na przedmieciach Bostonu (\texttt{medv}).
\EndKnitrBlock{example}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(MASS)}
\KeywordTok{head}\NormalTok{(Boston)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      crim zn indus chas   nox    rm  age    dis rad tax ptratio  black lstat
## 1 0.00632 18  2.31    0 0.538 6.575 65.2 4.0900   1 296    15.3 396.90  4.98
## 2 0.02731  0  7.07    0 0.469 6.421 78.9 4.9671   2 242    17.8 396.90  9.14
## 3 0.02729  0  7.07    0 0.469 7.185 61.1 4.9671   2 242    17.8 392.83  4.03
## 4 0.03237  0  2.18    0 0.458 6.998 45.8 6.0622   3 222    18.7 394.63  2.94
## 5 0.06905  0  2.18    0 0.458 7.147 54.2 6.0622   3 222    18.7 396.90  5.33
## 6 0.02985  0  2.18    0 0.458 6.430 58.7 6.0622   3 222    18.7 394.12  5.21
##   medv
## 1 24.0
## 2 21.6
## 3 34.7
## 4 33.4
## 5 36.2
## 6 28.7
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{2019}\NormalTok{)}
\NormalTok{boston.train <-}\StringTok{ }\NormalTok{Boston }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{sample_frac}\NormalTok{(}\DataTypeTok{size =} \DecValTok{2}\OperatorTok{/}\DecValTok{3}\NormalTok{)}
\NormalTok{boston.test <-}\StringTok{ }\KeywordTok{setdiff}\NormalTok{(Boston, boston.train)}
\end{Highlighting}
\end{Shaded}

Aby m贸c por贸wna wyniki predykcji z metody bagging, najpierw zostanie zbudowane jedno drzewo decyzyjne w oparciu o algorytm CART.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(rpart)}
\KeywordTok{library}\NormalTok{(rpart.plot)}
\NormalTok{boston.rpart <-}\StringTok{ }\KeywordTok{rpart}\NormalTok{(medv}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data =}\NormalTok{ boston.train)}
\NormalTok{x <-}\StringTok{ }\KeywordTok{summary}\NormalTok{(boston.rpart)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Call:
## rpart(formula = medv ~ ., data = boston.train)
##   n= 337 
## 
##           CP nsplit rel error    xerror       xstd
## 1 0.49839799      0 1.0000000 1.0086928 0.10259521
## 2 0.15725128      1 0.5016020 0.5442932 0.06125724
## 3 0.07485605      2 0.3443507 0.4031978 0.05139310
## 4 0.03672387      3 0.2694947 0.3127794 0.04599170
## 5 0.03552748      4 0.2327708 0.2974517 0.04560807
## 6 0.01695185      5 0.1972433 0.2553208 0.04022970
## 7 0.01422576      6 0.1802915 0.2713816 0.04099092
## 8 0.01103490      7 0.1660657 0.2744789 0.04107777
## 9 0.01000000      8 0.1550308 0.2720415 0.04119266
## 
## Variable importance
##      rm   lstat   indus ptratio    crim     age     nox     dis      zn     tax 
##      33      19       9       8       7       6       6       5       3       2 
##     rad    chas 
##       1       1 
## 
## Node number 1: 337 observations,    complexity param=0.498398
##   mean=22.69792, MSE=79.32964 
##   left son=2 (286 obs) right son=3 (51 obs)
##   Primary splits:
##       rm      < 6.92     to the left,  improve=0.4983980, (0 missing)
##       lstat   < 9.725    to the right, improve=0.4424796, (0 missing)
##       indus   < 6.66     to the right, improve=0.2796065, (0 missing)
##       ptratio < 19.65    to the right, improve=0.2600149, (0 missing)
##       nox     < 0.6695   to the right, improve=0.2346383, (0 missing)
##   Surrogate splits:
##       ptratio < 14.55    to the right, agree=0.884, adj=0.235, (0 split)
##       lstat   < 4.915    to the right, agree=0.878, adj=0.196, (0 split)
##       zn      < 87.5     to the left,  agree=0.864, adj=0.098, (0 split)
##       indus   < 1.605    to the right, agree=0.864, adj=0.098, (0 split)
##       crim    < 0.013355 to the right, agree=0.852, adj=0.020, (0 split)
## 
## Node number 2: 286 observations,    complexity param=0.1572513
##   mean=20.04266, MSE=37.17489 
##   left son=4 (114 obs) right son=5 (172 obs)
##   Primary splits:
##       lstat   < 14.405   to the right, improve=0.3954065, (0 missing)
##       nox     < 0.6695   to the right, improve=0.3012249, (0 missing)
##       crim    < 8.37969  to the right, improve=0.2817286, (0 missing)
##       ptratio < 20.15    to the right, improve=0.2392532, (0 missing)
##       dis     < 2.4737   to the left,  improve=0.2295258, (0 missing)
##   Surrogate splits:
##       age   < 84.3     to the right, agree=0.808, adj=0.518, (0 split)
##       dis   < 2.23935  to the left,  agree=0.773, adj=0.430, (0 split)
##       crim  < 4.067905 to the right, agree=0.762, adj=0.404, (0 split)
##       nox   < 0.5765   to the right, agree=0.762, adj=0.404, (0 split)
##       indus < 16.57    to the right, agree=0.759, adj=0.395, (0 split)
## 
## Node number 3: 51 observations,    complexity param=0.07485605
##   mean=37.58824, MSE=54.4677 
##   left son=6 (34 obs) right son=7 (17 obs)
##   Primary splits:
##       rm      < 7.47     to the left,  improve=0.72041550, (0 missing)
##       lstat   < 3.99     to the right, improve=0.34223650, (0 missing)
##       ptratio < 15.05    to the right, improve=0.21227430, (0 missing)
##       rad     < 2.5      to the left,  improve=0.10053340, (0 missing)
##       tax     < 267      to the right, improve=0.07935891, (0 missing)
##   Surrogate splits:
##       lstat < 3.99     to the right, agree=0.824, adj=0.471, (0 split)
##       indus < 1.215    to the right, agree=0.706, adj=0.118, (0 split)
##       chas  < 0.5      to the left,  agree=0.706, adj=0.118, (0 split)
##       tax   < 225      to the right, agree=0.706, adj=0.118, (0 split)
##       crim  < 1.3713   to the left,  agree=0.686, adj=0.059, (0 split)
## 
## Node number 4: 114 observations,    complexity param=0.03552748
##   mean=15.33333, MSE=21.50994 
##   left son=8 (77 obs) right son=9 (37 obs)
##   Primary splits:
##       crim    < 0.69916  to the right, improve=0.3873341, (0 missing)
##       nox     < 0.6615   to the right, improve=0.3541892, (0 missing)
##       dis     < 2.3497   to the left,  improve=0.3182514, (0 missing)
##       ptratio < 19.45    to the right, improve=0.3102781, (0 missing)
##       tax     < 567.5    to the right, improve=0.2823826, (0 missing)
##   Surrogate splits:
##       ptratio < 19.95    to the right, agree=0.895, adj=0.676, (0 split)
##       indus   < 14.345   to the right, agree=0.868, adj=0.595, (0 split)
##       nox     < 0.5825   to the right, agree=0.868, adj=0.595, (0 split)
##       tax     < 397      to the right, agree=0.868, adj=0.595, (0 split)
##       rad     < 16       to the right, agree=0.860, adj=0.568, (0 split)
## 
## Node number 5: 172 observations,    complexity param=0.03672387
##   mean=23.16395, MSE=23.11579 
##   left son=10 (82 obs) right son=11 (90 obs)
##   Primary splits:
##       lstat   < 9.645    to the right, improve=0.24693150, (0 missing)
##       rm      < 6.543    to the left,  improve=0.17749260, (0 missing)
##       ptratio < 17.85    to the right, improve=0.07815189, (0 missing)
##       nox     < 0.5125   to the right, improve=0.07760816, (0 missing)
##       tax     < 267.5    to the right, improve=0.07238020, (0 missing)
##   Surrogate splits:
##       nox   < 0.5125   to the right, agree=0.756, adj=0.488, (0 split)
##       indus < 7.625    to the right, agree=0.750, adj=0.476, (0 split)
##       rm    < 6.26     to the left,  agree=0.738, adj=0.451, (0 split)
##       age   < 65.25    to the right, agree=0.727, adj=0.427, (0 split)
##       dis   < 3.8824   to the left,  agree=0.709, adj=0.390, (0 split)
## 
## Node number 6: 34 observations
##   mean=33.15882, MSE=13.41419 
## 
## Node number 7: 17 observations
##   mean=46.44706, MSE=18.85661 
## 
## Node number 8: 77 observations,    complexity param=0.0110349
##   mean=13.33247, MSE=15.64998 
##   left son=16 (37 obs) right son=17 (40 obs)
##   Primary splits:
##       lstat < 20.1     to the right, improve=0.24481010, (0 missing)
##       crim  < 15.718   to the right, improve=0.23250740, (0 missing)
##       dis   < 2.0037   to the left,  improve=0.17113480, (0 missing)
##       nox   < 0.6615   to the right, improve=0.11757680, (0 missing)
##       rm    < 5.5675   to the left,  improve=0.09054612, (0 missing)
##   Surrogate splits:
##       dis   < 1.9733   to the left,  agree=0.792, adj=0.568, (0 split)
##       rm    < 5.632    to the left,  agree=0.727, adj=0.432, (0 split)
##       age   < 95.35    to the right, agree=0.675, adj=0.324, (0 split)
##       crim  < 9.08499  to the right, agree=0.662, adj=0.297, (0 split)
##       black < 396.295  to the right, agree=0.623, adj=0.216, (0 split)
## 
## Node number 9: 37 observations
##   mean=19.4973, MSE=8.034858 
## 
## Node number 10: 82 observations
##   mean=20.66098, MSE=6.55677 
## 
## Node number 11: 90 observations,    complexity param=0.01695185
##   mean=25.44444, MSE=27.29425 
##   left son=22 (83 obs) right son=23 (7 obs)
##   Primary splits:
##       age   < 86.7     to the left,  improve=0.1844883, (0 missing)
##       lstat < 4.46     to the right, improve=0.1773076, (0 missing)
##       dis   < 3.0037   to the right, improve=0.1652768, (0 missing)
##       crim  < 0.628575 to the left,  improve=0.1203635, (0 missing)
##       nox   < 0.5585   to the left,  improve=0.1122403, (0 missing)
##   Surrogate splits:
##       nox     < 0.5585   to the left,  agree=0.978, adj=0.714, (0 split)
##       dis     < 2.1491   to the right, agree=0.978, adj=0.714, (0 split)
##       crim    < 0.643205 to the left,  agree=0.967, adj=0.571, (0 split)
##       indus   < 16.57    to the left,  agree=0.956, adj=0.429, (0 split)
##       ptratio < 14.75    to the right, agree=0.956, adj=0.429, (0 split)
## 
## Node number 16: 37 observations
##   mean=11.2973, MSE=10.14026 
## 
## Node number 17: 40 observations
##   mean=15.215, MSE=13.37128 
## 
## Node number 22: 83 observations,    complexity param=0.01422576
##   mean=24.79277, MSE=13.56694 
##   left son=44 (55 obs) right son=45 (28 obs)
##   Primary splits:
##       rm      < 6.543    to the left,  improve=0.3377388, (0 missing)
##       lstat   < 5.41     to the right, improve=0.2548210, (0 missing)
##       tax     < 267.5    to the right, improve=0.2210129, (0 missing)
##       ptratio < 18.05    to the right, improve=0.1394682, (0 missing)
##       dis     < 6.4889   to the right, improve=0.1125739, (0 missing)
##   Surrogate splits:
##       lstat   < 5.055    to the right, agree=0.783, adj=0.357, (0 split)
##       ptratio < 15.75    to the right, agree=0.723, adj=0.179, (0 split)
##       crim    < 0.39646  to the left,  agree=0.699, adj=0.107, (0 split)
##       chas    < 0.5      to the left,  agree=0.687, adj=0.071, (0 split)
##       age     < 74.15    to the left,  agree=0.687, adj=0.071, (0 split)
## 
## Node number 23: 7 observations
##   mean=33.17143, MSE=125.3192 
## 
## Node number 44: 55 observations
##   mean=23.26545, MSE=8.880443 
## 
## Node number 45: 28 observations
##   mean=27.79286, MSE=9.189949
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{rpart.plot}\NormalTok{(boston.rpart)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{EksploracjaDanych_files/figure-latex/unnamed-chunk-32-1.pdf}
\caption{\label{fig:unnamed-chunk-32}Drzewo regresyjne pene}
\end{figure}

Przycinamy drzewo\ldots{}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{printcp}\NormalTok{(boston.rpart)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Regression tree:
## rpart(formula = medv ~ ., data = boston.train)
## 
## Variables actually used in tree construction:
## [1] age   crim  lstat rm   
## 
## Root node error: 26734/337 = 79.33
## 
## n= 337 
## 
##         CP nsplit rel error  xerror     xstd
## 1 0.498398      0   1.00000 1.00869 0.102595
## 2 0.157251      1   0.50160 0.54429 0.061257
## 3 0.074856      2   0.34435 0.40320 0.051393
## 4 0.036724      3   0.26949 0.31278 0.045992
## 5 0.035527      4   0.23277 0.29745 0.045608
## 6 0.016952      5   0.19724 0.25532 0.040230
## 7 0.014226      6   0.18029 0.27138 0.040991
## 8 0.011035      7   0.16607 0.27448 0.041078
## 9 0.010000      8   0.15503 0.27204 0.041193
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plotcp}\NormalTok{(boston.rpart)}
\end{Highlighting}
\end{Shaded}

\includegraphics{EksploracjaDanych_files/figure-latex/unnamed-chunk-33-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{boston.rpart2 <-}\StringTok{ }\KeywordTok{prune}\NormalTok{(boston.rpart, }\DataTypeTok{cp =} \FloatTok{0.016952}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{rpart.plot}\NormalTok{(boston.rpart2)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{EksploracjaDanych_files/figure-latex/unnamed-chunk-34-1.pdf}
\caption{\label{fig:unnamed-chunk-34}Drzewo regresyjne przycite}
\end{figure}

Predykcja na podstawie drzewa na zbiorze testowym.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{boston.pred <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(boston.rpart2, }\DataTypeTok{newdata =}\NormalTok{ boston.test)}
\NormalTok{rmse <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(pred, obs) }\KeywordTok{sqrt}\NormalTok{(}\DecValTok{1}\OperatorTok{/}\KeywordTok{length}\NormalTok{(pred)}\OperatorTok{*}\KeywordTok{sum}\NormalTok{((pred}\OperatorTok{-}\NormalTok{obs)}\OperatorTok{^}\DecValTok{2}\NormalTok{))}
\KeywordTok{rmse}\NormalTok{(boston.pred, boston.test}\OperatorTok{$}\NormalTok{medv)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 5.830722
\end{verbatim}

Teraz zbudujemy model metod bagging.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(randomForest)}
\NormalTok{boston.bag <-}\StringTok{ }\KeywordTok{randomForest}\NormalTok{(medv}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data =}\NormalTok{ boston.train, }
                           \DataTypeTok{mtry =} \KeywordTok{ncol}\NormalTok{(boston.train)}\OperatorTok{-}\DecValTok{1}\NormalTok{)}
\NormalTok{boston.bag}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
##  randomForest(formula = medv ~ ., data = boston.train, mtry = ncol(boston.train) -      1) 
##                Type of random forest: regression
##                      Number of trees: 500
## No. of variables tried at each split: 13
## 
##           Mean of squared residuals: 12.03374
##                     % Var explained: 84.83
\end{verbatim}

Predykcja na podstawie modelu

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{boston.pred2 <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(boston.bag, }\DataTypeTok{newdata =}\NormalTok{ boston.test)}
\KeywordTok{rmse}\NormalTok{(boston.pred2, boston.test}\OperatorTok{$}\NormalTok{medv)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 4.359119
\end{verbatim}

Zatem predykcja na podstawie modelu bagging jest nico lepsza ni偶 z pojedynczego drzewa. Dodatkowo mo偶emy oceni wa偶no zmiennych u偶ytych w budowie drzew.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{varImpPlot}\NormalTok{(boston.bag)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{EksploracjaDanych_files/figure-latex/unnamed-chunk-38-1.pdf}
\caption{\label{fig:unnamed-chunk-38}Wykres wa偶noci predyktor贸w}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{importance}\NormalTok{(boston.bag)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##         IncNodePurity
## crim       1335.62584
## zn           21.35274
## indus       134.28748
## chas         24.07230
## nox         423.26229
## rm        15413.69291
## age         380.78172
## dis        1204.86690
## rad          88.28151
## tax         454.99800
## ptratio     309.58412
## black       216.15512
## lstat      6217.95834
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x}\OperatorTok{$}\NormalTok{variable.importance}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##          rm       lstat       indus     ptratio        crim         age 
## 16276.30598  9170.91941  4427.10554  4039.00112  3412.53062  3170.82658 
##         nox         dis          zn         tax         rad        chas 
##  3063.70694  2681.24858  1306.29569   800.17910   539.07271   262.60146 
##       black 
##    63.78554
\end{verbatim}

W por贸wnaniu do wa偶noci zmiennych dla pojedynczego drzewa wida pewne r贸偶nice.

\hypertarget{lasy-losowe}{%
\section{Lasy losowe}\label{lasy-losowe}}

Lasy losowe s uog贸lnieniem metody bagging, polegajc na losowaniu dla ka偶dego drzewa wchodzcego w skad lasu \(m\) predyktor贸w spor贸d \(p\) dostpnych, a nastpnie budowaniu drzew z wykorzystaniem tylko tych predyktor贸w (Ho \protect\hyperlink{ref-ho1995}{1995}). Dziki temu za ka偶dy razem drzewo jest budowane w oparciu o nowy zestaw cech (najczciej przyjmujemy \(m=\sqrt{p}\)). W przypadku modeli bagging za ka偶dym razem najsilniejszy predyktor wchodzi w skad zbioru uczcego, a co za tym idzie r贸wnie偶 uczestniczy w tworzeniu regu podziau. W贸wczas wiele drzew zawierao reguy stosujce dany atrybut, a wtedy predykcje otrzymywane za pomoc drzew byy skorelowane. Dlatego nawet du偶a liczba pr贸b bootstrapowych nie zapewniaa poprawy precyzji. Implementacja tej metody znajduje si w pakiecie \textbf{randomForest}.

\BeginKnitrBlock{example}
\protect\hypertarget{exm:przyk52}{}{\label{exm:przyk52} }Kontynuujc poprzedni przykad \ref{exm:przyk51} mo偶emy zbudowa las losowy aby przekona si czy nastpi poprawa predykcji zmiennej wynikowej.
\EndKnitrBlock{example}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{boston.rf <-}\StringTok{ }\KeywordTok{randomForest}\NormalTok{(medv}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data =}\NormalTok{ boston.train)}
\NormalTok{boston.rf}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
##  randomForest(formula = medv ~ ., data = boston.train) 
##                Type of random forest: regression
##                      Number of trees: 500
## No. of variables tried at each split: 4
## 
##           Mean of squared residuals: 12.05123
##                     % Var explained: 84.81
\end{verbatim}

Por贸wnanie MSE na pr贸bach uczcych pomidzy lasem losowym i modelem bagging wypada nieco na korzy bagging.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{boston.pred3 <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(boston.rf, }\DataTypeTok{newdata =}\NormalTok{ boston.test)}
\KeywordTok{rmse}\NormalTok{(boston.pred3, boston.test}\OperatorTok{$}\NormalTok{medv)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 3.79973
\end{verbatim}

Wa偶no zmiennych r贸wnie偶 si nieco r贸偶ni.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{varImpPlot}\NormalTok{(boston.rf)}
\end{Highlighting}
\end{Shaded}

\includegraphics{EksploracjaDanych_files/figure-latex/unnamed-chunk-41-1.pdf}

\hypertarget{boosting}{%
\section{Boosting}\label{boosting}}

Rozwa偶ania na temat metody \emph{boosting} zaczy si od pyta postawionych w publikacji Kearns and Valiant (\protect\hyperlink{ref-kearns1989}{1989}), czy da si na podstawie na podstawie zbioru sabych modeli stworzy jeden dobry? Odpowiedzi pozytywnej na nie udzielili, najpierw Schapire (\protect\hyperlink{ref-schapire1990}{1990}), a potem Breiman (\protect\hyperlink{ref-breiman1998}{1998}). W metodzie boosting nie stosuje si pr贸b bootstrapowych ale odpowiednio modyfikuje si drzewo wyjciowe w kolejnych krokach na tym samym zbiorze uczcym. Algorytm dla drzewa regresyjnego jest nastpujcy:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Ustal \(\hat{f}(x)=0\) i \(r_i=y_i\) dla ka偶dego \(i\) w zbiorze uczcym.
\item
  Dla \(b=1,2,\ldots, B\) powtarzaj:

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    naucz drzewo \(\hat{f}^b\) o \(d\) reguach podziau (czyli \(d+1\) liciach) na zbiorze \((X_i, r_i)\),
  \item
    zaktualizuj drzewo do nowej ``skurczonej'' wersji
    \begin{equation}
     \hat{f}(x)\leftarrow \hat{f}(x)+\lambda\hat{f}^b(x),
    \end{equation}
  \item
    zaktualizuj reszty
    \begin{equation}
     r_i\leftarrow r_i-\lambda\hat{f}^b(x_i).
    \end{equation}
  \end{enumerate}
\item
  Wyznacz boosted model
  \begin{equation}
    \hat{f}(x) = \sum_{b=1}^B\lambda\hat{f}^b(x)
  \end{equation}
\end{enumerate}

Uczenie drzew klasyfikacyjnego metoda boosting przebiega w podobny spos贸b. Wynik uczenia drzew metod boosting zale偶y od trzech parametr贸w:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Liczby drzew \(B\). W przeciwiestwie do metody bagging i las贸w losowych, zbyt du偶e \(B\) mo偶e doprowadzi do przeuczenia modelu. \(B\) ustala si najczciej na podstawie walidacji krzy偶owej.
\item
  Parametru ``kurczenia'' (ang. \emph{shrinkage}) \(\lambda\). Kontroluje on szybko uczenia si kolejnych drzew. Typowe wartoci \(\lambda\) to 0.01 lub 0.001. Bardzo mae \(\lambda\) mo偶e wymaga dobrania wikszego \(B\), aby zapewni dobr jako predykcyjn modelu.
\item
  Liczby podzia贸w w drzewach \(d\), kt贸ra decyduje o zo偶onoci drzewa. Bywa, 偶e nawet \(d=1\) daje dobre rezultaty, poniewa偶 model w贸wczas uczy si powoli.
\end{enumerate}

Implementacj metody boosting mo偶na znale藕 w pakiecie \textbf{gbm} (Greenwell et al. \protect\hyperlink{ref-R-gbm}{2019})

\BeginKnitrBlock{example}
\protect\hypertarget{exm:przyk53}{}{\label{exm:przyk53} }Metod boosting zastosujemy do zadania predykcji ceny mieszka na przedmieciach Bostonu. Dob贸r parametr贸w modelu bdzie arbitralny, wic niekoniecznie model bdzie najlepiej dopasowany.
\EndKnitrBlock{example}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(gbm)}
\NormalTok{boston.boost <-}\StringTok{ }\KeywordTok{gbm}\NormalTok{(medv}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data =}\NormalTok{ boston.train,}
                    \DataTypeTok{distribution =} \StringTok{"gaussian"}\NormalTok{, }
                    \DataTypeTok{n.trees =} \DecValTok{5000}\NormalTok{,}
                    \DataTypeTok{interaction.depth =} \DecValTok{2}\NormalTok{,}
                    \DataTypeTok{shrinkage =} \FloatTok{0.01}\NormalTok{)}
\NormalTok{boston.boost}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## gbm(formula = medv ~ ., distribution = "gaussian", data = boston.train, 
##     n.trees = 5000, interaction.depth = 2, shrinkage = 0.01)
## A gradient boosted model with gaussian loss function.
## 5000 iterations were performed.
## There were 13 predictors of which 13 had non-zero influence.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(boston.boost)}
\end{Highlighting}
\end{Shaded}

\includegraphics{EksploracjaDanych_files/figure-latex/unnamed-chunk-43-1.pdf}

\begin{verbatim}
##             var    rel.inf
## rm           rm 38.3955886
## lstat     lstat 29.4805422
## dis         dis  9.0886721
## crim       crim  5.7399540
## nox         nox  3.7754214
## ptratio ptratio  3.2740541
## black     black  3.1164954
## age         age  2.9063950
## tax         tax  1.8433918
## chas       chas  0.9067974
## indus     indus  0.7627923
## rad         rad  0.5523485
## zn           zn  0.1575472
\end{verbatim}

Predykcja na podstawie metody boosting

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{boston.pred4 <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(boston.boost, }\DataTypeTok{newdata =}\NormalTok{ boston.test, }\DataTypeTok{n.trees =} \DecValTok{5000}\NormalTok{)}
\KeywordTok{rmse}\NormalTok{(boston.pred4, boston.test}\OperatorTok{$}\NormalTok{medv)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 3.801233
\end{verbatim}

\(RMSE\) jest w tym przypadku nieco wiksze ni偶 w lasach losowych ale sporo mniejsze ni偶 w metodzie bagging. Wszystkie metody wzmacnianych drzew daj wyniki lepsze ni偶 pojedyncze drzewa.

\hypertarget{klasyfikatory-liniowe}{%
\chapter{Klasyfikatory liniowe}\label{klasyfikatory-liniowe}}

Obszern rodzin klasyfikator贸w stanowi modele liniowe (ang. \emph{linear classification models}). Klasyfikacji w tej rodzinie technik dokonuje si na podstawie modeli funkcji kombinacji liniowej predyktor贸w. Jest to ujcie parametryczne, w kt贸rym klasyfikacji nowej wartoci dokonujemy na podstawie atrybut贸w obserwacji i wektora parametr贸w. Uczenie na podstawie zestawu treningowego polega na oszacowaniu parametr贸w modelu. W odr贸偶nieniu od metod nieparametrycznych posta modelu tym razem jest znana. Ka偶dy klasyfikator liniowy skad si z funkcji wewntrznej (ang. \emph{inner representation function}) i funkcji zewntrznej (ang. \emph{outer representation function}).
Pierwsza jest funkcj rzeczywist parametr贸w modelu i wartoci atrybut贸w obserwacji
\begin{equation}
    g(x) = F(\mathbf{a}(x),\mathbf{w})=\sum_{i=0}^pw_ia_i(x)=\mathbf{w}\circ \mathbf{a}(x),
\end{equation}
przyjmujc, 偶e \(a_0(x)=1\).

Funkcja zewntrzna przyporzdkowuje binarnie klasy na podstawie wartoci funkcji wewntrznej. Istniej dwa g贸wne typy tych klasyfikacji:

\begin{itemize}
\tightlist
\item
  brzegowa - przyjmujemy, 偶e funkcje wewntrzne tworz granice zbior贸w obserwacji r贸偶nych klas,
\item
  probabilistyczna - bazujca na tym, 偶e funkcje wewntrzne mog porednio wykazywa prawdopodobiestwo przynale偶noci do danej klasy.
\end{itemize}

Pierwsza dzieli przestrze obserwacji za pomoc hiperpaszczyzn na obszary jednorodne pod wzgldem przynale偶noci do klas. Druga jest pr贸b parametrycznej reprezentacji prawdopodobiestw przynale偶noci do klas. Klasyfikacji na podstawie prawdopodobiestw mo偶na dokona na r贸偶ne sposoby, stosujc:

\begin{itemize}
\tightlist
\item
  najwiksze prawdopodobiestwo,
\item
  funkcj najmniejszego kosztu bdnej klasyfikacji,
\item
  krzywych ROC (ang. \emph{Receiver Operating Characteristic} - o tym p贸藕niej).
\end{itemize}

Podejcie brzegowe lub probabilistyczne prowadzi najczciej do dw贸ch typ贸w reprezentacji funkcji zewntrznej:

\begin{itemize}
\tightlist
\item
  reprezentacji progowej (ang. \emph{threshold representation}) - najczciej przy podejciu brzegowym,
\item
  reprezentacji logistycznej (ang. \emph{logit representation}) - przy podejciu probabilistycznym.
\end{itemize}

\hypertarget{reprezentacja-progowa}{%
\section{Reprezentacja progowa}\label{reprezentacja-progowa}}

W przypadku klasyfikacji dwustanowej, dziedzina jest dzielona na dwa regiony (pozytywny i negatywny) poprzez por贸wnanie funkcji zewntrznej z wartoci progow. Bez straty og贸lnoci mo偶na sprawi, 偶e bdzie to warto 0
\begin{equation}
    h(x)=H(g(x))= \begin{cases}
        1, &\text{ jeli } g(x)\geq 0\\
        0, &\text{ w przeciwnym przypadku.}
    \end{cases}
\end{equation}
Czasami u偶ywa si parametryzacji \(\{-1,1\}\).
Przez por贸wnanie \(g(x)\) z 0 definiuje si hiperpaszczyzn w \(p\) wymiarowej przestrzeni, kt贸ra rozdziela dziedzin na regiony pozytywne i negatywne. W tym ujciu m贸wimy o liniowej separowalnoci obserwacji r贸偶nych klas, jeli istnieje hiperpaszczyzna je rozdzielajca.

\hypertarget{reprezentacja-logitowa}{%
\section{Reprezentacja logitowa}\label{reprezentacja-logitowa}}

Najbardziej popularn reprezentacj parametryczn stosowan w klasyfikacji jest reprezentacja logitowa
\begin{equation}
    \P(y=1|x)=\frac{e^{g(x)}}{e^{g(x)}+1}.
\end{equation}
W贸wczas \(g(x)\) nie reprezentuje bezporednio \(\P(y=1|x)\) ale jego logit
\begin{equation}
    g(x)=\logit(\P(y=1|x)),
\end{equation}
gdzie \(\logit(p)=\ln\frac{p}{1-p}\). Dlatego waciwa posta reprezentacji jest nastpujca
\begin{equation}
    \P(y=1|x)=\logit^{-1}(g(x)).
\end{equation}
W ten spos贸b reprezentacja logitowa jest r贸wnowa偶na reprezentacji progowej, poniewa偶
\begin{equation}
    g(x)=\ln\frac{\P(y=1|x)}{1-\P(y=1|x)}=\ln\frac{\P(y=1|x)}{\P(y=0|x)}>0.
\end{equation}
Jednak zalet reprezentacji logitowej, w por贸wnaniu do progowej, jest to, 偶e mo偶na wyznaczy prawdopodobiestwa przynale偶noci do obu klas. W przypadku klasyfikacji wielostanowej uczymy tyle funkcji \(h\) ile jest klas.

\hypertarget{wady-klasyfikatoruxf3w-liniowych}{%
\section{Wady klasyfikator贸w liniowych}\label{wady-klasyfikatoruxf3w-liniowych}}

\begin{itemize}
\tightlist
\item
  tylko w przypadku prostych funkcji wewntrznych jestemy w stanie oceni wpyw poszczeg贸lnych predykor贸w na klasyfikacj,
\item
  jako predykcji zale偶y od doboru funkcji wewntrznej (liniowa w cisym sensie jest najczciej niewystarczajca),
\item
  nie jest w stanie klasyfikowa poprawnie stan贸w (nie jest liniowo separowalna) w zagadnieniach typu XOR.
\end{itemize}

\hypertarget{regresja-logistyczna}{%
\chapter{Regresja logistyczna}\label{regresja-logistyczna}}

\hypertarget{model-1}{%
\section{Model}\label{model-1}}

Regresja logistyczna (ang. \emph{logistic regression}) jest technik z rodziny klasyfikator贸w liniowych z reprezentacj logistyczn, a formalnie nale偶y do rodziny uog贸lnionych modeli liniowych (GLM). Stosowana jest w贸wczas, gdy zmienna wynikowa posiada dwa stany (sukces i pora偶ka), kodowane najczciej za pomoc 1 i 0. W tej metodzie modelowane jest warunkowe prawdopodobiestwo sukcesu za pomoc kombinacji liniowej predyktor贸w \(X\).

Og贸lna posta modelu
\begin{align}
    Y\sim &B(1, p)\\
    p(X)=&\E(Y|X)=\frac{\exp(\beta X)}{1+\exp(\beta X)},
\end{align}
gdzie \(B(1,p)\) jest rozkadem dwumianowym o prawdopodobiestwie sukcesu \(p\), a \(\beta X\) oznacza kombinacj liniow parametr贸w modelu i wartoci zmiennych niezale偶nych, przyjmujc, 偶e \(x_0=1\). Jako funkcji czcej (czyli opisujcej zwizek midzy kombinacj liniow predyktor贸w i prawdopodobiestwem sukcesu) u偶yto \emph{logitu}. Pozwala on na wygodn interpretacj wynik贸w w terminach szans.

Szans (ang. \emph{odds}) nazywamy stosunek prawdopodobiestwa sukcesu do prawdopodobiestwa pora偶ki
\begin{equation}
    o = \frac{p}{1-p}.
\end{equation}

Poniewa偶 bdziemy przyjmowali, 偶e \(p\in (0,1)\), to \(o\in (0,\infty)\), a jej logarytm nale偶y do przedziau \((-\infty, \infty)\).

Zatem logarytm szansy jest kombinacj liniow predyktor贸w
\begin{equation}
    \log\left[\frac{p(X)}{1-p(X)}\right]=\beta_0+\beta_1x_1+\ldots+\beta_dx_d.
\end{equation}

\hypertarget{estymacja-parametruxf3w-modelu}{%
\section{Estymacja parametr贸w modelu}\label{estymacja-parametruxf3w-modelu}}

Estymacji parametr贸w modelu logistycznego dokonujemy za pomoc metody najwikszej wiarogodnoci. Funkcja wiarogodnoci w tym przypadku przyjmuje posta
\begin{equation}
    L(X_1,\ldots,X_n,\beta)=\prod_{i=1}^{n}p(X_i)^Y_i[1-p(X_i)]^{1-Y_i},
\end{equation}
gdzie wektor \(\beta\) jest uwikany w funkcji \(p(X_i)\). Maksymalizacji dokonujemy raczej po nao偶eniu na funkcj wiarogodnoci logarytmu, bo to uatwia szukanie ekstremum.
\begin{equation}
    \log L(X_1,\ldots,X_n,\beta) = \sum_{i=1}^n(Y_i\log p(X_i)+(1-Y_i)\log(1-p(X_i))).
\end{equation}

\hypertarget{interpretacja}{%
\section{Interpretacja}\label{interpretacja}}

Interpretacja (lat. \emph{ceteris paribus} - ``inne takie samo'') poszczeg贸lnych parametr贸w modelu jest nastpujca:

\begin{itemize}
\tightlist
\item
  jeli \(b_i>0\) - to zmienna \(x_i\) ma wpyw stymulujcy pojawienie si sukcesu,
\item
  jeli \(b_i<0\) - to zmienna \(x_i\) ma wpyw ograniczajcy pojawienie si sukcesu,
\item
  jeli \(b_i=0\) - to zmienna \(x_i\) nie ma wpywu na pojawienie si sukcesu.
\end{itemize}

Iloraz szans (ang. \emph{odds ratio}) stosuje si w przypadku por贸wnywania dw贸ch klas obserwacji. Jest on jak sama nazwa wskazuje ilorazem szans zajcia sukcesu w obu klasach
\begin{equation}
    OR = \frac{p_1}{1-p_1}\frac{1-p_2}{p_2},
\end{equation}
gdzie \(p_i\) oznacza zajcie sukcesu w \(i\)-tej klasie.

Interpretujemy go nastpujco:

\begin{itemize}
\tightlist
\item
  jeli \(OR>1\) - to w pierwszej grupie zajcie sukcesu jest bardziej prawdopodobne,
\item
  jeli \(OR<1\) - to w drugiej grupie zajcie sukcesu jest bardziej prawdopodobne,
\item
  jeli \(OR=1\) - to w obu grupach zajcie sukcesu jest jednakowo prawdopodobne.
\end{itemize}

\BeginKnitrBlock{example}
\protect\hypertarget{exm:logit}{}{\label{exm:logit} }Jako ilustracj dziaania regresji logistycznej u偶yjemy modelu dla danych ze zbioru \texttt{Default} pakietu \texttt{ISLR}.
\EndKnitrBlock{example}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(ISLR)}
\KeywordTok{head}\NormalTok{(Default)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   default student   balance    income
## 1      No      No  729.5265 44361.625
## 2      No     Yes  817.1804 12106.135
## 3      No      No 1073.5492 31767.139
## 4      No      No  529.2506 35704.494
## 5      No      No  785.6559 38463.496
## 6      No     Yes  919.5885  7491.559
\end{verbatim}

Zmienn zale偶n jest \texttt{default}, a pozostae s predyktorami. najpierw dokonamy podziau pr贸by na uczca i testow, a nastpnie zbudujemy model.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{2019}\NormalTok{)}
\NormalTok{ind <-}\StringTok{ }\KeywordTok{sample}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\KeywordTok{nrow}\NormalTok{(Default), }\DataTypeTok{size =} \DecValTok{2}\OperatorTok{/}\DecValTok{3}\OperatorTok{*}\KeywordTok{nrow}\NormalTok{(Default))}
\NormalTok{dt.ucz <-}\StringTok{ }\NormalTok{Default[ind,]}
\NormalTok{dt.test <-}\StringTok{ }\NormalTok{Default[}\OperatorTok{-}\NormalTok{ind,]}
\NormalTok{mod.logit <-}\StringTok{ }\KeywordTok{glm}\NormalTok{(default}\OperatorTok{~}\NormalTok{., dt.ucz, }\DataTypeTok{family =} \KeywordTok{binomial}\NormalTok{(}\StringTok{"logit"}\NormalTok{))}
\KeywordTok{summary}\NormalTok{(mod.logit)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## glm(formula = default ~ ., family = binomial("logit"), data = dt.ucz)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.1913  -0.1410  -0.0537  -0.0192   3.7527  
## 
## Coefficients:
##               Estimate Std. Error z value Pr(>|z|)    
## (Intercept) -1.128e+01  6.169e-01 -18.287   <2e-16 ***
## studentYes  -4.627e-01  2.862e-01  -1.617    0.106    
## balance      5.830e-03  2.883e-04  20.221   <2e-16 ***
## income       9.460e-06  9.833e-06   0.962    0.336    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 1967.2  on 6665  degrees of freedom
## Residual deviance: 1046.5  on 6662  degrees of freedom
## AIC: 1054.5
## 
## Number of Fisher Scoring iterations: 8
\end{verbatim}

Tylko \texttt{income} nie ma 偶adnego wpywu na prawdopodobiestwo stanu \texttt{Yes} zmiennej \texttt{default}. Zmienna \texttt{balance} wpywa stymulujco na prawdopodobiestwo pojawienia si sukcesu. Natomiast jeli badana osoba jest studentem (\texttt{studentYes}), to ma wpyw ograniczajcy na pojawienie si sukcesu. Chcc por贸wna dwie grupy obserwacji, przykadowo student贸w z nie-studentami, mo偶emy wykorzysta iloraz szans.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{exp}\NormalTok{(}\KeywordTok{cbind}\NormalTok{(}\DataTypeTok{OR =} \KeywordTok{coef}\NormalTok{(mod.logit), }\KeywordTok{confint}\NormalTok{(mod.logit))) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{kable}\NormalTok{(}\DataTypeTok{digits =} \DecValTok{4}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{l|r|r|r}
\hline
  & OR & 2.5 \% & 97.5 \%\\
\hline
(Intercept) & 0.0000 & 0.0000 & 0.0000\\
\hline
studentYes & 0.6296 & 0.3598 & 1.1060\\
\hline
balance & 1.0058 & 1.0053 & 1.0064\\
\hline
income & 1.0000 & 1.0000 & 1.0000\\
\hline
\end{tabular}

Z powy偶szej tabeli wynika, 偶e bycie studentem zmniejsza szanse na \texttt{Yes} w zmiennej \texttt{default} o okoo 37\% (w stosunku do nie-student贸w). Natomiast wzrost zmiennej \texttt{balance} przy zachowaniu pozostaych zmiennych na tym samym poziomie skutkuje wzrostem szans na \texttt{Yes} o okoo 0.6\%.

Chcc przeprowadzi predykcj na podstawie modelu dla ustalonych wartoci cech (np. \texttt{student\ =\ Yes}, \texttt{balance\ =\ \$1000} i \texttt{income\ =\ \$40000}) postpujemy nastpujco

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dt.new <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{student =} \StringTok{"Yes"}\NormalTok{, }\DataTypeTok{balance =} \DecValTok{1000}\NormalTok{, }\DataTypeTok{income =} \DecValTok{40000}\NormalTok{)}
\KeywordTok{predict}\NormalTok{(mod.logit, }\DataTypeTok{newdata =}\NormalTok{ dt.new, }\DataTypeTok{type =} \StringTok{"response"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##           1 
## 0.003931398
\end{verbatim}

Otrzymany wynik jest oszacowanym prawdopodobiestwem warunkowym wystpienia sukcesu (\texttt{default\ =\ Yes}). Wida zatem, 偶e poziomy badanych cech sprzyjaj raczej pora偶ce.

Jeli chcemy sprawdzi jako klasyfikacji na zbiorze testowym, to musimy ustali na jakim poziomie prawdopodobiestwa bdziemy uznawa obserwacj za sukces. W zale偶noci od tego, na predykcji jakiego stanu zale偶y nam bardziej, mo偶emy r贸偶nie dobiera ten pr贸g (bez 偶adnych dodatkowych przesanek najczciej jest to 0.5).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pred <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(mod.logit, }\DataTypeTok{newdata =}\NormalTok{ dt.test, }\DataTypeTok{type =} \StringTok{"response"}\NormalTok{)}
\NormalTok{pred.class <-}\StringTok{ }\KeywordTok{ifelse}\NormalTok{(pred }\OperatorTok{>}\StringTok{ }\FloatTok{0.5}\NormalTok{, }\StringTok{"Yes"}\NormalTok{, }\StringTok{"No"}\NormalTok{)}
\NormalTok{(tab <-}\StringTok{ }\KeywordTok{table}\NormalTok{(pred.class, dt.test}\OperatorTok{$}\NormalTok{default))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##           
## pred.class   No  Yes
##        No  3211   71
##        Yes   15   37
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(acc <-}\StringTok{ }\KeywordTok{sum}\NormalTok{(}\KeywordTok{diag}\NormalTok{(}\KeywordTok{prop.table}\NormalTok{(tab))))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.9742052
\end{verbatim}

Klasyfikacja na poziomie 97\% wskazuje na dobre dopasowanie modelu.

\hypertarget{LDA}{%
\chapter{Analiza dyskryminacyjna}\label{LDA}}

Analiza dyskryminacyjna (ang. \emph{discriminant analysis}) jest grup technik dyskryminacji obserwacji wzgldem przynale偶noci do klas. Cz z nich nale偶y do klasyfikator贸w liniowych (cho nie zawsze w cisym sensie). Za autor贸w tej metody uwa偶a si Fisher'a (\protect\hyperlink{ref-fisher1936}{1936}) i Welch'a (\protect\hyperlink{ref-welch1939}{1939}). Ka偶dy z nich prezentowa nieco inne podejcie do tematu klasyfikacji. Welch poszukiwa klasyfikacji minimalizujcej prawdopodobiestwo bdnej klasyfikacji, znane jako \protect\hyperlink{bayes}{klasyfikatory bayesowskie}. Podejcie Fisher'a skupiao si raczej na por贸wnaniu zmiennoci midzygrupowej do zmiennoci wewntrzgrupowej. Wychodzc z zao偶enia, 偶e iloraz tych wariancji powinien by stosunkowo du偶y przy r贸偶nych klasach, jeli do ich opisu u偶yjemy odpowiednich zmiennych niezale偶nych. W istocie chodzi o znalezienie takiego wektora, w kierunku kt贸rego wspomniany iloraz wariancji jest najwikszy.

\hypertarget{liniowa-analiza-dyskryminacyjna-fishera}{%
\section{Liniowa analiza dyskryminacyjna Fisher'a}\label{liniowa-analiza-dyskryminacyjna-fishera}}

\hypertarget{dwie-kategorie-zmiennej-grupujux105cej}{%
\subsection{Dwie kategorie zmiennej grupujcej}\label{dwie-kategorie-zmiennej-grupujux105cej}}

Niech \(\boldsymbol D\) bdzie zbiorem zawierajcym \(n\) punkt贸w \(\{\boldsymbol x, y\}\), gdzie \(\boldsymbol x\in \mathbb{R}^d\), a \(y\in \{c_1,\ldots,c_k\}\). Niech \(\boldsymbol D_i\) oznacza podzbi贸r punkt贸w zbioru \(\boldsymbol D\), kt贸re nale偶 do klasy \(c_i\), czyli \(\boldsymbol D_i=\{\boldsymbol x|y=c_i\}\) i niech \(|\boldsymbol D_i|=n_i\). Na pocztek za贸偶my, 偶e \(\boldsymbol D\) skada si tylko z \(\boldsymbol D_1\) i \(\boldsymbol D_2\).

Niech \(\boldsymbol w\) bdzie wektorem jednostkowym (\(\boldsymbol w'\boldsymbol w=1\)), w贸wczas rzut ortogonalny punku \(\boldsymbol x_i\) na wektor \(\boldsymbol w\) mo偶na zapisa nastpujco
\begin{equation}
    \tilde{\boldsymbol x}=\left(\frac{\boldsymbol w'\boldsymbol x}{\boldsymbol w'\boldsymbol w}\right)\boldsymbol w=(\boldsymbol w'\boldsymbol x)\boldsymbol w = a\boldsymbol w,
\end{equation}
gdzie \(a\) jest wsp贸rzdn punktu \(\tilde{\boldsymbol x}\) w kierunku wektora \(\boldsymbol w\), czyli
\begin{equation}
    a=\boldsymbol w'\boldsymbol x.
\end{equation}
Zatem \((a_1,\ldots,a_n)\) reprezentuj odwzorowanie \(\mathbb{R}^d\) w \(\mathbb{R}\), czyli z \(d\)-wymiarowej przestrzeni w przestrze generowan przez \(\boldsymbol w\).

\begin{figure}

{\centering \includegraphics[width=5.83in]{images/rzut} 

}

\caption{Rzut ortogonalny punkt贸w w kierunku wektora $\boldsymbol w$}\label{fig:rzut}
\end{figure}

Ka偶dy punkt nale偶y do pewnej klasy, dlatego mo偶emy wyliczy
\begin{align}
    m_1=&\frac{1}{n_1}\sum_{ \boldsymbol x\in \boldsymbol D_1}a=\\
    =&\frac{1}{n_1}\sum_{ \boldsymbol x\in \boldsymbol D_1} \boldsymbol w' \boldsymbol x=\\
    =& \boldsymbol w'\left(\frac{1}{n_1}\sum_{ \boldsymbol x\in \boldsymbol D_1} \boldsymbol x \right)=\\
    =& \boldsymbol w' \boldsymbol{\mu}_1,
    \label{eq:m}
\end{align}
gdzie \(\boldsymbol \mu_1\) jest wektorem rednich punkt贸w z \(\boldsymbol D_1\). W podobny spos贸b mo偶na policzy \(m_2 = \boldsymbol w' \boldsymbol \mu_2\). Oznacza to, 偶e rednia projekcji jest projekcj rednich.

Rozsdnym wydaje si teraz poszuka takiego wektora, aby \(|m_1-m_2|\) bya maksymalnie du偶a przy zachowaniu niezbyt du偶ej zmiennoci wewntrz grup. Dlatego kryterium Fisher'a przyjmuje posta
\begin{equation}
    \max_{ \boldsymbol w}J(\boldsymbol w)=\frac{(m_1-m_2)^2}{ss_1^2+ss_2^2},
    \label{eq:condFisher}
\end{equation}
gdzie \(ss_j^2=\sum_{ \boldsymbol x\in \boldsymbol D_j}(a-m_j)^2=n_j\sigma_j^2.\)

Zauwa偶my, 偶e licznik w \eqref{eq:condFisher} da si zapisa jako
\begin{align}
    (m_1-m_2)^2=& ( \boldsymbol w'( \boldsymbol \mu_1- \boldsymbol \mu_2))^2=\\
    =& \boldsymbol w'(\boldsymbol \mu_1- \boldsymbol \mu_2)(\boldsymbol \mu_1- \boldsymbol \mu_2)'\boldsymbol w=\\
    =& \boldsymbol w' \boldsymbol B \boldsymbol w
\end{align}
gdzie \(\boldsymbol B=(\boldsymbol \mu_1- \boldsymbol \mu_2)(\boldsymbol \mu_1- \boldsymbol \mu_2)'\) jest macierz \(d\times d\).

Ponadto
\begin{align}
    ss_j^2=&\sum_{ \boldsymbol x\in \boldsymbol D_j}(a-m_j)^2=\\
    =&\sum_{ \boldsymbol x\in \boldsymbol D_j}( \boldsymbol w' \boldsymbol x- \boldsymbol w' \boldsymbol\mu_j)^2=\\
    =& \sum_{ \boldsymbol x\in \boldsymbol D_j}( \boldsymbol{w}'( \boldsymbol{x}- \boldsymbol{\mu}_j))^2=\\
    =& \boldsymbol{w}'\left(\sum_{ \boldsymbol x\in \boldsymbol D_j}(\boldsymbol{x}-\boldsymbol \mu_j)(\boldsymbol x- \boldsymbol \mu_j)'\right) \boldsymbol{w}=\\
    =& \boldsymbol{w}' \boldsymbol{S}_j \boldsymbol{w},
    \label{eq:Sj}
\end{align}
gdzie \(\boldsymbol{S}_j=n_j \boldsymbol{\Sigma}_j\).
Zatem mianownik \eqref{eq:condFisher} mo偶emy zapisa jako
\begin{equation}
    ss_1^2+ss_2^2= \boldsymbol{w}'(\boldsymbol{S}_1+ \boldsymbol{S}_2) \boldsymbol{w}= \boldsymbol{w}' \boldsymbol{S} \boldsymbol{w},
\end{equation}
gdzie \(\boldsymbol{S}=\boldsymbol{S}_1+\boldsymbol{S}_2\).
Ostatecznie warunek Fisher'a przyjmuje posta
\begin{equation}
    \max_{ \boldsymbol{w}}J( \boldsymbol{w})=\frac{ \boldsymbol{w}' \boldsymbol{B} \boldsymbol{w}}{ \boldsymbol{w}' \boldsymbol{S} \boldsymbol{w}}.
    \label{eq:condFisher2}
\end{equation}

R贸偶niczkujc \eqref{eq:condFisher2} po \(\boldsymbol{w}\) i przyr贸wnujc go do 0, otrzymamy warunek
\begin{equation}
    \boldsymbol{B} \boldsymbol{w} = \lambda \boldsymbol{S} \boldsymbol{w},
    \label{eq:condFisher3}
\end{equation}
gdzie \(\lambda=J(\boldsymbol{w})\). Maksimum \eqref{eq:condFisher3} jest osigane dla wektora \(\boldsymbol{w}\) r贸wnego wektorowi wasnemu odpowiadajcemu najwikszej wartoci wasnej r贸wnania charakterystycznego \(|\boldsymbol{B}-\lambda\boldsymbol{S}|=0\). Jeli \(\boldsymbol{S}\) nie jest osobliwa, to rozwizanie \eqref{eq:condFisher3} otrzymujemy przez znalezienie najwikszej wartoci wasnej macierzy \(\boldsymbol{B}\boldsymbol{S}^{-1}\) lub bez wykorzystania wartoci i wektor贸w wasnych.

Poniewa偶 \(\boldsymbol{B}\boldsymbol w=\left((\boldsymbol{\mu}_1-\boldsymbol{\mu}_2)(\boldsymbol{\mu}_1-\boldsymbol{\mu}_2)'\right)\boldsymbol{w}\) jest macierz \(d \times 1\) rzdu 1, to \(\boldsymbol{B}\boldsymbol{w}\) jest punktem na kierunku wyznaczonym przez wektor \(\boldsymbol{\mu}_1-\boldsymbol{\mu}_2\), bo
\begin{align}
    \boldsymbol{B}\boldsymbol{w}=& \left((\boldsymbol{\mu}_1-\boldsymbol{\mu}_2)(\boldsymbol{\mu}_1-\boldsymbol{\mu}_2)'\right)\boldsymbol{w}=\\
    =&(\boldsymbol{\mu}_1-\boldsymbol{\mu}_2)((\boldsymbol{\mu}_1-\boldsymbol{\mu}_2)'\boldsymbol{w})=\\
    =& b(\boldsymbol{\mu}_1-\boldsymbol{\mu}_2),
\end{align}
gdzie \(b = (\boldsymbol{\mu}_1-\boldsymbol{\mu}_2)'\boldsymbol{w}\) jest skalarem.

W贸wczas \eqref{eq:condFisher3} zapiszemy jako
\begin{gather}
    b(\boldsymbol{\mu}_1-\boldsymbol{\mu}_2) = \lambda\boldsymbol{S}\boldsymbol{w}\\
    \boldsymbol{w}= \frac{b}{\lambda}\boldsymbol{S}^{-1}(\boldsymbol{\mu}_1-\boldsymbol{\mu}_2)
\end{gather}

A poniewa偶 \(b/\lambda\) jest liczb, to kierunek najlepszej dyskryminacji grup wyznacza wektor
\begin{equation}
    \boldsymbol{w}=\boldsymbol{S}^{-1}(\boldsymbol{\mu}_1-\boldsymbol{\mu}_2).
\end{equation}

\begin{figure}

{\centering \includegraphics[width=6.02in]{images/rzut2} 

}

\caption{Rzut ortogonalny w kierunku wektora $\boldsymbol{w}$, bdcego najlepiej dyskryminujcym obie grupy obserwacji}\label{fig:rzut2}
\end{figure}

\hypertarget{k-kategorii-zmiennej-grupujux105cej}{%
\subsection{\texorpdfstring{\(k\)-kategorii zmiennej grupujcej}{k-kategorii zmiennej grupujcej}}\label{k-kategorii-zmiennej-grupujux105cej}}

Uog贸lnieniem tej teorii na przypadek \(k\) klas otrzymujemy przez uwzgldnienie \(k-1\) funkcji dyskryminacyjnych. Zmienno wewntrzgrupowa przyjmuje w贸wczas posta
\begin{equation}
    \boldsymbol{S}_W=\sum_{i=1}^k\boldsymbol{S}_i,
\end{equation}
gdzie \(\boldsymbol{S}_i\) jest zdefiniowane jak w \eqref{eq:Sj}.
Niech rednia i zmienno cakowita bd dane wzorami
\begin{equation}
    \boldsymbol{m}=\frac{1}{n}\sum_{i=1}^kn_i\boldsymbol{m}_i,
\end{equation}
\begin{equation}
    \boldsymbol{S}_T=\sum_{j=1}^k\sum_{\boldsymbol{x}\in D_j}(\boldsymbol{x}-\boldsymbol{m})(\boldsymbol{x}-\boldsymbol{m})'
\end{equation}
gdzie \(\boldsymbol{m}_i\) jest okrelone jak w \eqref{eq:m}. Wtedy zmienno midzygrupow mo偶emy wyrazi jako
\begin{equation}
    \boldsymbol{S}_B=\sum_{i=1}^kn_i(\boldsymbol{m}_i-\boldsymbol{m})(\boldsymbol{m}_i-\boldsymbol{m})',
\end{equation}
bo \(\boldsymbol{S}_T=\boldsymbol{S}_W+\boldsymbol{S}_B.\)
Okrelamy projekcj \(d\)-wymiarowej przestrzeni na \(k-1\)-wymiarow przestrze za pomoc \(k-1\) funkcji dyskryminacyjnych postaci
\begin{equation}
    a_j=\boldsymbol{w}_j'\boldsymbol{x}, \quad j=1,\ldots,k-1.
\end{equation}
Poczone wszystkie \(k-1\) rzut贸w mo偶emy zapisa jako
\begin{equation}
    \boldsymbol{a}=\boldsymbol{W}'\boldsymbol{x}.
\end{equation}

W nowej przestrzeni \(k-1\)-wymiarowej mo偶emy zdefiniowa
\begin{equation}
    \tilde{\boldsymbol{m}}=\frac{1}{n}\sum_{i=1}^kn_i\tilde{\boldsymbol{m}}_i,
\end{equation}
gdzie \(\tilde{\boldsymbol{m}}_i= \frac{1}{n_i}\sum_{\boldsymbol{a}\in A_i}\boldsymbol{a}\), a \(A_i\) jest projekcj obiekt贸w z \(i\)-tej klasy na hiperpowierzchni generowan przez \(\boldsymbol{W}\).
Dalej mo偶emy zdefiniowa zmiennoci miedzy- i wewntrzgrupowe dla obiekt贸w przeksztaconych przez \(\boldsymbol{W}\)
\begin{align}
    \tilde{\boldsymbol{S}}_W=&\sum_{i=1}^k\sum_{\boldsymbol{a}\in A_i}(\boldsymbol{a}-\tilde{\boldsymbol{m}})(\boldsymbol{a}-\tilde{\boldsymbol{m}})'\\
    \tilde{\boldsymbol{S}}_B=&\sum_{i=1}^kn_i(\tilde{\boldsymbol{m}}_i-\tilde{\boldsymbol{m}})(\tilde{\boldsymbol{m}}_i-\tilde{\boldsymbol{m}})'.
\end{align}
atwo mo偶na zatem pokaza, 偶e
\begin{align}
    \tilde{\boldsymbol{S}}_W = & \boldsymbol{W}'\boldsymbol{S}_W\boldsymbol{W}\\
    \tilde{\boldsymbol{S}}_B = & \boldsymbol{W}'\boldsymbol{S}_B\boldsymbol{W}.
\end{align}
Ostatecznie warunek \eqref{eq:condFisher} w \(k\)-wymiarowym ujciu mo偶na przedstawi jako
\begin{equation}
    \max_{\boldsymbol{W}}J(\boldsymbol{W})=\frac{\tilde{\boldsymbol{S}}_W}{\tilde{\boldsymbol{S}}_B}=\frac{\boldsymbol{W}'\boldsymbol{S}_W\boldsymbol{W}}{\boldsymbol{W}'\boldsymbol{S}_B\boldsymbol{W}}.
\end{equation}
Maksimum mo偶na znale藕 poprzez rozwizanie r贸wnania charakterystycznego \begin{equation}
    |\boldsymbol{S}_B-\lambda_i\boldsymbol{S}_W|=0
\end{equation}
dla ka偶dego \(i\).

\BeginKnitrBlock{example}
\protect\hypertarget{exm:unnamed-chunk-51}{}{\label{exm:unnamed-chunk-51} }Dla danych ze zbioru \texttt{iris} przeprowadzimy analiz dyskryminacji. Implementacj metody LDA znajdziemy w pakiecie \texttt{MASS} w postaci funkcji \texttt{lda}.
\EndKnitrBlock{example}

Zaczynamy od standaryzacji zmiennych i podziau pr贸by na uczc i testow.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(MASS)}
\KeywordTok{library}\NormalTok{(tidyverse)}
\NormalTok{iris.std <-}\StringTok{ }\NormalTok{iris }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{mutate_if}\NormalTok{(is.numeric, scale)}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{44}\NormalTok{)}
\NormalTok{ind <-}\StringTok{ }\KeywordTok{sample}\NormalTok{(}\KeywordTok{nrow}\NormalTok{(iris.std), }\DataTypeTok{size =} \DecValTok{100}\NormalTok{)}
\NormalTok{dt.ucz <-}\StringTok{ }\NormalTok{iris.std[ind,]}
\NormalTok{dt.test <-}\StringTok{ }\NormalTok{iris.std[}\OperatorTok{-}\NormalTok{ind,]}
\end{Highlighting}
\end{Shaded}

Budowa modelu

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mod.lda <-}\StringTok{ }\KeywordTok{lda}\NormalTok{(Species}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data =}\NormalTok{ dt.ucz)}
\NormalTok{mod.lda}\OperatorTok{$}\NormalTok{prior}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     setosa versicolor  virginica 
##       0.32       0.32       0.36
\end{verbatim}

Prawdopodobiestwa \emph{a priori} przynale偶noci do klas przyjto na podstawie pr贸by uczcej.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mod.lda}\OperatorTok{$}\NormalTok{means}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##            Sepal.Length Sepal.Width Petal.Length Petal.Width
## setosa      -1.05240186   0.7790042   -1.2968064  -1.2536554
## versicolor   0.04201557  -0.6764307    0.2521529   0.1607657
## virginica    0.87352122  -0.2398799    1.0182730   1.0868584
\end{verbatim}

W czci \texttt{means} wywietlone s rednie poszczeg贸lnych zmiennych niezale偶nych w podziale na grupy. Dziki temu mo偶na okreli poo偶enia rodk贸w ci偶koci poszczeg贸lnych klas w oryginalnej przestrzeni.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mod.lda}\OperatorTok{$}\NormalTok{scaling}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                     LD1         LD2
## Sepal.Length  0.5884101  0.04738098
## Sepal.Width   0.7566030 -0.97757574
## Petal.Length -3.2910346  1.41170784
## Petal.Width  -2.3799488 -1.95325155
\end{verbatim}

Powy偶sza tabela zawiera wsp贸rzdne wektor贸w wyznaczajcych funkcje dyskryminacyjne. Na ich podstawie mo偶emy okreli, kt贸ra z nich wpywa najmocniej na tworzenie si nowej przestrzeni.

Obiekt \texttt{svd} przechowuje pierwiastki z \(\lambda_i\), dlatego podnoszc je do kwadratu i dzielc przez ich sum otrzymamy udzia poszczeg贸lnych zmiennych w dyskryminacji przypadk贸w. Jak wida pierwsza funkcja dyskryminacyjna w zupenoci by wystarczya.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mod.lda}\OperatorTok{$}\NormalTok{svd}\OperatorTok{^}\DecValTok{2}\OperatorTok{/}\KeywordTok{sum}\NormalTok{(mod.lda}\OperatorTok{$}\NormalTok{svd}\OperatorTok{^}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.992052359 0.007947641
\end{verbatim}

Klasyfikacja na podstawie modelu

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pred.lda <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(mod.lda, dt.test)}
\end{Highlighting}
\end{Shaded}

Wynik predykcji przechowuje trzy rodzaje obiekt贸w:

\begin{itemize}
\tightlist
\item
  klasy, kt贸re przypisa obiektom model (\texttt{class});
\item
  prawdopodobiestwa \emph{a posteriori} przynale偶noci do klas na podstawie modelu (\texttt{posterior});
\item
  wsp贸rzdne w nowej przestrzeni LD1, LD2 (\texttt{x}).
\end{itemize}

Sprawdzenie jakoci klasyfikacji

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tab <-}\StringTok{ }\KeywordTok{table}\NormalTok{(}\DataTypeTok{pred =}\NormalTok{ pred.lda}\OperatorTok{$}\NormalTok{class, }\DataTypeTok{obs =}\NormalTok{ dt.test}\OperatorTok{$}\NormalTok{Species)}
\NormalTok{tab}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##             obs
## pred         setosa versicolor virginica
##   setosa         18          0         0
##   versicolor      0         17         1
##   virginica       0          1        13
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sum}\NormalTok{(}\KeywordTok{diag}\NormalTok{(}\KeywordTok{prop.table}\NormalTok{(tab)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.96
\end{verbatim}

Jak wida z powy偶szej tabeli model dobrze sobie radzi z klasyfikacj obiekt贸w. Odsetek poprawnych klasyfikacji wynosi 96\%.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{cbind.data.frame}\NormalTok{(}\DataTypeTok{obs =}\NormalTok{ dt.test}\OperatorTok{$}\NormalTok{Species,}
\NormalTok{                 pred.lda}\OperatorTok{$}\NormalTok{x, }
                 \DataTypeTok{pred =}\NormalTok{ pred.lda}\OperatorTok{$}\NormalTok{class) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ LD1, }\DataTypeTok{y =}\NormalTok{ LD2))}\OperatorTok{+}
\StringTok{    }\KeywordTok{geom_point}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{color =}\NormalTok{ pred, }\DataTypeTok{shape =}\NormalTok{ obs), }\DataTypeTok{size =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{EksploracjaDanych_files/figure-latex/unnamed-chunk-59-1} 

}

\caption{Klasyfikacja w przestrzeni LD1, LD2 na podstawie modelu mod.lda}\label{fig:unnamed-chunk-59}
\end{figure}

\hypertarget{liniowa-analiza-dyskryminacyjna---podejux15bcie-probabilistyczne}{%
\section{Liniowa analiza dyskryminacyjna - podejcie probabilistyczne}\label{liniowa-analiza-dyskryminacyjna---podejux15bcie-probabilistyczne}}

Jak wspomniano na wstpie (patrz rozdzia \ref{LDA}), podejcie prezentowane przez Welcha polegao na minimalizacji prawdopodobiestwa popenienia bdu przy klasyfikacji. Caa rodzina klasyfikator贸w Bayesa (patrz rozdzia \ref{bayes}) polega na wyznaczeniu prawdopodobiestw \emph{a posteriori}, na podstawie kt贸rych dokonuje si decyzji o klasyfikacji obiekt贸w. Tym razem dodajemy r贸wnie偶 zao偶enie, 偶e zmienne niezale偶ne \(\boldsymbol{x}=(\boldsymbol{x}_1,\ldots,\boldsymbol{x}_d)\) charakteryzuj si wielowymiarowym rozkadem normalnym
\begin{equation}
    f(\boldsymbol{x}) = \frac{1}{(2\pi)^{d/2}|\boldsymbol{\Sigma}|^{1/2}}\exp\left[-\frac{1}{2}(\boldsymbol{x}-\boldsymbol{\mu})'\boldsymbol{\Sigma}(\boldsymbol{x}-\boldsymbol{\mu})\right],
    \label{eq:mnv}
\end{equation}
gdzie \(\boldsymbol{\mu}\) jest wektorem rednich \(\boldsymbol{x}\), a \(\boldsymbol{\Sigma}\) jest macierz kowariancji \(\boldsymbol{x}\).

\BeginKnitrBlock{remark}
\iffalse{} {Uwaga. } \fi{}Liniowa kombinacja zmiennych losowych o normalnym rozkadzie cznym ma r贸wnie偶 rozkad czny normalny. W szczeg贸lnoci, jeli \(A\) jest macierz wymiaru \(d\times k\) i \(\boldsymbol{y} = A'\boldsymbol{x}\), to \(f(\boldsymbol{y})\sim N(A'\boldsymbol{\mu}, A'\boldsymbol{\Sigma}A)\). Odpowiednia forma macierzy przeksztacenia \(A_w\), sprawia, 偶e zmienne po transformacji charakteryzuj si rozkadem normalnym cznym o wariancji okrelonej przez \(I\). Jeli \(\boldsymbol{\Phi}\) jest macierz, kt贸rej kolumny s ortonormalnymi wektorami wasnymi macierzy \(\boldsymbol{\Sigma}\), a \(\boldsymbol{\Lambda}\) macierz diagonaln wartoci wasnych, to transformacja \(A_w=\boldsymbol{\Phi}\boldsymbol{\Lambda}^{-1}\) przeksztaca \(\boldsymbol{x}\) w \(\boldsymbol{y}\sim N(A_w'\boldsymbol{\mu}, I)\).
\EndKnitrBlock{remark}

\begin{figure}

{\centering \includegraphics[width=4.52in]{images/transform} 

}

\caption{Transformacje rozkadu normalnego cznego. 殴r贸do: @duda2001}\label{fig:trans}
\end{figure}

\BeginKnitrBlock{definition}
\protect\hypertarget{def:unnamed-chunk-60}{}{\label{def:unnamed-chunk-60} }Niech \(g_i(\boldsymbol{x}),\ i=1,\ldots,k\) bd pewnymi funkcjami dyskryminacyjnymi. W贸wczas obiekt \(\boldsymbol{x}\) klasyfikujemy do grupy \(c_i\) jeli speniony jest warunek
\begin{equation}
    g_i(\boldsymbol{x})>g_j(\boldsymbol{x}), \quad j\neq i.
\end{equation}
\EndKnitrBlock{definition}

W podejciu polegajcym na minimalizacji prawdopodobiestwa bdnej klasyfikacji, przyjmuje si najczciej, 偶e
\begin{equation}
    g_i(\boldsymbol{x})=\P(c_i|\boldsymbol{x}),
\end{equation}
czyli jako prawdopodobiestwo a posteriori.
Wszystkie trzy poni偶sze postaci funkcji dyskryminacyjnych s dopuszczalne i r贸wnowa偶ne ze wzgldu na rezultat grupowania
\begin{align}
    g_i(\boldsymbol{x})=&\P(c_i|\boldsymbol{x})=\frac{\P(\boldsymbol{x}|c_i)\P(c_i)}{\sum_{i=1}^k\P(\boldsymbol{x}|c_i)\P(c_i)},\\
    g_i(\boldsymbol{x})=&\P(\boldsymbol{x}|c_i)\P(c_i),\\
    g_i(\boldsymbol{x})=&\log\P(\boldsymbol{x}|c_i)+\log\P(c_i)
    \label{eq:gi3}
\end{align}
W przypadku gdy \(\boldsymbol{x}|c_i\sim N(\boldsymbol{\mu}_i, \boldsymbol{\Sigma}_i)\), to na podstawie \eqref{eq:mnv} \(g_i\) danej jako \eqref{eq:gi3} przyjmuje posta
\begin{equation}
    g_i(\boldsymbol{x})=-\frac{1}{2}(\boldsymbol{x}-\boldsymbol{\mu}_i)'\boldsymbol{\Sigma}_i^{-1}(\boldsymbol{x}-\boldsymbol{\mu}_i)-\frac{d}{2}\log(2\pi)-\frac{1}{2}\log|\boldsymbol{\Sigma}_i|+\log\P(c_i).
\end{equation}

W kolejnych podrozdziaach przeanalizujemy trzy mo偶liwe formy macierzy kowariancji.

\hypertarget{przypI}{%
\subsection{\texorpdfstring{Przypadek gdy \(\boldsymbol{\Sigma}_i=\sigma^2I\)}{Przypadek gdy \textbackslash boldsymbol\{\textbackslash Sigma\}\_i=\textbackslash sigma\^{}2I}}\label{przypI}}

To najprostszy przypadek, zakadajcy niezale偶no zmiennych wchodzcych w skad \(\boldsymbol x\), kt贸rych wariancje s stae \(\sigma^2\).
W贸wczas \(g_i\) przyjmuje posta
\begin{equation}
    g_i(\boldsymbol x)=-\frac{||\boldsymbol x-\boldsymbol \mu_i||^2}{2\sigma^2}+\log\P(c_i),
    \label{eq:row88}
\end{equation}
gdzie \(||\cdot ||\) jest norm euklidesow.

Rozpisujc licznik r贸wnania \eqref{eq:row88} mamy
\begin{equation}
    ||\boldsymbol x-\boldsymbol \mu_i||^2=(\boldsymbol x-\boldsymbol \mu_i)'(\boldsymbol x-\boldsymbol \mu_i).
\end{equation}
Zatem
\begin{equation}
    g_i(\boldsymbol x)=-\frac{1}{2\sigma^2}[\boldsymbol x'\boldsymbol x-2\boldsymbol \mu_i'\boldsymbol x+\boldsymbol \mu_i'\boldsymbol \mu_i]+\log\P(c_i).
\end{equation}
A poniewa偶 \(\boldsymbol x'\boldsymbol x\) nie zale偶y do \(i\), to funkcj dyskryminacyjn mo偶emy zapisa jako
\begin{equation}
    g_i(\boldsymbol x)=\boldsymbol w_i'\boldsymbol x+w_{i0},
\end{equation}
gdzie \(\boldsymbol w_i=\frac{1}{\sigma^2}\boldsymbol \mu_i\), a \(w_{i0}=\frac{-1}{2\sigma^2}\boldsymbol \mu_i'\boldsymbol \mu_i+\log\P(c_i).\)

Na podstawie funkcji dyskryminacyjnych wyznaczamy hiperpaszczyzny decyzyjne jako zbiory punkt贸w dla kt贸rych \(g_i(\boldsymbol x)=g_j(\boldsymbol x)\), gdzie \(g_i, g_j\) s najwiksze. Mo偶emy to zapisa w nastpujcy spos贸b
\begin{equation}
    \boldsymbol w'(\boldsymbol x-\boldsymbol x_0)=0,
    \label{eq:row89}
\end{equation}
gdzie
\begin{equation}
    \boldsymbol w = \boldsymbol \mu_i-\boldsymbol \mu_j,
\end{equation}
oraz
\begin{equation}
    \boldsymbol x_0 = \frac12(\boldsymbol \mu_i+\boldsymbol\mu_j)-\frac{\sigma^2}{||\boldsymbol \mu_i-\boldsymbol \mu_j||^2}\log\frac{\P(c_i)}{\P(c_j)}(\boldsymbol \mu_i-\boldsymbol \mu_j).
\end{equation}

R贸wnanie \eqref{eq:row89} okrela hiperpaszczyzn przechodzc przez \(\boldsymbol x_0\) i prostopad do \(\boldsymbol w\).

\begin{figure}

{\centering \includegraphics[width=6.47in]{images/dyskrym1} 

}

\caption{Dyskrymiancja hiperpaszczyznami w sygucaji dw贸ch klas. Wykres po lewej, to ujcie jednowymiarowe, wykresy po rodu - ujcie 2-wymiarowe i wykresy po prawej, to ujcie 3-wymiarowe. 殴r贸do: @duda2001}\label{fig:hiper}
\end{figure}

\hypertarget{przypSig}{%
\subsection{\texorpdfstring{Przypadek gdy \(\boldsymbol \Sigma_i=\boldsymbol \Sigma\)}{Przypadek gdy \textbackslash boldsymbol \textbackslash Sigma\_i=\textbackslash boldsymbol \textbackslash Sigma}}\label{przypSig}}

Przypadek ten opisuje sytuacj, gdy rozkady \(\boldsymbol x\) s identyczne we wszystkich grupach ale zmienne w ich skad wchodzce nie s niezale偶ne.
W tym przypadku funkcje dyskryminacyjne przyjmuj posta
\begin{equation}
    g_i(\boldsymbol x)=\frac12(\boldsymbol x-\boldsymbol \mu_i)'\boldsymbol\Sigma^{-1}(\boldsymbol x-\boldsymbol \mu_i)+\log\P(c_i).
    \label{eq:row810}
\end{equation}
Jeli \(\P(c_i)\) s identyczne dla wszystkich klas, to mo偶na je pomin we wzorze \eqref{eq:row810}. Metryka euklidesowa ze wzoru \eqref{eq:row88} zostaa zastpiona przez odlego Mahalonobis'a. Podobnie ja w przypadku gdy \(\boldsymbol \Sigma_i=\sigma^2I\), tak i teraz mo偶na uproci \eqref{eq:row810} przez rozpisanie formy kwadratowej, aby otrzyma, 偶e
\begin{equation}
    g_i(\boldsymbol x)=\boldsymbol w_i'\boldsymbol x+w_{i0},
\end{equation}
gdzie \(\boldsymbol w_i=\boldsymbol\Sigma^{-1}\boldsymbol \mu_i\), a \(w_{i0}=-\frac{1}{2}\boldsymbol \mu_i'\boldsymbol\Sigma^{-1}\boldsymbol \mu_i+\log\P(c_i).\)

Poniewa偶 funkcje dyskryminacyjne s liniowe, to hiperpaszczyzny s ograniczeniami obszar贸w obserwacji ka偶dej z klas
\begin{equation}
    \boldsymbol w'(\boldsymbol x-\boldsymbol x_0)=0,
    \label{eq:row812}
\end{equation}
gdzie
\begin{equation}
    \boldsymbol w = \boldsymbol\Sigma^{-1} (\boldsymbol \mu_i-\boldsymbol \mu_j),
\end{equation}
oraz
\begin{equation}
    \boldsymbol x_0 = \frac12(\boldsymbol \mu_i+\boldsymbol\mu_j)-\frac{\log[ \P(c_i)/\P(c_j)]}{(\boldsymbol x-\boldsymbol \mu_i)'\boldsymbol\Sigma^{-1}(\boldsymbol x-\boldsymbol \mu_i)}(\boldsymbol \mu_i-\boldsymbol \mu_j).
\end{equation}
Tym razem \(\boldsymbol{w}=\Sigma^{-1}(\boldsymbol \mu_i-\boldsymbol \mu_j)\) nie jest wektorem w kierunku \(\boldsymbol \mu_i-\boldsymbol \mu_j\), wic hiperpaszczyzna rozdzielajca obiekty r贸偶nych klas nie jest prostopada do wektora \(\boldsymbol \mu_i-\boldsymbol \mu_j\) ale przecina go w poowie (w punkcie \(\boldsymbol x_0\)).

\begin{figure}

{\centering \includegraphics[width=5.45in]{images/dyskrym2} 

}

\caption{Hiperpaszczyzna rozdzielajca obszary innych klas mo偶e by przesunita w kierunku bardziej prawdopodobnej klasy, jeli prawdopodobiestwa a priori s r贸偶ne. 殴r贸do: @duda2001}\label{fig:hiper2}
\end{figure}

\hypertarget{przypadek-gdy-boldsymbol-sigma_i-jest-dowolnej-postaci}{%
\subsection{\texorpdfstring{Przypadek gdy \(\boldsymbol \Sigma_i\) jest dowolnej postaci}{Przypadek gdy \textbackslash boldsymbol \textbackslash Sigma\_i jest dowolnej postaci}}\label{przypadek-gdy-boldsymbol-sigma_i-jest-dowolnej-postaci}}

Jest to najbardziej og贸lny przypadek, kiedy nie nakada si 偶adnych ogranicze na macierze kowariancji grupowych. Posta funkcji dyskryminacyjnych jest nastpujca
\begin{equation}
    g_i(\boldsymbol x)=\boldsymbol x'\boldsymbol W_i\boldsymbol x+\boldsymbol w_i'\boldsymbol x+w_{i0}
    \label{eq:row813}
\end{equation}
gdzie
\begin{align}
    \boldsymbol W_i = &-\frac12 \boldsymbol\Sigma_i^{-1},\\
    \boldsymbol w_i=& \boldsymbol\Sigma_i^{-1}\boldsymbol\mu_i,\\
    w_{i0} = &-\frac12\boldsymbol\mu_i'\boldsymbol\Sigma_i^{-1}\boldsymbol\mu_i-\frac12\log|\boldsymbol\Sigma_i|+\log\P(c_i).
\end{align}

Ograniczenia w ten spos贸b budowane s hiperpowierzchniami (nie koniecznie hiperpaszczyznami). W literaturze ta metoda znana jest pod nazw kwadratowej analizy dyskryminacyjnej (ang. \emph{Quadratic Discriminant Analysis}).

\begin{figure}

{\centering \includegraphics[width=6.35in]{images/dyskrym3} 

}

\caption{Przykad zastosowania kwadratowej analizy dyskryminacyjnej. Pokazane s dopuszczalne postaci zbior贸w ograniczajcych. 殴r贸do: @duda2001}\label{fig:hiper3}
\end{figure}

\BeginKnitrBlock{example}
\protect\hypertarget{exm:caravan}{}{\label{exm:caravan} }Przeprowadzimy klasyfikacj na podstawie zbioru \texttt{Smarket} pakietu \texttt{ILSR}. Dane zawieraj kursy indeksu giedowego S\&P500 w latach 2001-2005. Na podstawie wartoci waloru z poprzednich 2 dni bdziemy chcieli przewidzie czy ruch w kolejnym okresie czasu bdzie w g贸r czy w d贸.
\EndKnitrBlock{example}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(ISLR)}
\KeywordTok{head}\NormalTok{(Smarket)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   Year   Lag1   Lag2   Lag3   Lag4   Lag5 Volume  Today Direction
## 1 2001  0.381 -0.192 -2.624 -1.055  5.010 1.1913  0.959        Up
## 2 2001  0.959  0.381 -0.192 -2.624 -1.055 1.2965  1.032        Up
## 3 2001  1.032  0.959  0.381 -0.192 -2.624 1.4112 -0.623      Down
## 4 2001 -0.623  1.032  0.959  0.381 -0.192 1.2760  0.614        Up
## 5 2001  0.614 -0.623  1.032  0.959  0.381 1.2057  0.213        Up
## 6 2001  0.213  0.614 -0.623  1.032  0.959 1.3491  1.392        Up
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{44}\NormalTok{)}
\NormalTok{dt.ucz <-}\StringTok{ }\NormalTok{Smarket }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{mutate_if}\NormalTok{(is.numeric, scale) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{sample_frac}\NormalTok{(}\DataTypeTok{size =} \DecValTok{2}\OperatorTok{/}\DecValTok{3}\NormalTok{) }
\NormalTok{dt.test <-}\StringTok{ }\NormalTok{Smarket[}\OperatorTok{-}\KeywordTok{as.numeric}\NormalTok{(}\KeywordTok{rownames}\NormalTok{(dt.ucz)),]}
\NormalTok{mod.qda <-}\StringTok{ }\KeywordTok{qda}\NormalTok{(Direction}\OperatorTok{~}\NormalTok{Lag1}\OperatorTok{+}\NormalTok{Lag2, }\DataTypeTok{data =}\NormalTok{ dt.ucz)}
\NormalTok{mod.qda}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Call:
## qda(Direction ~ Lag1 + Lag2, data = dt.ucz)
## 
## Prior probabilities of groups:
##      Down        Up 
## 0.4909964 0.5090036 
## 
## Group means:
##            Lag1        Lag2
## Down  0.0328367  0.06722714
## Up   -0.0671615 -0.08814914
\end{verbatim}

Poniewa偶 funkcje dyskryminacyjne mog by nieliniowe, to podsumowanie modelu nie zawiera wsp贸czynnik贸w funkcji. Podsumowanie zawiera tylko prawdopodobiestwa a priori i rednie poszczeg贸lnych zmiennych niezale偶nych w klasach.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pred.qda <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(mod.qda, dt.test)}
\NormalTok{tab <-}\StringTok{ }\KeywordTok{table}\NormalTok{(}\DataTypeTok{pred =}\NormalTok{ pred.qda}\OperatorTok{$}\NormalTok{class, dt.test}\OperatorTok{$}\NormalTok{Direction)}
\NormalTok{tab}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##       
## pred   Down  Up
##   Down   42  34
##   Up    139 202
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sum}\NormalTok{(}\KeywordTok{diag}\NormalTok{(}\KeywordTok{prop.table}\NormalTok{(tab)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.5851319
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(klaR)}
\KeywordTok{partimat}\NormalTok{(Direction }\OperatorTok{~}\StringTok{ }\NormalTok{Lag1}\OperatorTok{+}\NormalTok{Lag2, }
         \DataTypeTok{data =}\NormalTok{ dt.ucz,}
         \DataTypeTok{method =} \StringTok{"qda"}\NormalTok{,}
         \DataTypeTok{col.correct=}\StringTok{'blue'}\NormalTok{,}
         \DataTypeTok{col.wrong=}\StringTok{'red'}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{EksploracjaDanych_files/figure-latex/qda-1} 

}

\caption{Wykres klasyfikacji na podstawie QDA. Obserwacje zaznczone kolorem niebieskim s prawidowo zaklasyfikowane, a czerwonym 藕le}\label{fig:qda}
\end{figure}

\hypertarget{analiza-dyskryminacyjna-metodux105-czux119ux15bciowych-najmniejszych-kwadratuxf3w}{%
\section{Analiza dyskryminacyjna metod czciowych najmniejszych kwadrat贸w}\label{analiza-dyskryminacyjna-metodux105-czux119ux15bciowych-najmniejszych-kwadratuxf3w}}

Analiza dyskryminacyjna metod czciowych najmniejszych kwadrat贸w (ang. \emph{Partial Least Squares Discriminant Analysis}) jest wykorzystywana szczeg贸lnie w sytuacjach gdy zestaw predyktor贸w zwiera zmienne silnie ze sob skorelowane. Jak wiadomo z wczeniejszych rozwa偶a, metody dyskryminacji obserwacji s mao odporne na nadmiarowo zmiennych niezale偶nych. Std powsta pomys zastosowania poczenia LDA z PLS (Partial Least Squares), kt贸rej celem jest redukcja wymiaru przestrzeni jednoczenie maksymalizujc korelacj zmiennych niezale偶nych ze zmienn wynikow.

Parametrem, kt贸ry jest kontrolowany podczas budowy modelu jest liczba ukrytych zmiennych. Metoda PLSDA ma kilka implementacji w R, ale najbardziej znana jest funkcja \texttt{plsda} z pakietu \texttt{caret} (Jed Wing et al. \protect\hyperlink{ref-kuhn}{2018}).

\BeginKnitrBlock{example}
\protect\hypertarget{exm:plsda}{}{\label{exm:plsda} }Kontynujc poprzedni przykad przeprowadzimy klasyfikacje ruchu waloru korzystajc z metody PLSDA. W przeciwiestwie do poprzednich funkcji \texttt{plsda} potrzebuje przekazania zbioru predyktor贸w i wektora zmiennej wynikowej oddzielnie, a nie za pomoc formuy. Doboru liczby zmiennych latentnych dokonamy arbitralnie.
\EndKnitrBlock{example}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(caret)}
\NormalTok{mod.plsda <-}\StringTok{ }\KeywordTok{plsda}\NormalTok{(dt.ucz[,}\OperatorTok{-}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{7}\OperatorTok{:}\DecValTok{9}\NormalTok{)],}
                   \KeywordTok{as.factor}\NormalTok{(dt.ucz}\OperatorTok{$}\NormalTok{Direction), }
                   \DataTypeTok{ncomp =} \DecValTok{2}\NormalTok{)}
\NormalTok{mod.plsda}\OperatorTok{$}\NormalTok{loadings}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Loadings:
##      Comp 1 Comp 2
## Lag1 -0.548  0.375
## Lag2 -0.805 -0.204
## Lag3        -0.520
## Lag4 -0.143  0.652
## Lag5  0.186  0.351
## 
##                Comp 1 Comp 2
## SS loadings     1.004  1.001
## Proportion Var  0.201  0.200
## Cumulative Var  0.201  0.401
\end{verbatim}

Dwie ukryte zmienne u偶yte do redukcji wymiaru przestrzeni wyjaniaj okoo 40\% zmiennoci pierwotnych zmiennych. adunki (\texttt{Loadings}) pokazuj kontrybucje poszczeg贸lnych zmiennych w tworzenie si zmiennych ukrytych.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pred.plsda <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(mod.plsda, dt.test[,}\OperatorTok{-}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{7}\OperatorTok{:}\DecValTok{9}\NormalTok{)])}
\NormalTok{tab <-}\StringTok{ }\KeywordTok{table}\NormalTok{(pred.plsda, dt.test}\OperatorTok{$}\NormalTok{Direction)}
\NormalTok{tab}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##           
## pred.plsda Down  Up
##       Down   87  97
##       Up     94 139
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sum}\NormalTok{(}\KeywordTok{diag}\NormalTok{(}\KeywordTok{prop.table}\NormalTok{(tab)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.5419664
\end{verbatim}

Poniewa偶 korelacje pomidzy predyktorami w naszym przypadku nie byy du偶e, to zastosowanie PLSDA nie poprawio klasyfikacji w stosunku do metody QDA.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{cor}\NormalTok{(dt.ucz[,}\DecValTok{2}\OperatorTok{:}\DecValTok{6}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##              Lag1         Lag2         Lag3        Lag4        Lag5
## Lag1  1.000000000 -0.003318026 -0.004329303  0.02574559  0.01831679
## Lag2 -0.003318026  1.000000000 -0.057166931  0.01209305  0.02127975
## Lag3 -0.004329303 -0.057166931  1.000000000  0.01574896 -0.07541592
## Lag4  0.025745592  0.012093049  0.015748958  1.00000000 -0.04607207
## Lag5  0.018316786  0.021279754 -0.075415921 -0.04607207  1.00000000
\end{verbatim}

\hypertarget{regularyzowana-analiza-dyskryminacyjna}{%
\section{Regularyzowana analiza dyskryminacyjna}\label{regularyzowana-analiza-dyskryminacyjna}}

Regularyzowana analiza dyskryminacyjna (ang. \emph{Regularized Discriminant Analysis}) powstaa jako technika r贸wnowa偶ca zalety i wady LDA i QDA. Ze wzgldu na zdolnoci generalizacyjne model LDA jest lepszy od QDA (mniejsza wariancja modelu), ale jednoczenie QDA ma bardziej elastyczn posta hiperpowierzchni brzegowych rozdzielajcych obiekty r贸偶nych klas. Dlatego Friedman (\protect\hyperlink{ref-friedman1989}{1989}) wprowadzi technik bdc kompromisem pomidzy LDA i QDA poprzez odpowiednie okrelenie macierzy kowariancji
\begin{equation}
    \tilde{\boldsymbol \Sigma}_i(\lambda) = \lambda\boldsymbol\Sigma_i + (1-\lambda)\boldsymbol\Sigma,
\end{equation}
gdzie \(\boldsymbol \Sigma_i\) jest macierz kowariancji dla \(i\)-tej klasy, a \(\boldsymbol \Sigma\) jest urednion macierz kowariancji wszystkich klas. Zatem odpowiedni dob贸r parametru \(\lambda\) decyduje czy poszukujemy modelu prostszego (\(\lambda = 0\) odpowiada LDA), czy bardziej elastycznego (\(\lambda=1\) oznacza QDA).

Dodatkowo metoda RDA pozwala na elastyczny wyb贸r pomidzy postaciami macierzy kowariancji wsp贸lnej dla wszystkich klas \(\boldsymbol\Sigma\). Mo偶e ona by macierz jednostkow, jak w przypadku \ref{przypI}, co oznacza niezale偶no predyktor贸w modelu, mo偶e te偶 by jak w przypadku \ref{przypSig}, gdzie dopuszcza si korelacje midzy predyktorami. Dokonuje si tego przez odpowiedni dob贸r parametru \(\gamma\)
\begin{equation}
    \boldsymbol \Sigma(\gamma) = \gamma\boldsymbol \Sigma+(1-\gamma)\sigma^2I.
\end{equation}

\BeginKnitrBlock{example}
\protect\hypertarget{exm:rda}{}{\label{exm:rda} }Funkcja \texttt{rda} pakietu \texttt{klaR} jest implementacj powy偶szej metody. Ilustraj jej dziaania bdzie klasyfikacja stan贸w z poprzedniego przykadu.
\EndKnitrBlock{example}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(klaR)}
\NormalTok{mod.rda <-}\StringTok{ }\KeywordTok{rda}\NormalTok{(Direction}\OperatorTok{~}\NormalTok{Lag1}\OperatorTok{+}\NormalTok{Lag2}\OperatorTok{+}\NormalTok{Lag3}\OperatorTok{+}\NormalTok{Lag4}\OperatorTok{+}\NormalTok{Lag5, dt.ucz)}
\NormalTok{mod.rda}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Call: 
## rda(formula = Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5, data = dt.ucz)
## 
## Regularization parameters: 
##       gamma      lambda 
## 0.713621809 0.004013224 
## 
## Prior probabilities of groups: 
##      Down        Up 
## 0.4909964 0.5090036 
## 
## Misclassification rate: 
##        apparent: 44.058 %
## cross-validated: 46.482 %
\end{verbatim}

Model zosta oszacowany z parametrami wyznaczonymi na podstawie sprawdzianu krzy偶owego zastosowanego w funkcji \texttt{rda}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pred.rda <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(mod.rda, dt.test)}
\NormalTok{(tab <-}\StringTok{ }\KeywordTok{table}\NormalTok{(}\DataTypeTok{pred =}\NormalTok{ pred.rda}\OperatorTok{$}\NormalTok{class, dt.test}\OperatorTok{$}\NormalTok{Direction))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##       
## pred   Down  Up
##   Down   24  31
##   Up    157 205
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sum}\NormalTok{(}\KeywordTok{diag}\NormalTok{(}\KeywordTok{prop.table}\NormalTok{(tab)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.5491607
\end{verbatim}

Jako klasyfikacji jest na zbli偶onym poziomie jak przy poprzednich metodach.

\hypertarget{analiza-dyskryminacyjna-mieszana}{%
\section{Analiza dyskryminacyjna mieszana}\label{analiza-dyskryminacyjna-mieszana}}

Liniowa analiza dyskryminacyjna zakadaa, 偶e rednie (centroidy) w klasach s r贸偶ne ale macierz kowariancji wszystkich klas jest jednakowa. Analiza dyskryminacyjna mieszana (ang. \emph{Mixture Discriminant Analysis}) prezentuje jeszcze inne podejcie poniewa偶 zakada, 偶e ka偶da klasa mo偶e by charakteryzowana przez wiele wielowymiarowych rozkad贸w normalnych, kt贸rych centroidy mog si r贸偶nic, ale macierze kowariancji nie.

W贸wczas rozkad dla danej klasy jest mieszanin rozkad贸w skadowych, a funkcja dyskryminacyjna dla \(i\)-tej klasy przyjmuje posta
\begin{equation}
    g_i(\boldsymbol x)\propto \sum_{k=1}^{L_i}\phi_{ik}g_{ik}(\boldsymbol x),
\end{equation}
gdzie \(L_i\) jest liczb rozkad贸w skadajcych si na \(i\)-t klas, a \(\phi_{ik}\) jest wsp贸czynnikiem proporcji estymowanych w czasie uczenia modelu.

\BeginKnitrBlock{example}
\protect\hypertarget{exm:mda}{}{\label{exm:mda} }Funkcja \texttt{mda} pakietu \texttt{mda} (Trevor Hastie et al. \protect\hyperlink{ref-R-mda}{2017}) jest implementacj tej techniki w R. Jej zastosowanie poka偶emy na przykadzie danych giedowych z poprzedniego przykadu. U偶yjemy domylnych ustawie funkcji (trzy rozkady dla ka偶dej klasy).
\EndKnitrBlock{example}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(mda)}
\NormalTok{mod.mda <-}\StringTok{ }\KeywordTok{mda}\NormalTok{(Direction}\OperatorTok{~}\NormalTok{Lag1}\OperatorTok{+}\NormalTok{Lag2}\OperatorTok{+}\NormalTok{Lag3}\OperatorTok{+}\NormalTok{Lag4}\OperatorTok{+}\NormalTok{Lag5, dt.ucz)}
\NormalTok{mod.mda}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Call:
## mda(formula = Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5, data = dt.ucz)
## 
## Dimension: 5 
## 
## Percent Between-Group Variance Explained:
##     v1     v2     v3     v4     v5 
##  63.78  95.81  98.97  99.84 100.00 
## 
## Degrees of Freedom (per dimension): 6 
## 
## Training Misclassification Error: 0.43337 ( N = 833 )
## 
## Deviance: 1134.288
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pred.mda <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(mod.mda, dt.test)}
\NormalTok{(tab <-}\StringTok{ }\KeywordTok{table}\NormalTok{(}\DataTypeTok{pred =}\NormalTok{ pred.mda, dt.test}\OperatorTok{$}\NormalTok{Direction))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##       
## pred   Down  Up
##   Down   38  46
##   Up    143 190
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sum}\NormalTok{(}\KeywordTok{diag}\NormalTok{(}\KeywordTok{prop.table}\NormalTok{(tab)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.5467626
\end{verbatim}

Kolejny raz model dyskryminacyjny charakteryzuje si podobn jakoci klasyfikacji.

\hypertarget{elastyczna-analiza-dyskryminacyjna}{%
\section{Elastyczna analiza dyskryminacyjna}\label{elastyczna-analiza-dyskryminacyjna}}

Zupenie inne podejcie w stosunku do wczeniejszych rozwiza, prezentuje elastyczna analiza dyskryminacyjna (ang. \emph{Flexible Discriminant Analysis}) . Kodujc klasy wynikowe jako zmienne dychotomiczne (dla ka偶dej klasy jest odrbna zmienna wynikowa) dla ka偶dej z nich budowanych jest \(k\) modeli regresji. Mog to by modele regresji penalizowanej, jak regresja grzbietowa lub LASSO, modele regresji wielomianowej albo modele regresji sklejanej (MARS), o kt贸rych bdzie mowa w dalszej czci tego opracowania.

Przykadowo, jeli modelem bazowym jest MARS, to funkcja dyskryminacyjna \(i\)-tej klasy mo偶e by postaci
\begin{equation}
    g_i(\boldsymbol x)=\beta_0+\beta_1h(1-x_1)+\beta_2h(x_2-1)+\beta_3h(1-x_3)+\beta_4h(x_1-1),
\end{equation}
gdzie \(h\) s tzw. funkcjami bazowymi postaci
\begin{equation}
    h(x)= \begin{cases}
        x, & x> 0\\
        0, & x\leq 0.
    \end{cases}
\end{equation}
Klasyfikacji dokonujemy sprawdzajc znak funkcji dyskryminacyjnej \(g_i\), jeli jest dodatni, to funkcja przypisuje obiekt do klasy \(i\)-tej. W przeciwnym przypadku nie nale偶y do tej klasy.

\begin{figure}

{\centering \includegraphics[width=3.09in]{images/fda} 

}

\caption{Przykad klasyfikacji dwustanowej za pomoc metody FDA}\label{fig:fda}
\end{figure}

\BeginKnitrBlock{example}
\protect\hypertarget{exm:przykFDA}{}{\label{exm:przykFDA} }Funkcja \texttt{fda} pakietu \texttt{mda} jest implementacj techniki FDA w R. Na postawie danych z poprzedniego przykadu zostanie przedstawiona zasada dzieania. Przyjmiemy domylne ustawienia funkcji, z wyjtkiem metody estymacji modelu, jako kt贸r przyjmiemy MARS.
\EndKnitrBlock{example}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mod.fda <-}\StringTok{ }\KeywordTok{fda}\NormalTok{(Direction }\OperatorTok{~}\StringTok{ }\NormalTok{Lag1}\OperatorTok{+}\NormalTok{Lag2, dt.ucz, }\DataTypeTok{method =}\NormalTok{ mars)}
\NormalTok{mod.fda}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Call:
## fda(formula = Direction ~ Lag1 + Lag2, data = dt.ucz, method = mars)
## 
## Dimension: 1 
## 
## Percent Between-Group Variance Explained:
##  v1 
## 100 
## 
## Training Misclassification Error: 0.44418 ( N = 833 )
\end{verbatim}

Poniewa偶, zmienna wynikowa jest dwustanowa, to powstaa tylko jedna funkcja dyskryminacyjna.
Parametry modelu s nastpujce

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mod.fda}\OperatorTok{$}\NormalTok{fit}\OperatorTok{$}\NormalTok{coefficients}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##             [,1]
## [1,]  0.05846465
## [2,] -0.17936208
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pred.fda <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(mod.fda, dt.test)}
\NormalTok{(tab <-}\StringTok{ }\KeywordTok{table}\NormalTok{(}\DataTypeTok{pred =}\NormalTok{ pred.fda, dt.test}\OperatorTok{$}\NormalTok{Direction))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##       
## pred   Down  Up
##   Down   40  50
##   Up    141 186
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sum}\NormalTok{(}\KeywordTok{diag}\NormalTok{(}\KeywordTok{prop.table}\NormalTok{(tab)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.5419664
\end{verbatim}

Jako klasyfikacji jest tylko nieco lepsza ni偶 w przypadku poprzednich metod.

\hypertarget{bayes}{%
\chapter{Klasyfikatory bayesowskie}\label{bayes}}

Ca gam klasyfikator贸w opartych na twierdzeniu Bayesa nazywa bdziemy bayesowskimi.
\begin{equation}\label{bayes}
        P(A|B)=\frac{P(A)P(B|A)}{P(B)},
\end{equation}
gdzie \(P(B)>0\).

Bayesowskie reguy podejmowania decyzji day podstawy takich metod jak:

\begin{itemize}
\tightlist
\item
  liniowa analiza dyskryminacyjna;
\item
  kwadratowa analiza dyskryminacyjna;
\end{itemize}

W ustaleniu klasyfikatora bayesowskiego bdzie nam przywiecaa cay czas ta sama regua: \emph{jeli znam wartoci cech charakteryzujcych badane obiekty oraz klasy do kt贸rych nale偶 (w pr贸bie uczcej), to na ich podstawie mog wyznaczy miary prawdopodobiestw a posteriori, kt贸re pomog mi w ustaleniu klasy do kt贸rej nale偶y nowy testowy element.}

W dalszej czci bdziemy przyjmowali nastpujce oznaczenia:

\begin{itemize}
\tightlist
\item
  \(T\) - zbi贸r danych uczcych (treningowych),
\item
  \(T^j\) - zbi贸r danych uczcych dla kt贸rych przyjlimy decyzj o przynale偶noci do \(j\)-tej klasy,
\item
  \(T^j_{a_i=v}\) - zbi贸r danych uczcych o wartoci atrybutu \(a_i\) r贸wnej \(v\) i klasy \(j\)-tej,
\item
  \(\mathbb{H}\) - przestrze hipotez,
\item
  \(P(h|a_1=v_1, a_2=v_2,\ldots,a_p=v_p)\) - prawdopodobiestwo a posteriori, 偶e prawdziwa jest hipoteza \(h\in \mathbb{H}\), jeli znamy atrybuty obiektu,
\item
  \(P(h)\) - prawdopodobiestwo a priori zajcia hipotezy \(h\in \mathbb{H}\),
\item
  \(c\) - prawdziwy stan obiektu.
\end{itemize}

\hypertarget{klasyfikator-maximum-a-posteriori-map}{%
\section{Klasyfikator maximum a posteriori (MAP)}\label{klasyfikator-maximum-a-posteriori-map}}

Na podstawie wiedzy o atrybutach obiektu \(x\) podejmujemy decyzj o klasyfikacji tego obiektu zgodnie z hipotez \(h_{MAP}\in \mathbb{H}\), kt贸ra przyjmuje posta
\begin{align}\label{MAP}
        h_{MAP}=&\operatorname{arg}\max_{h\in \mathbb{H}}P(h|a_1=v_1, a_2=v_2,\ldots,a_p=v_p)\\
            =& \operatorname{arg}\max_{h\in \mathbb{H}}P(a_1=v_1, a_2=v_2,\ldots,a_p=v_p|h)\cdot P(h),
\end{align}
gdzie ostatnia r贸wno wynika z twierdzenia Bayesa oraz faktu, 偶e dla konkretnego obiektu \(x\) wielkoci atrybut贸w nie zale偶 od postawionej hipotezy.

\hypertarget{klasyfikator-najwiux119kszej-wiarogodnoux15bci-ml}{%
\section{Klasyfikator najwikszej wiarogodnoci (ML)}\label{klasyfikator-najwiux119kszej-wiarogodnoux15bci-ml}}

Na podstawie wiedzy o atrybutach obiektu \(x\) podejmujemy decyzj o klasyfikacji tego obiektu zgodnie z hipotez \(h_{ML}\in \mathbb{H}\), kt贸ra przyjmuje posta
\begin{equation}\label{ML}
        h_{ML}=\operatorname{arg}\max_{h\in \mathbb{H}}P(a_1=v_1, a_2=v_2,\ldots,a_p=v_p|h).
\end{equation}

\BeginKnitrBlock{remark}
\iffalse{} {Uwaga. } \fi{}Obie wspomniane metody wymagaj znajomoci prawdopodobiestwa \(P(a_1=v_1,a_2=v_2,\ldots,a_p=v_p|h)\), ale r贸偶ni si podejciem do wiedzy o prawdopodobiestwach a priori. W metodzie MAP brana pod uwag jest wiedza o prawdopodobiestwie przynale偶noci do poszczeg贸lnych klas, a w ML nie. Dla klasyfikacji, w kt贸rych prawdopodobiestwa przynale偶noci do klas s takie same, klasyfikatory MAP i ML s r贸wnowa偶ne.
\EndKnitrBlock{remark}

\hypertarget{naiwny-klasyfikator-bayesa-nb}{%
\section[Naiwny klasyfikator Bayesa (NB)]{\texorpdfstring{Naiwny klasyfikator Bayesa (NB)\footnote{ang. \emph{Naive Bayes Classifier}}}{Naiwny klasyfikator Bayesa (NB)}}\label{naiwny-klasyfikator-bayesa-nb}}

Najwikszy problem w wyznaczeniu klasyfikator贸w MAP i ML stanowi wyznaczenie rozkadu cznego \(P(a_1=v_1, a_2=v_2,\ldots,a_p=v_p|h)\). W naiwnym klasyfikatorze Bayesa zakada si niezale偶no warunkow poszczeg贸lnych atrybut贸w wzgldem klasy do kt贸rej ma nale偶e wg hipotezy obiekt. Zao偶enie to czsto nie jest spenione i std nazwa przymiotnik ``naiwny''.

Definicja naiwnego klasyfikatora bayesowskiego r贸偶ni si od klasyfikatora MAP tylko podejciem do prawdopodobiestwa a posteriori.
\begin{equation}\label{naiwny_bayes}
        h_{NB}=\operatorname{arg}\max_{h_j\in \mathbb{H}}P(h_j)\prod_{i=1}^{p}P(a_i=v_i|h_j),
\end{equation}
gdzie \(h_j\) oznacza hipotez (decyzj), 偶e badany obiekt nale偶y do \(j\)-tej klasy.

Oczywicie zar贸wno prawdopodobiestwo a priori jak i a posteriori s wyznaczane na podstawie pr贸by, i tak prawdopodobiestwo a priori wynosi
\begin{equation}\label{apriori}
        P(h_j)=P_T(h_j)=\frac{|T^j|}{|T|}, 
\end{equation}
gdzie \(|A|\) oznacza moc zbioru \(A\).

Natomiast prawdopodobiestwo a posteriori dla \(i\)-tego atrybutu wynosi
\begin{equation}\label{aposteriori}
        P(a_i=v_i|h_j)=P_{T^j}(a_i=v_i)=\frac{|T^j_{a_i=v_i}|}{|T^j|}.
\end{equation}
Na mocy powy偶szego mo偶emy zauwa偶y, 偶e je偶eli zao偶enie o warunkowej niezale偶noci jest spenione, to klasyfikatory NB i MAP s r贸wnowa偶ne.

Chcc przypisa klas nowemu obiektowi powstaje problem praktyczny, polegajcy na tym, 偶e dla pewnych konfiguracji atrybut贸w nie ma odpowiednik贸w w nauczonym modelu. Powodem takiego stanu rzeczy jest fakt, 偶e takie kombinacje nie wystpiy w pr贸bie uczcej.

Istniej dwa sposoby predykcji w takiej sytuacji:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \begin{equation}\label{pred1}
           P(a_i=v_i|h_j)=
           \begin{cases}
               \frac{|T^j_{a_i=v_i}|}{|T^j|}, & T^j_{a_i=v_i}\neq \emptyset\\
               \epsilon, & \text{w przeciwnym przypadku.}
           \end{cases}
   \end{equation}
  W tym przypadku przyjmuje si, 偶e \(\epsilon \ll 1/|T_j|\).
\item
  Drugi spos贸b wykorzystuje estymacj z poprawk
  \begin{equation}\label{pred2}
       P(a_i=v_i|h_j)=\frac{|T^j_{a_i=v_i}|+mp}{|T^j|+mp},
  \end{equation}
  gdzie \(p\) oznacza prawdopodobiestwo a priori przyjcia przez atrybut \(a\) wartoci \(v\) (najczciej \(p=1/|A|\), \(A\) - zbi贸r wszystkich mo偶liwych wartoci atrybutu \(a\)), \(m\) - waga (najczciej \(m=|A|\)).
\end{enumerate}

W przypadku gdy atrybuty s mierzone na skali cigej najczciej stosuje si dyskretyzacj ich do zmiennych ze skali przedziaowej. Inna metoda stosowana w przypadku cigych atrybut贸w, to u偶ycie gstoci \(g_i^j\) o rozkadzie normalnym w miejsce \(P(a_i=v_i|h_j)\). Przy czym do obliczenia parametr贸w rozkadu stosujemy wzory
\begin{equation}\label{sred}
        m_i^j=\frac{1}{|T^j|}\sum_{x\in T^j}a_i(x),
\end{equation}
oraz
\begin{equation}\label{odch}
        (s_i^j)^2=\frac{1}{|T^j|-1}\sum_{x\in T^j}(a_i(x)-m_i^j)^2.
\end{equation}

Obsuga brak贸w danych przez naiwny klasyfikator Bayesa jest do prosta i opiera si na liczeniu prawdopodobiestw a posteriori wycznie dla obiekt贸w, kt贸rych wartoci atrybut贸w s znane. Dlatego prawdopodobiestwa warunkowe liczy si wg wzoru
\begin{equation}\label{pr_war}
        P(a_i=v_i|h_j)=\frac{|T^j_{a_i=v_i}|}{|T^j|-|T^j_{a_i=NA}|}.
\end{equation}
Jeli brakujce dane nie nios w sobie istotnych informacji dotyczcych klasyfikacji obiekt贸w, to naiwny klasyfikator Bayesa bdzie dziaa poprawnie.

Naiwny klasyfikator Bayesa jest implementowany w pakietach \textbf{e1071} (Meyer et al. \protect\hyperlink{ref-R-e1071}{2019}) i \textbf{klaR} (Weihs et al. \protect\hyperlink{ref-R-klaR}{2005}).

\BeginKnitrBlock{example}
\protect\hypertarget{exm:unnamed-chunk-74}{}{\label{exm:unnamed-chunk-74} }Przeprowadzimy klasyfikacj dla zbioru \texttt{Titanic}. W przypadku funkcji z pakietu \texttt{e1071} nie potrzeba zamienia tabeli na przypadki. W pakiecie \texttt{klaR} istnieje inna funkcja budujca klasyfikator Bayesa \texttt{NaiveBayes}, ale w tym przypadku jeli zbi贸r jest w formie tabeli, to nale偶y go zamieni na ramk danych z oddzielnymi przypadkami.
\EndKnitrBlock{example}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(e1071)}
\NormalTok{Titanic}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## , , Age = Child, Survived = No
## 
##       Sex
## Class  Male Female
##   1st     0      0
##   2nd     0      0
##   3rd    35     17
##   Crew    0      0
## 
## , , Age = Adult, Survived = No
## 
##       Sex
## Class  Male Female
##   1st   118      4
##   2nd   154     13
##   3rd   387     89
##   Crew  670      3
## 
## , , Age = Child, Survived = Yes
## 
##       Sex
## Class  Male Female
##   1st     5      1
##   2nd    11     13
##   3rd    13     14
##   Crew    0      0
## 
## , , Age = Adult, Survived = Yes
## 
##       Sex
## Class  Male Female
##   1st    57    140
##   2nd    14     80
##   3rd    75     76
##   Crew  192     20
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nb <-}\StringTok{ }\KeywordTok{naiveBayes}\NormalTok{(Survived }\OperatorTok{~}\StringTok{ }\NormalTok{., }\DataTypeTok{data =}\NormalTok{ Titanic)}
\NormalTok{nb}\OperatorTok{$}\NormalTok{apriori}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Survived
##   No  Yes 
## 1490  711
\end{verbatim}

Poni偶sze tabele zawieraj warunkowe prawdopodobiestwa przynale偶noci do poszczeg贸lnych klas.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nb}\OperatorTok{$}\NormalTok{tables}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $Class
##         Class
## Survived        1st        2nd        3rd       Crew
##      No  0.08187919 0.11208054 0.35436242 0.45167785
##      Yes 0.28551336 0.16596343 0.25035162 0.29817159
## 
## $Sex
##         Sex
## Survived       Male     Female
##      No  0.91543624 0.08456376
##      Yes 0.51617440 0.48382560
## 
## $Age
##         Age
## Survived      Child      Adult
##      No  0.03489933 0.96510067
##      Yes 0.08016878 0.91983122
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dane <-}\StringTok{ }\KeywordTok{as.data.frame}\NormalTok{(Titanic)}
\NormalTok{pred <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(nb, dane)}
\NormalTok{pred}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] Yes No  No  No  Yes Yes Yes Yes No  No  No  No  Yes Yes Yes Yes Yes No  No 
## [20] No  Yes Yes Yes Yes No  No  No  No  Yes Yes Yes Yes
## Levels: No Yes
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tab <-}\StringTok{ }\KeywordTok{table}\NormalTok{(pred, dane}\OperatorTok{$}\NormalTok{Survived)}
\NormalTok{tab}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      
## pred  No Yes
##   No   7   7
##   Yes  9   9
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sum}\NormalTok{(}\KeywordTok{diag}\NormalTok{(}\KeywordTok{prop.table}\NormalTok{(tab)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.5
\end{verbatim}

Naiwny klasyfikator spisa si bardzo sabo, poniewa偶 klasyfikacja na poziomie 0.5 jest taka jak przy rzucie monet.

\BeginKnitrBlock{example}
\protect\hypertarget{exm:unnamed-chunk-78}{}{\label{exm:unnamed-chunk-78} }Przeprowadzimy klasyfikacj gatunk贸w irys贸w na podstawie szerokoci i dugoci kielicha i patka.
\EndKnitrBlock{example}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(klaR)}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{2019}\NormalTok{)}
\NormalTok{uczaca <-}\StringTok{ }\KeywordTok{sample}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\KeywordTok{nrow}\NormalTok{(iris), }\DecValTok{2}\OperatorTok{*}\KeywordTok{nrow}\NormalTok{(iris)}\OperatorTok{/}\DecValTok{3}\NormalTok{)}
\NormalTok{pr.ucz <-}\StringTok{ }\NormalTok{iris[uczaca,]}
\NormalTok{pr.test <-}\StringTok{ }\NormalTok{iris[}\OperatorTok{-}\NormalTok{uczaca,]}
\NormalTok{nb2 <-}\StringTok{ }\KeywordTok{NaiveBayes}\NormalTok{(Species}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data =}\NormalTok{ pr.ucz)}
\NormalTok{nb2}\OperatorTok{$}\NormalTok{apriori}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## grouping
##     setosa versicolor  virginica 
##       0.35       0.32       0.33
\end{verbatim}

Prawdopodobiestwa a priori zostay oszacowane na podstawie pr贸by uczcej. Poni偶sze tabele zawieraj rednie i odchylenia standardowe zmiennych w poszczeg贸lnych klasach.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nb2}\OperatorTok{$}\NormalTok{tables}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $Sepal.Length
##                [,1]      [,2]
## setosa     4.982857 0.3485143
## versicolor 6.003125 0.5462390
## virginica  6.463636 0.5808497
## 
## $Sepal.Width
##                [,1]      [,2]
## setosa     3.408571 0.3616721
## versicolor 2.750000 0.3537814
## virginica  2.960606 0.2838787
## 
## $Petal.Length
##                [,1]      [,2]
## setosa     1.480000 0.1875539
## versicolor 4.275000 0.4852668
## virginica  5.460606 0.5225774
## 
## $Petal.Width
##                 [,1]      [,2]
## setosa     0.2657143 0.1109925
## versicolor 1.3437500 0.2198790
## virginica  2.0151515 0.3123712
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pred <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(nb2, }\DataTypeTok{newdata =}\NormalTok{ pr.test)}
\NormalTok{tab <-}\StringTok{ }\KeywordTok{table}\NormalTok{(pred}\OperatorTok{$}\NormalTok{class, pr.test}\OperatorTok{$}\NormalTok{Species)}
\NormalTok{tab}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##             
##              setosa versicolor virginica
##   setosa         15          0         0
##   versicolor      0         17         0
##   virginica       0          1        17
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sum}\NormalTok{(}\KeywordTok{diag}\NormalTok{(}\KeywordTok{prop.table}\NormalTok{(tab)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.98
\end{verbatim}

Klasyfikacja na podstawie modelu jest bardzo dobra (98\%).

\hypertarget{zalety-i-wady-1}{%
\section{Zalety i wady}\label{zalety-i-wady-1}}

\begin{itemize}
\tightlist
\item
  Zalety:

  \begin{itemize}
  \tightlist
  \item
    prostota konstrukcji i prosty algorytm;
  \item
    jeli jest spenione zao偶enie warunkowej niezale偶noci, to ten klasyfikator dziaa szybciej i czasem lepiej ni偶 inne metody klasyfikacji;
  \item
    nie potrzebuje du偶ych zbior贸w danych do estymacji parametr贸w;
  \end{itemize}
\item
  Wady:

  \begin{itemize}
  \tightlist
  \item
    czsto nie spenione zao偶enie o warunkowej niezale偶noci powoduje obci偶enie wynik贸w;
  \item
    brak mo偶liwoci wprowadzania interakcji efekt贸w kilku zmiennych;
  \item
    potrzebuje zao偶enia normalnoci warunkowych gstoci w przypadku cigych atrybut贸w;
  \item
    czsto istniej lepsze klasyfikatory.
  \end{itemize}
\end{itemize}

\hypertarget{metoda-k-najbliux17cszych-sux105siaduxf3w}{%
\chapter{\texorpdfstring{Metoda \(k\) najbli偶szych ssiad贸w}{Metoda k najbli偶szych ssiad贸w}}\label{metoda-k-najbliux17cszych-sux105siaduxf3w}}

Technika \(k\) najbli偶szych ssiad贸w (ang. \emph{\(k\)-Nearest Neighbors}) przewiduje warto zmiennej wynikowej na podstawie \(k\) najbli偶szych obserwacji zbioru uczcego. W przeciwiestwie do wspomnianych wczeniej modeli liniowych, nie posiada ona jawnej formy i nale偶y do klasy technik nazywanych czarnymi skrzynkami (ang. \emph{black box}). Mo偶e by wykorzystywana, zar贸wno do zada klasyfikacyjnych, jak i regresyjnych. W obu przypadkach predykcja dla nowych wartoci predyktor贸w przebiega podobnie.

Niech \(\boldsymbol x_0\) bdzie obserwacj, dla kt贸rej poszukujemy wartoci zmiennej wynikowej \(y_0\). Na podstawie zbioru obserwacji \(\boldsymbol x\in T\) zbioru uczcego wyznacza si \(k\) najbli偶szych ssiad贸w\footnote{metryk mo偶na wybiera dowolnie, cho najczciej jest to metryka euklidesowa}, gdzie \(k\) jest z g贸ry ustalon wartoci. Nastpnie, jeli zadanie ma charakter klasyfikacyjny, to \(y_0\) przypisuje si mod zmiennej wynikowej obserwacji bdcych \(k\) najbli偶szymi ssiadami. W przypadku zada regresyjnych \(y_0\) przypisuje si redni lub median.

Olbrzymie znaczenie dla wynik贸w predykcji na podstawie metody \emph{kNN} ma dob贸r metryki. Nie istnieje obiektywna technika wyboru najlepszej metryki, dlatego jej doboru dokonujemy metod pr贸b i bd贸w. Nale偶y dodatkowo pamita, 偶e wielkoci mierzone \(\boldsymbol x\) mog si r贸偶ni zakresami zmiennoci, a co za tym idzie, mog znaczco wpyn na mierzone odlegoci pomidzy punktami. Dlatego zaleca si standaryzacj zmiennych przed zastosowaniem metody \emph{kNN}.

Kolejnym parametrem, kt贸ry ma znaczcy wpyw na predykcj, jest liczba ssiad贸w \(k\). Wyb贸r zbyt maej liczby \(k\) mo偶e doprowadzi do przeuczenia modelu jak to jest pokazane na rysunku \ref{fig:knn1}

\begin{figure}

{\centering \includegraphics{EksploracjaDanych_files/figure-latex/knn1-1} 

}

\caption{Przykad klasyfikacji dla $k=1$}\label{fig:knn1}
\end{figure}

Z kolei zbyt du偶a liczba ssiad贸w powoduje obci偶enie wynik贸w (patrz rysunek \ref{fig:knn2})

\begin{figure}

{\centering \includegraphics{EksploracjaDanych_files/figure-latex/knn2-1} 

}

\caption{Przykad zastosowania 100 ssiad贸w}\label{fig:knn2}
\end{figure}

Dopiero dob贸r odpowiedniego \(k\) daje model o stosunkowo niskiej wariancji i obci偶eniu. Najczciej liczby \(k\) poszukujemy za pomoc pr贸bkowania.

\begin{figure}

{\centering \includegraphics{EksploracjaDanych_files/figure-latex/knn3-1} 

}

\caption{Model z optymaln liczb ssiad贸w}\label{fig:knn3}
\end{figure}

\BeginKnitrBlock{example}
\protect\hypertarget{exm:knnprzklad1}{}{\label{exm:knnprzklad1} }Klasyfikacj z wykorzystaniem metody \emph{kNN} przeprowadzimy na przykadzie danych zbioru \texttt{spam} pakietu \texttt{ElemStatLearn}. Metoda \emph{kNN} ma wiele implementacji R-owych ale na potrzeby przykadu wykorzystamy funkcj \texttt{knn3} pakietu \texttt{caret}.

Najpierw dokonamy oszacowania optymalnego \(k\)
\EndKnitrBlock{example}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(ElemStatLearn)}
\KeywordTok{library}\NormalTok{(tidyverse)}

\NormalTok{spam.std <-}\StringTok{ }\NormalTok{spam }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{mutate_if}\NormalTok{(is.numeric, scale)}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{ind <-}\StringTok{ }\KeywordTok{sample}\NormalTok{(}\KeywordTok{nrow}\NormalTok{(spam), }\DataTypeTok{size =} \KeywordTok{nrow}\NormalTok{(spam)}\OperatorTok{*}\DecValTok{2}\OperatorTok{/}\DecValTok{3}\NormalTok{)}
\NormalTok{dt.ucz <-}\StringTok{ }\NormalTok{spam.std[ind,]}
\NormalTok{dt.test <-}\StringTok{ }\NormalTok{spam.std[}\OperatorTok{-}\NormalTok{ind,]}

\NormalTok{acc <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(pred, obs)\{}
\NormalTok{    tab <-}\StringTok{ }\KeywordTok{table}\NormalTok{(pred,obs)}
\NormalTok{    acc <-}\StringTok{ }\KeywordTok{sum}\NormalTok{(}\KeywordTok{diag}\NormalTok{(}\KeywordTok{prop.table}\NormalTok{(tab)))}
\NormalTok{    acc}
\NormalTok{\}}

\DecValTok{1}\OperatorTok{:}\DecValTok{40} \OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{map}\NormalTok{(}\OperatorTok{~}\KeywordTok{knn3}\NormalTok{(spam}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data =}\NormalTok{ dt.ucz, }\DataTypeTok{k =}\NormalTok{ .x)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{map}\NormalTok{(}\OperatorTok{~}\KeywordTok{predict}\NormalTok{(.x, }\DataTypeTok{newdata =}\NormalTok{ dt.test, }\DataTypeTok{type =} \StringTok{"class"}\NormalTok{)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{map_dbl}\NormalTok{(}\OperatorTok{~}\KeywordTok{acc}\NormalTok{(}\DataTypeTok{pred =}\NormalTok{ .x, }\DataTypeTok{obs =}\NormalTok{ dt.test}\OperatorTok{$}\NormalTok{spam)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{tibble}\NormalTok{(}\DataTypeTok{k =} \DecValTok{1}\OperatorTok{:}\KeywordTok{length}\NormalTok{(.), }\DataTypeTok{acc=}\NormalTok{.) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(k, acc))}\OperatorTok{+}
\StringTok{     }\KeywordTok{geom_line}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{EksploracjaDanych_files/figure-latex/knnrys-1} 

}

\caption{Ocena jakoci dopasowania modelu dla r贸偶nej liczby ssiad贸w}\label{fig:knnrys}
\end{figure}

Biorc pod uwag wykres \ref{fig:knnrys} mo偶na rozwa偶a 3 lub 5 ssiad贸w jako optymalne rozwizanie, poniewa偶 w贸wczas poprawno klasyfikacji jest najwy偶sza. Proponuje unika rozwizania z 1 najbli偶szym ssiadem poniewa偶, bdzie si ono charakteryzowao du偶a zmiennoci. Wyb贸r \(k=3\) wydaje si by optymalny.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mod.knn <-}\StringTok{ }\KeywordTok{knn3}\NormalTok{(spam}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data =}\NormalTok{ dt.ucz,}
                \DataTypeTok{k =} \DecValTok{3}\NormalTok{)}
\NormalTok{mod.knn}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 3-nearest neighbor model
## Training set outcome distribution:
## 
## email  spam 
##  1860  1207
\end{verbatim}

Predykcji dokonujemy w ten sam spos贸b co w innych modelach klasyfikacyjnych

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pred.knn.class <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(mod.knn, }\DataTypeTok{newdata =}\NormalTok{ dt.test, }\DataTypeTok{type =} \StringTok{"class"}\NormalTok{)}
\KeywordTok{head}\NormalTok{(pred.knn.class)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] spam spam spam spam spam spam
## Levels: email spam
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pred.knn <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(mod.knn, }\DataTypeTok{newdata =}\NormalTok{ dt.test)}
\KeywordTok{head}\NormalTok{(pred.knn)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##          email      spam
## [1,] 0.0000000 1.0000000
## [2,] 0.3333333 0.6666667
## [3,] 0.3333333 0.6666667
## [4,] 0.0000000 1.0000000
## [5,] 0.3333333 0.6666667
## [6,] 0.0000000 1.0000000
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(tab <-}\StringTok{ }\KeywordTok{table}\NormalTok{(pred.knn.class, dt.test}\OperatorTok{$}\NormalTok{spam))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##               
## pred.knn.class email spam
##          email   869   88
##          spam     59  518
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sum}\NormalTok{(}\KeywordTok{diag}\NormalTok{(}\KeywordTok{prop.table}\NormalTok{(tab)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.9041721
\end{verbatim}

\hypertarget{uoguxf3lnione-modele-addytywne}{%
\chapter{Uog贸lnione modele addytywne}\label{uoguxf3lnione-modele-addytywne}}

Modele liniowe, jako techniki klasyfikacji i regresji, maj niewtpliw zalet - jawna posta zale偶noci pomidzy predyktorami i zmienn wynikow. Czsto w rzeczywistoci tak uproszczony model nie potrafi odda zo偶onoci natury badanego zjawiska. Dlatego powsta pomys aby w miejsce kombinacji liniowej predyktor贸w wstawi kombinacj liniow ich funkcji, czyli
\begin{equation}
    \E(Y|X)=f(X) = \sum_{i=1}^M\beta_mh_m(X),
    \label{eq:row111}
\end{equation}
gdzie \(h_m:\mathbb{R}^d\to\mathbb{R}\) nazywana czsto funkcj bazow (ang. \emph{linear basis expansion}). W贸wczas w zale偶noci od postaci funkcji bazowej otrzymujemy modele z r贸偶nymi poziomami elastycznoci:

\begin{itemize}
\tightlist
\item
  gdy \(h_m(X)=X_m,\ m=1,\ldots,M\), to otrzymujemy model liniowy;
\item
  gdy \(h_m(X)=X_j^2\) lub \(h_m(X)=X_jX_k\), to otrzymujemy struktury wielomianowe, charakteryzujce si wiksz elastycznoci modelu;
\item
  gdy \(h_m(X)=\log X_j\) lub \(h_m(X)=\sqrt{X_j}\), to uzyskujemy nieliniowo czynnik贸w wchodzcych w skad kombinacji liniowej \eqref{eq:row111};
\item
  dopuszczalne s r贸wnie偶 kawakami liniowe funkcje postaci \(h_m(X)= I(l_m\leq X_k <u_m)\), gdzie \(I\) jest funkcj charakterystyczn (ang. \emph{indicator}) przedziau \([l_m,u_m)\).
\end{itemize}

Zbiory wszystkich funkcji bazowych definiowanych w ten spos贸b tworzy sownik funkcji bazowych \(\mathcal{D}\). Aby kontrolowa zo偶ono modeli, majc do dyspozycji tak zasobny sownik, wprowadza si nastpujce podejcia:

\begin{itemize}
\tightlist
\item
  ogranicza si klas dostpnych funkcji bazowych
  \begin{equation}
    f(X) = \sum_{j=1}^df_j(X_j)=\sum_{j=1}^d\sum_{m=1}^{M_j}\beta_{jm}h_{jm}(X_j),
  \end{equation}
\item
  wcza si do modelu jedynie te funkcje ze sownika \(\mathcal{D}\), kt贸re istotnie poprawiaj dopasowanie modelu,
\item
  u偶ywa si metod penalizowanych, czyli dopuszcza si stosowanie wszystkich funkcji bazowych ze sownika \(\mathcal{D}\), ale wsp贸czynniki przy nich stojce s ograniczane.
\end{itemize}

\hypertarget{przypadek-jednowymiarowy}{%
\section{Przypadek jednowymiarowy}\label{przypadek-jednowymiarowy}}

Dla uproszczenia rozwa偶a przyjmiemy, 偶e \(X\) jest jednowymiarowe.

\begin{figure}

{\centering \includegraphics[width=1.91in]{images/spline1} 

}

\caption{Przykadowe zastosowanie kilku rodzaj贸w funkcji bazowych. Wykres w lewym g贸rnym rogu powsta ze staych na przedziaach, wykres w g贸rnym prawym rogu powsta z liniowych funkcji bazowych na przedzialach, w lewym dolnym rogu model powsta r贸wnie偶 z liniowych funkcji bazowych na przedzialach ale z zao偶eniem cigoci, a prawym dolnym rogu powsta z zastosowania funcji bazowej $\max(X-\xi_1,0)$}\label{fig:spline1}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=1.85in]{images/spline2} 

}

\caption{Kolejne wykresy przedstawiaj coraz bardziej gadkie modele bdce efektem dodawania wielomian贸w trzeciego stopnia na przedziaach. Na ka偶dym kolejnym modelu mymuszone zostay silniejsze zao偶enia dotyczce gadkoci}\label{fig:spline2}
\end{figure}

Przykadowo szecienny splajn\footnote{czyli funkcja czona} dla dw贸ch punk贸w wzowych skada si z nastpujcych funkcji bazowych

\begin{gather}
    h_1(X)=1,\quad h_3(X)=X^2,\quad h_5(X)=(X-\xi_1)_+^3\\
    h_2(X)=X,\quad h_4(X)=X^3,\quad h_6(X)=(X-\xi_2)_+^3.
\end{gather}

Zachowanie wielomian贸w poza punktami wzowymi jest czasami bardzo dziwne. Zdarza si, 偶e charakteryzuj si tam du偶 zmiennoci. Dlatego wprowadza si takie splajny aby w obszarach brzegowych zachowyway si przewidywalnie.
Naturalny splajn szecienny zakada liniowo modelu poza wzami brzegowymi. Dla \(K\) wz贸w naturalny splajn szecienny skada si z nastpujcych funkcji bazowych

\begin{gather}
    N_1(X)=1,\quad N_2(X)=X,\quad N_{k+2}(X)=d_k(X)-d_{K-1}(X),
\end{gather}
gdzie \(d_k(X)=\frac{(X-\xi_k)^3_+-(X-\xi_K)^3_+}{\xi_K-\xi_k}.\)

Estymacji parametr贸w modelu dokonujemy metod najmniejszych kwadrat贸w, minimalizujc
\begin{equation}
    RSS(f,\lambda) = \sum_{i=1}^N(y_i-f(x_i))^2+\lambda\int(f''(t))^2dt,
\end{equation}
gdzie \(\lambda\) jest parametrem wygadzania. Pierwsze wyra偶enie po prawej stronie to ocena dopasowania, a drugie to kara za krzywoliniowo.
Dla naturalnego splajna
\begin{equation}
    f(x)=\sum_{j=1}^NN_j(x)\beta_j
\end{equation}
minimalizujemy
\begin{equation}
    RSS(\beta, \lambda)=(\boldsymbol y -\boldsymbol N\beta)'(\boldsymbol y-\boldsymbol N\beta)+\lambda\beta'\boldsymbol \Omega \beta,
\end{equation}
gdzie \(\{\boldsymbol N\}_{ij}= N_j(x_i)\) i \(\{\boldsymbol \Omega\}_{jk}=\int N''_j(t)N''_k(t)dt\). Rozwizaniem zaganienia minimalizacji \(RSS(\beta,\lambda)\) jest
\begin{equation}
    \hat{\beta}=((\boldsymbol N'\boldsymbol N)+\lambda\boldsymbol \Omega)^{-1}\boldsymbol N'\boldsymbol y.
\end{equation}

\hypertarget{przypadek-wielowymiarowy}{%
\section{Przypadek wielowymiarowy}\label{przypadek-wielowymiarowy}}

W przypadku gdy \(X\in \mathbb{R}^d\) poszukujemy takiej \(d\)-wymiarowej regresji \(f(x)\), kt贸ra bdzie minimalizowaa wyra偶enie
\begin{equation}
    \min_f\sum_{i=1}^N(y_i-f(x_i))^2+\lambda J(f),
\end{equation}
gdzie \(J\) jest odpowiedni funkcj wyra偶ajc krzywoliniowo modelu. Dla \(X\in \mathbb{R}^2\) przyjmuje posta
\begin{equation}
    J(f)=\iint_{\mathbb{R}^2}\left[\left(\frac{\partial^2 f(x)}{\partial^2 x_1}\right)^2+2\left(\frac{\partial^2 f(x)}{\partial x_1\partial x_2}\right)^2+
    \left(\frac{\partial^2 f(x)}{\partial^2 x_2}\right)^2\right]dx_1dx_2.
\end{equation}
Rozwizanie przyjmuje posta
\begin{equation}
    f(x) = \beta_0+\beta'x+\sum_{i=1}^N \alpha_ih_i(x),
\end{equation}
gdzie \(h_i(x)=||x-x_j||^2\log||x-x_j||\).

\hypertarget{uoguxf3lnione-modele-addytywne-1}{%
\section{Uog贸lnione modele addytywne}\label{uoguxf3lnione-modele-addytywne-1}}

Przez uog贸lnione modele addytywne (ang. \emph{Generalized Additive Models}) rozumiemy klas modeli, kt贸re poprzez funkcj czc, opisuj warunkow warto zmiennej wynikowej w nastpujcy spos贸b
\begin{equation}
    g(\E(Y|X))=g(\mu(X))=\alpha+f_1(X_1)+\dots+f_d(X_d),
\end{equation}
gdzie \(g\) jest funkcj czc. Najczciej stosowanymi funkcjami czcymi s:

\begin{itemize}
\tightlist
\item
  \(g(\mu)=\mu\) - stosowana w modelach, gdy zmienna wynikowa ma rozkad normalny;
\item
  \(g(\mu)=\logit\mu\) - stosowana, gdy zmienna wynikowa ma rozkad dwumianowy;
\item
  \(g(\mu)=\probit\mu\) - stosowana r贸wnie偶 w przypadku gdy zmienna ma rozkad dwumianowy, a \(\Phi^{-1}\) oznacza odwrotno dystrybuanty standaryzowanego rozkadu normalnego;
\item
  \(g(\mu)=\log\mu\) - stosowana, gdy zmienna wynikowa jest zmienn typu zliczeniowego (rozkad Poissona).
\end{itemize}

\hypertarget{algorytm-uczenia-modelu-gam}{%
\subsection{Algorytm uczenia modelu GAM}\label{algorytm-uczenia-modelu-gam}}

Algorytm uczenia wstecznego (ang. \emph{backfitting}) przebiega wg nastpujcych krok贸w:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Ustalamy wstpne oszacowania na \(\alpha=\bar{y}\) i \(\hat{f}_j=0\).
\item
  Dla \(j=1,\ldots,d,1,\ldots,d,1,\ldots\) powtarzamy szacowanie
  \begin{align}
   \hat{f}_j\leftarrow &\mathcal{S}_j\left[(y_i-\hat{\alpha}-\sum_{k\neq j}\hat{f}_k(x_{ik}))^N_1\right],\\
   \hat{f}_j\leftarrow &\hat{f}_j-\frac{1}{N}\sum_{i=1}^N\hat{f}_j(x_{ij})
  \end{align}
  dop贸ki \(\hat{f}_j\) osignie zbie偶no. Funkcja \(\mathcal{S}_j\left[(y_i-\hat{\alpha}-\sum_{k\neq j}\hat{f}_k(x_{ik}))^N_1\right]\) jest jednowymiarowym szeciennym splajnem o \(N\) wzach. W jej miejsce mo偶na przyj r贸wnie偶 inne funkcje, takie jak: jednowymiarowe lokalne regresje wielomianowe (ang. \emph{LOESS - locally estimated scatterplot smoothing}), regresje liniowe, wielomianowe.
\end{enumerate}

\BeginKnitrBlock{example}
\protect\hypertarget{exm:gam}{}{\label{exm:gam} }Dla zilustrowania zasady dziaania uog贸lnionych modeli addytywnych przeprowadzimy analiz st偶enia ozonu (\(O_3\)) w zale偶noci od wybranych parametr贸w meteorologicznych. Do zbudowania modelu GAM wykorzystamy funkcj \texttt{gam} pakietu \texttt{mgcv} (Wood \protect\hyperlink{ref-R-mgcv}{2003}).
\EndKnitrBlock{example}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(faraway)}
\KeywordTok{head}\NormalTok{(ozone)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   O3   vh wind humidity temp  ibh dpg ibt vis doy
## 1  3 5710    4       28   40 2693 -25  87 250  33
## 2  5 5700    3       37   45  590 -24 128 100  34
## 3  5 5760    3       51   54 1450  25 139  60  35
## 4  6 5720    4       69   35 1568  15 121  60  36
## 5  4 5790    6       19   45 2631 -33 123 100  37
## 6  4 5790    3       25   55  554 -28 182 250  38
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(mgcv)}
\NormalTok{mod.gam <-}\StringTok{ }\KeywordTok{gam}\NormalTok{(O3}\OperatorTok{~}\KeywordTok{s}\NormalTok{(temp, }\DataTypeTok{bs =} \StringTok{"cr"}\NormalTok{, }\DataTypeTok{m =} \DecValTok{2}\NormalTok{)}\OperatorTok{+}\KeywordTok{s}\NormalTok{(ibh)}\OperatorTok{+}\KeywordTok{s}\NormalTok{(ibt), }\DataTypeTok{data =}\NormalTok{ ozone)}
\KeywordTok{summary}\NormalTok{(mod.gam)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Family: gaussian 
## Link function: identity 
## 
## Formula:
## O3 ~ s(temp, bs = "cr", m = 2) + s(ibh) + s(ibt)
## 
## Parametric coefficients:
##             Estimate Std. Error t value Pr(>|t|)    
## (Intercept)  11.7758     0.2382   49.44   <2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Approximate significance of smooth terms:
##           edf Ref.df      F  p-value    
## s(temp) 3.357  4.216 20.758 5.99e-16 ***
## s(ibh)  4.171  5.072  7.344 1.37e-06 ***
## s(ibt)  2.111  2.729  1.403    0.213    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## R-sq.(adj) =  0.708   Deviance explained = 71.7%
## GCV = 19.348  Scale est. = 18.724    n = 330
\end{verbatim}

W powy偶szym modelu u偶yto splajn贸w jako funkcji \(f_i\). W przypadku zmiennej \texttt{temp} by to szecienny splajn z regularyzacj w postaci cigoci drugiej pochodnej, a pozostae s prostymi splajnami. Dopasowanie modelu siga 71.7\% a warto uog贸lnionego sprawdzianu krzy偶owego 19.35.

Poni偶szy wykres pokazuje rodzaje transformacji u偶yte przy dopasowaniu modelu.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{))}
\KeywordTok{plot}\NormalTok{(mod.gam, }\DataTypeTok{shade =}\NormalTok{ T, }\DataTypeTok{residuals =}\NormalTok{ T)}
\end{Highlighting}
\end{Shaded}

\includegraphics{EksploracjaDanych_files/figure-latex/unnamed-chunk-85-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\hypertarget{metoda-wektoruxf3w-noux15bnych}{%
\chapter{Metoda wektor贸w nonych}\label{metoda-wektoruxf3w-noux15bnych}}

\hypertarget{wprowadzenie}{%
\section{Wprowadzenie}\label{wprowadzenie}}

\textbf{Metoda wektor贸w nonych}\footnote{lub podpierajcych} (ang. \emph{Support Vector Machines}) to kolejna metoda klasyfikacji obserwacji na podstawie cech (atrybut贸w). Jest technik z nauczycielem tzn., 偶e w pr贸bie uczcej wystpuj zar贸wno cechy charakteryzujce badane obiekty jak i ich przynale偶no do klasy.

\begin{figure}

{\centering \includegraphics[width=3.71in]{images/SVM_Example_of_Hyperplanes} 

}

\caption{Przykad prostych separujcych obiekty obu grup}\label{fig:svm1}
\end{figure}

\hypertarget{definicja-modelu-dla-klas-liniowo-separowalnych}{%
\section{Definicja modelu dla klas liniowo separowalnych}\label{definicja-modelu-dla-klas-liniowo-separowalnych}}

Istot tej metody jest znalezienie wektor贸w nonych, definiujcych hiperpowierzchnie optymalnie separujce obiekty w homogeniczne grupy.

Niech \(D\) bdzie zbiorem \(n\) punkt贸w w \(d\)-wymiarowej przestrzeni okrelonych nastpujco \((\vec{x}_i, y_i)\), \(i=1,\ldots, d\), gdzie \(y_i\) przyjmuje wartoci -1 lub 1 w zale偶noci od tego do kt贸rej grupy nale偶y (zakadamy istnienie tylko dw贸ch grup). Poszukujemy takiej hiperpaszczyzny, kt贸ra maksymalizuje margines pomidzy punktami obu klas w przestrzeni cech \(\vec{x}\).

\begin{figure}

{\centering \includegraphics[width=11.11in]{images/Svm_max_sep_hyperplane_with_margin} 

}

\caption{Paszczyzna najlepiej rozdzielajca obiekty obu grup (biae i czarne kropki) wraz z prostymi wyznaczajcymi maksymalny margines separujcy obie grupy}\label{fig:svm2}
\end{figure}

Margines ten jest okrelany jako najmniejsza odlego pomidzy hiperpaszczyzn i elementami z ka偶dej z grup. Dowolna hiperpaszczyzna mo偶e by zapisana r贸wnaniem \(\vec{w}\vec{x}-b=0\), gdzie \(\vec{w}\) jest wektorem normalnym do hiperpaszczyzny. Jeli dane s liniowo separowalne to, mo偶na wybra takie dwie hiperpaszczyzny, 偶e odlego pomidzy nimi jest najwiksza. R贸wnania tych hiperpaszczyzn dane s wzorami
\begin{equation}
    \vec{w}\vec{x}-b=1, \quad \vec{w}\vec{x}-b=-1
    \label{eq:hiper1}
\end{equation}

Odlego pomidzy tymi hiperpaszczyznami wynosi \(\tfrac{2}{\|\vec{w}\|}\). Zatem 偶eby zmaksymalizowa odlego pomidzy hiperpaszczyznami (margines) musimy zminimalizowa \(\tfrac{\|\vec{w}\|}{2}\).
Dodatkowo, 偶eby nie pozwoli aby punkty wpaday do marginesu musimy nao偶y dodatkowe ograniczenia
\begin{align}
    \vec{w}\vec{x}_i-b\geq& 1, \quad   y_i=1\\
    \vec{w}\vec{x}_i-b\leq& -1, \quad y_i=-1
    \label{eq:hiper2}
\end{align}
Co mo偶na zapisa prociej
\begin{equation}
    y_i(\vec{w}\vec{x}_i-b)\geq 1,\quad 1\leq i\leq n.
    \label{eq:hiper3}
\end{equation}
Zatem \(\vec{w}\) i \(b\) minimalizujce \(\|\vec{w}\|\) przy jednoczesnym spenieniu warunku \eqref{hiper3} definiuj klasyfikator postaci
\begin{equation}
    \vec{x}\rightarrow \operatorname{sgn}(\vec{w}\vec{x}-b).
    \label{eq:hiper4}
\end{equation}

Z racji, 偶e \(\|\vec{w}\|\) jest okrelona jako pierwiastek sumy kwadrat贸w poszczeg贸lnych wsp贸rzdnych wektora, to czciej w minimalizacji stosuje si \(\|\vec{w}\|^2\).

Sformuowany powy偶ej problem nale偶y do grupy optymalizacji funkcji kwadratowej przy liniowych ograniczeniach. Rozwizuje si go metod mno偶nik贸w Lagrange'a.

Minimalizujemy funkcj
\begin{equation}
    L(w, b, \alpha) = \frac{1}{2}\|\vec{w}\|^2-\sum_{i=1}^{n}\alpha_i\big(y_i(\vec{w}\vec{x}_i-b)-1\big),
    \label{eq:lagrange}
\end{equation}
gdzie \(\alpha_i\) s mno偶nikami Lagrange'a.

Niestety rozwizanie takiego r贸wnania r贸偶niczkujc po \(\vec{w}\) i \(b\) i przyr贸wnujc do zera nie jest atwe. Dlatego Karush-Kuhn Tucker wprowadzili ograniczenia na mno偶niki \(\alpha_i\geq 0\) oraz \(\alpha_i\big(y_i(\vec{w}\vec{x}_i-b)-1\big)=0\). Co w konsekwencji powoduje, 偶e \(\alpha_i\) s niezerowe jedynie dla wektor贸w nonych, a dla pozostaych 0.

Dalej jednak poszukiwanie rozwizania zagadnienia minimalizacji funkcji \(L\) ze wzgldu na tak wiele parametr贸w mo偶e by uci偶liwe. W贸wczas stosuje si maksymalizacj dualnej wersji\footnote{w przypadku przestrzeni wypukej oba rozwizania si pokrywaj}
\begin{equation}
    L_D(\alpha) = \sum_{i=1}^{n}\alpha_i+\frac{1}{2}\sum_{i=1}^{n}\sum_{j=1}^{n}\alpha_i\alpha_jy_iy_j\vec{x}_i'\vec{x}_j
    \label{eq:lagrange2}
\end{equation}
przy ograniczeniach \(\alpha_i\geq 0\) i \(\sum_{i=1}^{n}\alpha_iy_i=0\).

Rozwizaniem powy偶szego zagadnienia jest
\begin{align}
    \vec{w}=&\sum_{i=1}^{n}\alpha_iy_i\vec{x}_i,\label{eq:lagrange3}\\
        b=&y_i-\vec{w}\vec{x}_i,
        \label{eq:lagrange4}
\end{align}
a hiperpaszczyzna decyzyjna
\begin{equation}
\sum_{i=1}^{n}\alpha_iy_i\vec{x}_i\vec{x}-b=0,
\label{eq:hiper5}
\end{equation}
gdzie \(\vec{x}_i\) s wektorami nonymi ze zbioru uczcego, a \(\vec{x}\) jest nowym wektorem dla kt贸rego przeprowadzamy klasyfikacj.
Nale偶y r贸wnie偶 zauwa偶y, 偶e im wiksza warto \(\alpha_i\), tym wikszy wpyw wektora na granic decyzyjn.

\hypertarget{definicja-modelu-dla-klas-nieliniowo-separowalnych}{%
\section{Definicja modelu dla klas nieliniowo separowalnych}\label{definicja-modelu-dla-klas-nieliniowo-separowalnych}}

Niestety rzadko przestrze atrybut贸w jest liniowo separowalna. Stosuje si w贸wczas modyfikacj powy偶szej metody przez wprowadzenie nastpujcej funkcji straty
\begin{equation}
    \zeta_i=\max\big(0,1-y_i(\vec{w}\vec{x}_i-b)\big).
    \label{eq:strata1}
\end{equation}
Zauwa偶my, 偶e \(\zeta_i\) jest najmniejsz liczb nieujemn speniajc nier贸wno
\begin{equation}
    y_i(\vec{w}\vec{x}_i-b)\geq 1-\zeta_i.
    \label{eq:nier}
\end{equation}
Mo偶emy j interpretowa tak, 偶e jeli warunek \eqref{eq:hiper3} jest speniony, czyli punkty le偶 na zewntrz marginesu (po waciwych stronach), to funkcja straty przyjmuje warto 0. W przeciwnym przypadku warto funkcji jest proporcjonalna do odlegoci od brzegu marginesu. Dlatego wystarczy zminimalizowa warto
\begin{equation}
    \frac{1}{n}\sum_{i=1}^{n}\zeta_i+\lambda\|\vec{w}\|^2,
    \label{eq:strata2}
\end{equation}
przy warunku \eqref{eq:nier} i \(\zeta_i\geq 0\) oraz gdzie \(\lambda\) jest wag kompromisu pomidzy szerokoci marginesu a zapewnieniem, 偶e punkty le偶 po waciwych stronach marginesu. Przy dostatecznie maych wartociach \(\lambda\) i separowalnoci liniowej punkt贸w przestrzeni atrybut贸w powy偶szy klasyfikator bdzie si zachowywa podobnie jak \eqref{eq:hiper4}.

Rozwizanie problemu minimalizacji funkcji straty okrelonej w \eqref{eq:strata2} za pomoc dualnej wersji mno偶nik贸w Lagrange'a sprowadza si do minimalizacji funkcji
\begin{equation}
    L(\alpha_i) = \sum_{i=1}^{n}\alpha_i+\frac{1}{2}\sum_{i=1}^{n}\sum_{j=1}^{n}\alpha_i\alpha_jy_iy_j\vec{x}_i'\vec{x}_j,
\label{eq:strata3}  
\end{equation}
przy warunkach
\begin{equation}
    \sum_{i=1}^{n}\alpha_iy_i=0,\quad 0\leq \alpha_i\leq \frac{1}{2n\lambda}.
    \label{eq:nier2}
\end{equation}
Wektor normalny do hiperpaszczyzny jest postaci
\begin{equation}
    \vec{w}=\sum_{i=1}^{n}\alpha_iy_i\vec{x}_i,
    \label{eq:wagi2}
\end{equation}
a parametr \(b\) taki jak w \eqref{eq:lagrange4}.

Powy偶szy algorytm zosta przedstawiony przez Vapnika w 1963 roku jako klasyfikator liniowy ale dopiero po wprowadzeniu funkcji jdrowych przeksztacajcych liniowy brzeg decyzyjny na nieliniowy, metoda ta zyskaa w oczach statystyk贸w.

\hypertarget{definicja-modelu-jux105drowego}{%
\section{Definicja modelu jdrowego}\label{definicja-modelu-jux105drowego}}

W roku 1992 Boser, Guyon i Vapnik wprowadzili pojcie nieliniowego klasyfikatora opartego na metodzie wektor贸w nonych, kt贸ry byo uog贸lnieniem techniki przedstawionej przez Vapnika w 1963 roku. Pozwala ona na nieliniowy ksztat brzegu obszaru decyzyjnego.

Zasada dziaania polega na znalezieniu takiego jdra przeksztacenia (ang. \emph{kernel}) \(\phi\), kt贸re odwzoruje przestrze \(d\)-wymiarow w \(d'\)-wymiarow, gdzie \(d'>d\) tak, 偶e \(D_{\phi}=\{\phi(\vec{x}_i), y_i\}\) jest mo偶liwie jak najbardziej separowalna.

\begin{figure}

{\centering \includegraphics[width=13.46in]{images/Kernel_Machine} 

}

\caption{Przykad zastosowania takiego przeksztacenia jdrowego aby z sytuacji braku liniowej separowalnoci do niej doprowadzi}\label{fig:svm3}
\end{figure}

Dla funkcji jdrowej okrelonej wzorem \(k(\vec{x}_i,\vec{x}_j)=\phi(\vec{x}_i)\phi(\vec{x}_j)\) przeprowadzamy identyczne rozumowanie jak w przypadku liniowych brzeg贸w obszar贸w decyzyjnych.
Minimalizujemy zatem wyra偶enie
\begin{align}
    L(\alpha_i) =& \sum_{i=1}^{n}\alpha_i+\frac{1}{2}\sum_{i=1}^{n}\sum_{j=1}^{n}\alpha_i\alpha_jy_iy_j\phi(\vec{x}_i)\phi(\vec{x}_j)\\
        =&\sum_{i=1}^{n}\alpha_i+\frac{1}{2}\sum_{i=1}^{n}\sum_{j=1}^{n}\alpha_i\alpha_jy_iy_jk(\vec{x}_i,\vec{x}_j),
        \label{eq:strata4}
\end{align}
przy warunkach
\begin{equation}
    \sum_{i=1}^{n}\alpha_iy_i=0,\quad 0\leq \alpha_i\leq \frac{1}{2n\lambda}.
    \label{eq:nier3}
\end{equation}
Rozwizanie powy偶szego problemu s r贸wnie偶 podobne do ich liniowych odpowiednik贸w
\begin{equation}
    \vec{w}=\sum_{i=1}^{n}\alpha_iy_i\phi(\vec{x}_i),
    \label{eq:wagi3}
\end{equation}
a parametr \(b=\vec{w}\phi(\vec{x}_i)-y_i\).

Najczciej stosowanymi funkcjami jdrowymi s:

\begin{itemize}
\tightlist
\item
  wielomianowa \(k(\vec{x}_i,\vec{x}_j)=(a\vec{x}_i'\vec{x}_j+b)^q\),
\item
  gaussowska \(k(\vec{x}_i,\vec{x}_j)=\exp(-\gamma\|\vec{x}_i-\vec{x}_j\|^2)\),
\item
  Laplace'a \(k(\vec{x}_i,\vec{x}_j)=\exp(-\gamma\|\vec{x}_i-\vec{x}_j\|)\),
\item
  hiperboliczna \(k(\vec{x}_i,\vec{x}_j)=\tanh(\vec{x}_i'\vec{x}_j+b)\),
\item
  sigmoidalna \(k(\vec{x}_i,\vec{x}_j)=\tanh(a\vec{x}_i'\vec{x}_j+b)\),
\item
  Bessel'a \(k(\vec{x}_i,\vec{x}_j)=\frac{Bessel^n_{(\nu+1)}(\sigma\|\vec{x}_i-\vec{x}_j\|)}{(\|\vec{x}_i-\vec{x}_j\|)^{n(\nu+1)}}\),
\item
  ANOVA \(k(\vec{x}_i,\vec{x}_j)=\left(\sum_{k=1}^{n}\exp\big(-\sigma(x^k_i-x^k_j)^2\big)\right)^d\),
\item
  sklejana dla jednowymiarowej przestrzeni \(k(x_i,x_j)=1+x_ix_j\min(x_i,x_j)-\frac{x_i+x_j}{2}\big(\min(x_i,x_j)\big)^2+\frac{(\min(x_i,x_j))^3}{3}\).
\end{itemize}

W przypadku braku wiedzy o danych funkcja gaussowska, Laplace'a i Bessel'a s zalecane.

\textbf{Przykady obszar贸w zastosowa:}

\begin{itemize}
\tightlist
\item
  w kategoryzacji tekstu i hipertekstu;
\item
  klasyfikacji obraz贸w - rezultaty eksperyment贸w pokazuj, 偶e SVM daje lepsze rezultaty ni偶 inne techniki;
\item
  rozpoznawanie obiekt贸w 3D;
\item
  odnajdowanie wama do systemu;
\item
  rozpoznawanie pisma rcznego;
\item
  odkrywanie ukrytych treci na zdjciach;
\item
  klasyfikacja protein;
\item
  odnajdowanie sekwencji kodu genetycznego
\item
  itp\ldots{}
\end{itemize}

\hypertarget{zalety-i-wady-2}{%
\section{Zalety i wady}\label{zalety-i-wady-2}}

Mocne strony:

\begin{itemize}
\tightlist
\item
  stopie skomplikowania nie jest zale偶ny od wymiaru przestrzeni atrybut贸w;
\item
  optymalny klasyfikator (znajduje minimum globalne);
\item
  nie jest czuy na przetrenowanie;
\item
  bardzo du偶a skuteczno w praktyce.
\end{itemize}

Sabe strony:

\begin{itemize}
\tightlist
\item
  przy du偶ej iloci danych estymacja modelu mo偶e trwa dugo;
\item
  estymacja poprawnego modelu wymaga pewnej wiedzy;
\item
  nie ma miejsca na wprowadzenie wasnej wiedzy.
\end{itemize}

\hypertarget{refs}{}
\leavevmode\hypertarget{ref-R-rmarkdown}{}%
Allaire, JJ, Yihui Xie, Jonathan McPherson, Javier Luraschi, Kevin Ushey, Aron Atkins, Hadley Wickham, Joe Cheng, Winston Chang, and Richard Iannone. 2018. \emph{Rmarkdown: Dynamic Documents for R}. \url{https://CRAN.R-project.org/package=rmarkdown}.

\leavevmode\hypertarget{ref-breiman1996}{}%
Breiman, Leo. 1996. ``Bagging Predictors.'' \emph{Machine Learning} 24 (2): 123--40. \url{https://doi.org/10.1007/BF00058655}.

\leavevmode\hypertarget{ref-breiman1998}{}%
---------. 1998. ``Arcing Classifier (with Discussion and a Rejoinder by the Author).'' \emph{Ann. Statist.} 26 (3): 801--49. \url{https://doi.org/10.1214/aos/1024691079}.

\leavevmode\hypertarget{ref-R-rio}{}%
Chan, Chung-hong, and Thomas J. Leeper. 2018. \emph{Rio: A Swiss-Army Knife for Data I/O}. \url{https://CRAN.R-project.org/package=rio}.

\leavevmode\hypertarget{ref-R-janitor}{}%
Firke, Sam. 2018. \emph{Janitor: Simple Tools for Examining and Cleaning Dirty Data}. \url{https://CRAN.R-project.org/package=janitor}.

\leavevmode\hypertarget{ref-fisher1936}{}%
Fisher, R. A. 1936. ``The Use of Multiple Measurements in Taxonomic Problems.'' \emph{Annals of Eugenics} 7 (2): 179--88. \url{https://doi.org/10.1111/j.1469-1809.1936.tb02137.x}.

\leavevmode\hypertarget{ref-friedman1989}{}%
Friedman, Jerome H. 1989. ``Regularized Discriminant Analysis.'' \emph{Journal of the American Statistical Association} 84 (405): 165--75. \url{https://doi.org/10.2307/2289860}.

\leavevmode\hypertarget{ref-R-gbm}{}%
Greenwell, Brandon, Bradley Boehmke, Jay Cunningham, and GBM Developers. 2019. \emph{Gbm: Generalized Boosted Regression Models}. \url{https://CRAN.R-project.org/package=gbm}.

\leavevmode\hypertarget{ref-ho1995}{}%
Ho, Tin Kam. 1995. ``Random Decision Forests.'' In \emph{Proceedings of 3rd International Conference on Document Analysis and Recognition}, 1:278--82. IEEE.

\leavevmode\hypertarget{ref-R-Rweka}{}%
Hornik, Kurt, Christian Buchta, and Achim Zeileis. 2009. ``Open-Source Machine Learning: R Meets Weka.'' \emph{Computational Statistics} 24 (2): 225--32. \url{https://doi.org/10.1007/s00180-008-0119-7}.

\leavevmode\hypertarget{ref-R-party}{}%
Hothorn, Torsten, Kurt Hornik, and Achim Zeileis. 2006. ``Unbiased Recursive Partitioning: A Conditional Inference Framework.'' \emph{Journal of Computational and Graphical Statistics} 15 (3): 651--74. \url{https://doi.org/10.1198/106186006X133933}.

\leavevmode\hypertarget{ref-R-partykit}{}%
Hothorn, Torsten, and Achim Zeileis. 2015. ``Partykit: A Modular Toolkit for Recursive Partytioning in R.'' \emph{Journal of Machine Learning Research} 16: 3905--9. \url{http://jmlr.org/papers/v16/hothorn15a.html}.

\leavevmode\hypertarget{ref-kuhn}{}%
Jed Wing, Max Kuhn. Contributions from, Steve Weston, Andre Williams, Chris Keefer, Allan Engelhardt, Tony Cooper, Zachary Mayer, et al. 2018. \emph{Caret: Classification and Regression Training}. \url{https://CRAN.R-project.org/package=caret}.

\leavevmode\hypertarget{ref-kearns1989}{}%
Kearns, M., and L. G. Valiant. 1989. ``Crytographic Limitations on Learning Boolean Formulae and Finite Automata.'' \emph{Annual ACM Symposium on Theory of Computing}, 433. \url{http://search.ebscohost.com/login.aspx?direct=true\&db=edb\&AN=73725380\&lang=pl\&site=eds-live\&scope=site}.

\leavevmode\hypertarget{ref-R-C50}{}%
Kuhn, Max, and Ross Quinlan. 2018. \emph{C50: C5.0 Decision Trees and Rule-Based Models}. \url{https://CRAN.R-project.org/package=C50}.

\leavevmode\hypertarget{ref-R-las}{}%
Liaw, Andy, and Matthew Wiener. 2002. ``Classification and Regression by randomForest.'' \emph{R News} 2 (3): 18--22. \url{https://CRAN.R-project.org/doc/Rnews/}.

\leavevmode\hypertarget{ref-R-e1071}{}%
Meyer, David, Evgenia Dimitriadou, Kurt Hornik, Andreas Weingessel, and Friedrich Leisch. 2019. \emph{E1071: Misc Functions of the Department of Statistics, Probability Theory Group (Formerly: E1071), Tu Wien}. \url{https://CRAN.R-project.org/package=e1071}.

\leavevmode\hypertarget{ref-R-ipred}{}%
Peters, Andrea, and Torsten Hothorn. 2018. \emph{Ipred: Improved Predictors}. \url{https://CRAN.R-project.org/package=ipred}.

\leavevmode\hypertarget{ref-quinlan1993}{}%
Quinlan, J Ross. 1993. \emph{C4. 5: Programs for Machine Learning}. Morgan Kaufmann.

\leavevmode\hypertarget{ref-R-base}{}%
R Core Team. 2018. \emph{R: A Language and Environment for Statistical Computing}. Vienna, Austria: R Foundation for Statistical Computing. \url{https://www.R-project.org/}.

\leavevmode\hypertarget{ref-schapire1990}{}%
Schapire, Robert E. 1990. ``The Strength of Weak Learnability.'' \emph{Machine Learning} 5 (2): 197--227. \url{https://doi.org/10.1007/BF00116037}.

\leavevmode\hypertarget{ref-R-CHAID}{}%
Team, The FoRt Student Project. 2015. \emph{CHAID: CHi-Squared Automated Interaction Detection}.

\leavevmode\hypertarget{ref-R-VIM}{}%
Templ, Matthias, Alexander Kowarik, Andreas Alfons, and Bernd Prantner. 2019. \emph{VIM: Visualization and Imputation of Missing Values}. \url{https://CRAN.R-project.org/package=VIM}.

\leavevmode\hypertarget{ref-R-rpart}{}%
Therneau, Terry, and Beth Atkinson. 2018. \emph{Rpart: Recursive Partitioning and Regression Trees}. \url{https://CRAN.R-project.org/package=rpart}.

\leavevmode\hypertarget{ref-R-DMwR}{}%
Torgo, Luis. 2013. \emph{DMwR: Functions and Data for "Data Mining with R"}. \url{https://CRAN.R-project.org/package=DMwR}.

\leavevmode\hypertarget{ref-R-mda}{}%
Trevor Hastie, S original by, Robert Tibshirani. Original R port by Friedrich Leisch, Kurt Hornik, and Brian D. Ripley. 2017. \emph{Mda: Mixture and Flexible Discriminant Analysis}. \url{https://CRAN.R-project.org/package=mda}.

\leavevmode\hypertarget{ref-R-mice}{}%
van Buuren, Stef, and Karin Groothuis-Oudshoorn. 2018. \emph{Mice: Multivariate Imputation by Chained Equations}. \url{https://CRAN.R-project.org/package=mice}.

\leavevmode\hypertarget{ref-R-MASS}{}%
Venables, W. N., and B. D. Ripley. 2002. \emph{Modern Applied Statistics with S}. Fourth. New York: Springer. \url{http://www.stats.ox.ac.uk/pub/MASS4}.

\leavevmode\hypertarget{ref-R-klaR}{}%
Weihs, Claus, Uwe Ligges, Karsten Luebke, and Nils Raabe. 2005. ``KlaR Analyzing German Business Cycles.'' In \emph{Data Analysis and Decision Support}, edited by D. Baier, R. Decker, and L. Schmidt-Thieme, 335--43. Berlin: Springer-Verlag.

\leavevmode\hypertarget{ref-welch1939}{}%
Welch, B. L. 1939. ``Note on Discriminant Functions.'' \emph{Biometrika} 31 (1/2): 218--20. \url{https://doi.org/10.2307/2334985}.

\leavevmode\hypertarget{ref-R-mgcv}{}%
Wood, S. N. 2003. ``Thin-Plate Regression Splines.'' \emph{Journal of the Royal Statistical Society (B)} 65 (1): 95--114.

\leavevmode\hypertarget{ref-R-bookdown}{}%
Xie, Yihui. 2018a. \emph{Bookdown: Authoring Books and Technical Documents with R Markdown}. \url{https://CRAN.R-project.org/package=bookdown}.

\leavevmode\hypertarget{ref-R-knitr}{}%
---------. 2018b. \emph{Knitr: A General-Purpose Package for Dynamic Report Generation in R}. \url{https://CRAN.R-project.org/package=knitr}.

\end{document}
