
@book{tsay2010,
  title = {Analysis of Financial Time Series.},
  isbn = {978-0-470-41435-4},
  series = {Wiley {{Series}} in {{Probability}} and {{Statistics}}.},
  publisher = {{John Wiley \& Sons, Inc., Hoboken, NJ}},
  date = {2010},
  author = {Tsay, Ruey S.}
}

@book{miao2006,
  title = {A New Test of Symmetry about an Unknown Median},
  pagetotal = {199-214},
  date = {2006},
  author = {Miao, W. and Gel, Y.R. and Gastwirth, J.L.}
}

@book{box1983,
  title = {Analiza Szeregów Czasowych : Prognozowanie i Sterowanie.},
  isbn = {83-01-00360-X},
  publisher = {{Warszawa : Państwowe Wydaw. Naukowe, 1983.}},
  date = {1983},
  author = {Box, George E. P. and Jenkins, Gwilym M. and Herer, Wojciech}
}

@article{breiman1998,
  title = {Arcing Classifier (with Discussion and a Rejoinder by the Author)},
  volume = {26},
  url = {https://doi.org/10.1214/aos/1024691079},
  doi = {10.1214/aos/1024691079},
  number = {3},
  journaltitle = {Ann. Statist.},
  date = {1998-06},
  pages = {801-849},
  author = {Breiman, Leo},
  publisher = {{The Institute of Mathematical Statistics}},
  fjournal = {The Annals of Statistics}
}

@article{schapire1990,
  title = {The Strength of Weak Learnability},
  volume = {5},
  issn = {1573-0565},
  url = {https://doi.org/10.1007/BF00116037},
  doi = {10.1007/BF00116037},
  number = {2},
  journaltitle = {Machine Learning},
  date = {1990-06},
  pages = {197-227},
  author = {Schapire, Robert E.},
  day = {01}
}

@article{kearns1989,
  title = {Crytographic Limitations on Learning {{Boolean}} Formulae and Finite Automata.},
  issn = {07349025},
  url = {http://search.ebscohost.com/login.aspx?direct=true&db=edb&AN=73725380&lang=pl&site=eds-live&scope=site},
  journaltitle = {Annual ACM Symposium on Theory of Computing},
  date = {1989},
  pages = {433},
  author = {Kearns, M. and Valiant, L. G.}
}

@inproceedings{ho1995,
  title = {Random Decision Forests},
  volume = {1},
  booktitle = {Proceedings of 3rd International Conference on Document Analysis and Recognition},
  date = {1995},
  pages = {278-282},
  author = {Ho, Tin Kam},
  organization = {{IEEE}}
}

@book{breiman2017,
  title = {Classification and {{Regression Trees}}.},
  isbn = {978-0-412-04841-8},
  url = {http://search.ebscohost.com/login.aspx?direct=true&db=edsebk&AN=1619230&lang=pl&site=eds-live&scope=site},
  abstract = {The methodology used to construct tree structured rules is the focus of this monograph. Unlike many other statistical procedures, which moved from pencil and paper to calculators, this text's use of trees was unthinkable before computers. Both the practical and theoretical sides have been developed in the authors'study of tree methods. Classification and Regression Trees reflects these two sides, covering the use of trees as a data analysis method, and in a more mathematical framework, proving some of their fundamental properties.},
  publisher = {{Routledge}},
  date = {2017},
  author = {Breiman, Leo and Friedman, J. H. and Olshen, Richard A. and Stone, Charles J.}
}

@article{kavakiotis2017,
  title = {Machine {{Learning}} and {{Data Mining Methods}} in {{Diabetes Research}}},
  volume = {15},
  issn = {2001-0370},
  journaltitle = {Computational and Structural Biotechnology Journal},
  date = {2017},
  pages = {104-116},
  author = {Kavakiotis, Ioannis and Tsave, Olga and Salifoglou, Athanasios and Maglaveras, Nicos and Vlahavas, Ioannis and Chouvarda, Ioanna}
}

@article{breiman1996,
  title = {Bagging Predictors},
  volume = {24},
  issn = {1573-0565},
  url = {https://doi.org/10.1007/BF00058655},
  doi = {10.1007/BF00058655},
  number = {2},
  journaltitle = {Machine Learning},
  date = {1996-08},
  pages = {123-140},
  author = {Breiman, Leo},
  day = {01}
}

@book{quinlan1993,
  title = {C4. 5: {{Programs}} for {{Machine Learning}}},
  publisher = {{Morgan Kaufmann}},
  date = {1993},
  author = {Quinlan, J Ross}
}

@book{xie2015,
  location = {{Boca Raton, Florida}},
  title = {Dynamic {{Documents}} with {{R}} and Knitr},
  edition = {2nd},
  url = {http://yihui.name/knitr/},
  publisher = {{Chapman and Hall/CRC}},
  date = {2015},
  author = {Xie, Yihui},
  note = {ISBN 978-1498716963}
}

@book{rcoreteam2016,
  location = {{Vienna, Austria}},
  title = {R: {{A Language}} and {{Environment}} for {{Statistical Computing}}},
  url = {https://www.R-project.org/},
  date = {2016},
  author = {{R Core Team}},
  organization = {{R Foundation for Statistical Computing}}
}

@book{mclachlan1992,
  location = {{New York}},
  title = {Discriminant Analysis and Statistical Pattern Recognition},
  publisher = {{John Wiley \& Sons}},
  date = {1992},
  author = {McLachlan, G.J.}
}

@book{aggrwal2015,
  langid = {english},
  location = {{New York, NY}},
  title = {Data Mining: The Textbook},
  edition = {1st edition},
  isbn = {978-3-319-14141-1},
  shorttitle = {Data Mining},
  series = {Springer Texts in Statistics},
  publisher = {{Springer Science+Business Media}},
  date = {2015},
  author = {Aggrwal, Charu C.},
  file = {C:\\Users\\majer\\Zotero\\storage\\TU4MXHSJ\\2015 - Data mining the textbook.pdf}
}

@book{gray2017,
  langid = {english},
  location = {{Hauppauge, New York}},
  title = {Principal {{Component Analysis}} : {{Methods}}, {{Applications}}, and {{Technology}}},
  isbn = {978-1-5361-0889-7},
  url = {http://search.ebscohost.com/login.aspx?direct=true&db=nlebk&AN=1464656&lang=pl&site=eds-live&scope=site},
  series = {Mathematics {{Research Developments}}},
  publisher = {{Nova Science Publishers, Inc}},
  date = {2017},
  keywords = {Factor analysis,MATHEMATICS / Applied,MATHEMATICS / Probability & Statistics / General,Multivariate analysis,Principal components analysis},
  author = {Gray, Virginia}
}

@article{pearson1901,
  title = {{{LIII}}. {{On}} Lines and Planes of Closest Fit to Systems of Points in Space},
  volume = {2},
  issn = {1941-5982},
  url = {https://doi.org/10.1080/14786440109462720},
  doi = {10.1080/14786440109462720},
  number = {11},
  journaltitle = {The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science},
  urldate = {2019-03-19},
  date = {1901-11-01},
  pages = {559-572},
  author = {Pearson, Karl}
}

@article{hotelling1936,
  title = {Relations {{Between Two Sets}} of {{Variates}}},
  volume = {28},
  issn = {0006-3444},
  url = {https://www.jstor.org/stable/2333955},
  doi = {10.2307/2333955},
  number = {3/4},
  journaltitle = {Biometrika},
  urldate = {2019-03-19},
  date = {1936},
  pages = {321-377},
  author = {Hotelling, Harold}
}

@article{welch1939,
  title = {Note on {{Discriminant Functions}}},
  volume = {31},
  issn = {0006-3444},
  url = {https://www.jstor.org/stable/2334985},
  doi = {10.2307/2334985},
  number = {1/2},
  journaltitle = {Biometrika},
  urldate = {2019-03-26},
  date = {1939},
  pages = {218-220},
  author = {Welch, B. L.}
}

@article{fisher1936,
  langid = {english},
  title = {The {{Use}} of {{Multiple Measurements}} in {{Taxonomic Problems}}},
  volume = {7},
  issn = {2050-1439},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1469-1809.1936.tb02137.x},
  doi = {10.1111/j.1469-1809.1936.tb02137.x},
  abstract = {The articles published by the Annals of Eugenics (1925–1954) have been made available online as an historical archive intended for scholarly use. The work of eugenicists was often pervaded by prejudice against racial, ethnic and disabled groups. The online publication of this material for scholarly research purposes is not an endorsement of those views nor a promotion of eugenics in any way.},
  number = {2},
  journaltitle = {Annals of Eugenics},
  urldate = {2019-03-26},
  date = {1936},
  pages = {179-188},
  author = {Fisher, R. A.},
  file = {C:\\Users\\majer\\Dysk Google\\literatura\\Fisher\\fisher_1936_the_use_of_multiple_measurements_in_taxonomic_problems.pdf}
}

@article{friedman1989,
  title = {Regularized {{Discriminant Analysis}}},
  volume = {84},
  issn = {0162-1459},
  url = {https://www.jstor.org/stable/2289860},
  doi = {10.2307/2289860},
  abstract = {[Linear and quadratic discriminant analysis are considered in the small-sample, high-dimensional setting. Alternatives to the usual maximum likelihood (plug-in) estimates for the covariance matrices are proposed. These alternatives are characterized by two parameters, the values of which are customized to individual situations by jointly minimizing a sample-based estimate of future misclassification risk. Computationally fast implementations are presented, and the efficacy of the approach is examined through simulation studies and application to data. These studies indicate that in many circumstances dramatic gains in classification accuracy can be achieved.]},
  number = {405},
  journaltitle = {Journal of the American Statistical Association},
  urldate = {2019-03-26},
  date = {1989},
  pages = {165-175},
  author = {Friedman, Jerome H.},
  file = {C:\\Users\\majer\\Dysk Google\\literatura\\Friedman\\friedman_1989_regularized_discriminant_analysis.pdf}
}

@article{guo2007,
  langid = {english},
  title = {Regularized Linear Discriminant Analysis and Its Application in Microarrays},
  volume = {8},
  issn = {1465-4644},
  doi = {10.1093/biostatistics/kxj035},
  abstract = {In this paper, we introduce a modified version of linear discriminant analysis, called the "shrunken centroids regularized discriminant analysis" (SCRDA). This method generalizes the idea of the "nearest shrunken centroids" (NSC) (Tibshirani and others, 2003) into the classical discriminant analysis. The SCRDA method is specially designed for classification problems in high dimension low sample size situations, for example, microarray data. Through both simulated data and real life data, it is shown that this method performs very well in multivariate classification problems, often outperforms the PAM method (using the NSC algorithm) and can be as competitive as the support vector machines classifiers. It is also suitable for feature elimination purpose and can be used as gene selection method. The open source R package for this method (named "rda") is available on CRAN (http://www.r-project.org) for download and testing.},
  number = {1},
  journaltitle = {Biostatistics (Oxford, England)},
  shortjournal = {Biostatistics},
  date = {2007-01},
  pages = {86-100},
  keywords = {Computer Simulation,Discriminant Analysis,DNA; Neoplasm,Gene Expression Profiling,Humans,Linear Models,Neoplasms,Oligonucleotide Array Sequence Analysis},
  author = {Guo, Yaqian and Hastie, Trevor and Tibshirani, Robert},
  file = {C:\\Users\\majer\\Dysk Google\\literatura\\Guo et al\\guo_et_al_2007_regularized_linear_discriminant_analysis_and_its_application_in_microarrays.pdf},
  eprinttype = {pmid},
  eprint = {16603682}
}

@article{hastie1996,
  title = {Discriminant {{Analysis}} by {{Gaussian Mixtures}}},
  volume = {58},
  issn = {0035-9246},
  url = {https://www.jstor.org/stable/2346171},
  abstract = {[Fisher-Rao linear discriminant analysis (LDA) is a valuable tool for multigroup classification. LDA is equivalent to maximum likelihood classification assuming Gaussian distributions for each class. In this paper, we fit Gaussian mixtures to each class to facilitate effective classification in non-normal settings, especially when the classes are clustered. Low dimensional views are an important by-product of LDA--our new techniques inherit this feature. We can control the within-class spread of the subclass centres relative to the between-class spread. Our technique for fitting these models permits a natural blend with nonparametric versions of LDA.]},
  number = {1},
  journaltitle = {Journal of the Royal Statistical Society. Series B (Methodological)},
  urldate = {2019-03-26},
  date = {1996},
  pages = {155-176},
  author = {Hastie, Trevor and Tibshirani, Robert}
}

@article{hastie1994,
  title = {Flexible {{Discriminant Analysis}} by {{Optimal Scoring}}},
  volume = {89},
  issn = {0162-1459},
  url = {https://www.jstor.org/stable/2290989},
  doi = {10.2307/2290989},
  abstract = {[Fisher's linear discriminant analysis is a valuable tool for multigroup classification. With a large number of predictors, one can find a reduced number of discriminant coordinate functions that are "optimal" for separating the groups. With two such functions, one can produce a classification map that partitions the reduced space into regions that are identified with group membership, and the decision boundaries are linear. This article is about richer nonlinear classification schemes. Linear discriminant analysis is equivalent to multiresponse linear regression using optimal scorings to represent the groups. In this paper, we obtain nonparametric versions of discriminant analysis by replacing linear regression by any nonparametric regression method. In this way, any multiresponse regression technique (such as MARS or neural networks) can be postprocessed to improve its classification performance]},
  number = {428},
  journaltitle = {Journal of the American Statistical Association},
  urldate = {2019-03-26},
  date = {1994},
  pages = {1255-1270},
  author = {Hastie, Trevor and Tibshirani, Robert and Buja, Andreas},
  file = {C:\\Users\\majer\\Dysk Google\\literatura\\Hastie et al\\hastie_et_al_1994_flexible_discriminant_analysis_by_optimal_scoring.pdf}
}

@article{witten2011,
  title = {Penalized Classification Using {{Fisher}}’s Linear Discriminant},
  volume = {73},
  issn = {1369-7412},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3272679/},
  doi = {10.1111/j.1467-9868.2011.00783.x},
  abstract = {We consider the supervised classification setting, in which the data consist of p features measured on n observations, each of which belongs to one of K classes. Linear discriminant analysis (LDA) is a classical method for this problem. However, in the high-dimensional setting where p ≫ n, LDA is not appropriate for two reasons. First, the standard estimate for the within-class covariance matrix is singular, and so the usual discriminant rule cannot be applied. Second, when p is large, it is difficult to interpret the classification rule obtained from LDA, since it involves all p features. We propose penalized LDA, a general approach for penalizing the discriminant vectors in Fisher’s discriminant problem in a way that leads to greater interpretability. The discriminant problem is not convex, so we use a minorization-maximization approach in order to efficiently optimize it when convex penalties are applied to the discriminant vectors. In particular, we consider the use of L1 and fused lasso penalties. Our proposal is equivalent to recasting Fisher’s discriminant problem as a biconvex problem. We evaluate the performances of the resulting methods on a simulation study, and on three gene expression data sets. We also survey past methods for extending LDA to the high-dimensional setting, and explore their relationships with our proposal.},
  number = {5},
  journaltitle = {Journal of the Royal Statistical Society. Series B, Statistical methodology},
  shortjournal = {J R Stat Soc Series B Stat Methodol},
  urldate = {2019-03-26},
  date = {2011-11},
  pages = {753-772},
  author = {Witten, Daniela M. and Tibshirani, Robert},
  file = {C:\\Users\\majer\\Dysk Google\\literatura\\Witten_Tibshirani\\witten_tibshirani_2011_penalized_classification_using_fisher’s_linear_discriminant.pdf},
  eprinttype = {pmid},
  eprint = {22323898},
  pmcid = {PMC3272679}
}

@book{duda2001,
  langid = {english},
  title = {Pattern Classification},
  isbn = {978-0-471-05669-0},
  abstract = {The first edition, published in 1973, has become a classic reference in the field. Now with the second edition, readers will find information on key new topics such as neural networks and statistical pattern recognition, the theory of machine learning, and the theory of invariances. Also included are worked examples, comparisons between different methods, extensive graphics, expanded exercises and computer project topics.  An Instructor's Manual presenting detailed solutions to all the problems in the book is available from the Wiley editorial department.},
  pagetotal = {688},
  publisher = {{Wiley}},
  date = {2001},
  keywords = {Business & Economics / Statistics,Computers / Computer Vision & Pattern Recognition,Computers / Intelligence (AI) & Semantics,Computers / Optical Data Processing,Fiction / General,Technology & Engineering / Electrical,Technology & Engineering / Electronics / Digital,Technology & Engineering / Electronics / General},
  author = {Duda, Richard O. and Hart, Peter E. and Stork, David G.},
  eprinttype = {googlebooks}
}

@Book{wood2017,
  title = {Generalized Additive Models: An Introduction with R},
  year = {2017},
  author = {S.N Wood},
  edition = {2},
  publisher = {Chapman and Hall/CRC},
}
